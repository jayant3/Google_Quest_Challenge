{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALBERT + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import pickle\n",
    "#from transformers import \n",
    "import numpy as np\n",
    "from numpy import zeros, newaxis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm\n",
    "import random as rn\n",
    "import tensorflow \n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#nltk.download('vader_lexicon')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D,AveragePooling1D,MaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Flatten,Embedding,Input,concatenate,Activation,Dropout,BatchNormalization,LSTM\n",
    "from tensorflow.keras import regularizers,Model\n",
    "from tensorflow.keras.regularizers import l1,l2\n",
    "from tensorflow.keras.callbacks import TensorBoard, Callback, EarlyStopping, ModelCheckpoint,LearningRateScheduler\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from transformers import AlbertTokenizer, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/google-quest-challenge/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_sentiments(df,column_list):\n",
    "    '''\n",
    "    sentiments from question_title\n",
    "    '''\n",
    "     \n",
    "    for col in column_list:\n",
    "        \n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        neg=[]\n",
    "        neu=[]\n",
    "        pos=[]\n",
    "        comp=[]\n",
    "    \n",
    "        for txt in df[col]:\n",
    "            ss = sid.polarity_scores(txt)\n",
    "            neg.append(ss['neg'])\n",
    "            neu.append(ss['neu'])\n",
    "            pos.append(ss['pos'])\n",
    "            comp.append(ss['compound'])\n",
    "\n",
    "        df[col+\"_neg\"] = neg\n",
    "        df[col+\"_neu\"] = neu\n",
    "        df[col+\"_pos\"] = pos\n",
    "        df[col+\"_comp\"] = comp\n",
    "    \n",
    "    return df\n",
    "\n",
    "#==================================================\n",
    "\n",
    "def countVectorizer(df_train,df_cv,column):\n",
    "    '''\n",
    "    function to vectorize categorical data\n",
    "    '''\n",
    "    # one-hot encoding 'category' feature\n",
    "    vect = CountVectorizer(binary=True)\n",
    "    vect.fit(df_train[column])\n",
    "    encoded_train = vect.transform(df_train[column]).todense()\n",
    "    encoded_cv = vect.transform(df_cv[column]).todense()\n",
    "    \n",
    "    return encoded_train, encoded_cv, vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "def spearman(y_true,y_pred):\n",
    "    '''\n",
    "    function to calculate mean spearman correlation of all 30 class-labels\n",
    "    '''\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    spearman_y = []\n",
    "    for i in range(30):\n",
    "        score = spearmanr(y_true[:,i], y_pred[:,i] + np.random.normal(0, 1e-7, y_pred.shape[0]) , \n",
    "                                                                        nan_policy='omit').correlation\n",
    "                                                                        \n",
    "\n",
    "        spearman_y.append(score)\n",
    "    mean_score = np.nanmean(spearman_y)\n",
    "    return mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================PREPROCESSING===================================\n",
    "    \n",
    "# preprocessing host name \n",
    "train['host'] = train['host'].apply(lambda x: x.split('.')[-2])\n",
    "train['host'] = train['host'].apply(lambda x: x.lower())\n",
    "train['host'] = train['host'].apply(lambda x: x.strip())\n",
    "    \n",
    "# preprocessing 'category' to lower-case and stripping leading and tailing spaces \n",
    "train['category'] = train['category'].apply(lambda x: x.lower())\n",
    "train['category'] = train['category'].apply(lambda x: x.strip())\n",
    "    \n",
    "#=================================FEATURE ENGINEERING=============================\n",
    "    \n",
    "# taking length of question title, question body, answer\n",
    "train['q_title_length'] = train['question_title'].apply(lambda x: len(x.split(' ')))\n",
    "train['q_body_length'] = train['question_body'].apply(lambda x: len(x.split(' ')))\n",
    "train['answer_length'] = train['answer'].apply(lambda x: len(x.split(' ')))\n",
    "#train.head()\n",
    "    \n",
    "train = get_sentiments(train,['question_title','question_body','answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6079, 26) (6079, 30)\n"
     ]
    }
   ],
   "source": [
    "# separation of features and class-labels\n",
    "class_labels = ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational', \n",
    "                    'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer', \n",
    "                    'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', \n",
    "                    'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', \n",
    "                    'question_type_compare', 'question_type_consequence', 'question_type_definition', \n",
    "                    'question_type_entity', 'question_type_instructions', 'question_type_procedure', \n",
    "                    'question_type_reason_explanation', 'question_type_spelling', 'question_well_written',\n",
    "                    'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance', \n",
    "                    'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', \n",
    "                    'answer_type_reason_explanation', 'answer_well_written']\n",
    "\n",
    "X = train.drop(columns=class_labels,axis=1)\n",
    "y = train[class_labels]\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  (5775, 26) (5775, 30)\n",
      "Test shape:  (304, 26) (304, 30)\n"
     ]
    }
   ],
   "source": [
    "# splitting data\n",
    "x_train,x_cv,y_train,y_cv = train_test_split(X,y,test_size=0.05,stratify = X['category'])\n",
    "print(\"Train shape: \", x_train.shape,y_train.shape)\n",
    "print(\"Test shape: \", x_cv.shape,y_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5775, 5)\n",
      "(5775, 6)\n"
     ]
    }
   ],
   "source": [
    "# Encoding categorical data\n",
    "category_encoded_train, category_encoded_cv, category_vectorizer = countVectorizer(x_train,x_cv,'category')\n",
    "host_encoded_train, host_encoded_cv, host_vectorizer = countVectorizer(x_train,x_cv,'host')\n",
    "\n",
    "print(category_encoded_train.shape)\n",
    "print(host_encoded_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT tokenizer (pretrained)\n",
    "tokenizer = BertTokenizer.from_pretrained('../input/bert-base-uncased3/bert_en_uncased_L-12_H-768_A-12_2/assets/vocab.txt')\n",
    "\n",
    "\n",
    "def encode_text(text,max_len):\n",
    "    \n",
    "    '''\n",
    "    function to encode text for input to ALBERT\n",
    "    '''\n",
    "    \n",
    "    encoded_dict = tokenizer.encode_plus(text, None, max_length=max_len, pad_to_max_length=True,\n",
    "                                       add_special_tokens=True)\n",
    "    return encoded_dict\n",
    "    \n",
    "#================================================================================\n",
    "\n",
    "def get_encoded_bert_inputs(df,max_len):\n",
    "    '''\n",
    "    function to encode text data into BERT input form\n",
    "    '''\n",
    "    q_input_ids = []\n",
    "    q_masks = []\n",
    "    q_segments = []\n",
    "    a_input_ids = []\n",
    "    a_masks = []\n",
    "    a_segments = []\n",
    "    \n",
    "    # question encoding\n",
    "    for i in df.index:\n",
    "        q_text = df['question_title'][i] + \" [SEP] \" + df['question_body'][i]\n",
    "        q_encoded_dict = encode_text(q_text,max_len)\n",
    "        q_input_ids.append(q_encoded_dict['input_ids'])\n",
    "        q_masks.append(q_encoded_dict['attention_mask'])\n",
    "        q_segments.append(q_encoded_dict['token_type_ids'])\n",
    "    \n",
    "    # answer encoding\n",
    "    for i in df.index:\n",
    "        a_text = df['answer'][i]\n",
    "        a_encoded_dict = encode_text(a_text,max_len)\n",
    "        a_input_ids.append(a_encoded_dict['input_ids'])\n",
    "        a_masks.append(a_encoded_dict['attention_mask'])\n",
    "        a_segments.append(a_encoded_dict['token_type_ids'])\n",
    "    \n",
    "    return q_input_ids, q_masks, q_segments, a_input_ids, a_masks, a_segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting inputs for BERT\n",
    "q_input_ids,q_mask,q_seg,a_input_ids,a_mask,a_seg = get_encoded_bert_inputs(x_train,max_len=450)\n",
    "q_input_ids_cv,q_mask_cv,q_seg_cv,a_input_ids_cv,a_mask_cv,a_seg_cv = get_encoded_bert_inputs(x_cv,max_len=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# defining base BERT layer\n",
    "bert_layer = hub.KerasLayer(\"../input/bert-base-uncased3/bert_en_uncased_L-12_H-768_A-12_2\",\n",
    "                              trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum_of_word_vec(input_vec):\n",
    "    # function to get sum of vectors for each word in a question/answer\n",
    "    v = np.zeros(shape=(1,768))\n",
    "    num_of_words = input_vec.shape[1]\n",
    "    for j in range(num_of_words):\n",
    "        v = v + input_vec[0,j,:]\n",
    "    return v\n",
    "\n",
    "#=========================================================================\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_text_vector(albert_layer,input_ids,mask,seg):\n",
    "    '''\n",
    "    function to get BERT output that represents whole input sequence\n",
    "    function takes the encoded output for each word and then sums up to get the final vector for each datapoint \n",
    "    '''\n",
    "    \n",
    "    for i in tqdm(range(len(input_ids))):\n",
    "        pool, seq = albert_layer([input_ids[i:i+1],mask[i:i+1],seg[i:i+1]])\n",
    "        \n",
    "        seq = get_sum_of_word_vec(seq)\n",
    "        \n",
    "        if i>0:\n",
    "            \n",
    "            final_text_seq = tf.keras.layers.concatenate([final_text_seq,seq],axis=0)\n",
    "        \n",
    "        else:\n",
    "            final_text_seq = seq\n",
    "            \n",
    "    return final_text_seq\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5775/5775 [12:15<00:00,  7.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# getting vector output (representing whole input sequence) of question from ALBERT for train data\n",
    "question_vect = get_text_vector(bert_layer,q_input_ids,q_mask,q_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5775/5775 [12:08<00:00,  7.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# getting vector output (representing whole inpur sequence) of answer from ALBERT for train data\n",
    "answer_vect = get_text_vector(bert_layer,a_input_ids,a_mask,a_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 304/304 [00:38<00:00,  7.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# getting vector output (representing whole inpur sequence) of answer from ALBERT for cross-validation data\n",
    "question_vect_cv = get_text_vector(bert_layer,q_input_ids_cv,q_mask_cv,q_seg_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 304/304 [00:38<00:00,  7.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# getting vector output (representing whole inpur sequence) of answer from ALBERT for cross-validation data\n",
    "answer_vect_cv = get_text_vector(bert_layer,a_input_ids_cv,a_mask_cv,a_seg_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train text vector shape =  (5775, 1536)\n",
      "CV text vector shape =  (304, 1536)\n"
     ]
    }
   ],
   "source": [
    "# concatenating question and answer output from ALBERT\n",
    "final_text_vector_train = tf.keras.layers.concatenate([question_vect,answer_vect])\n",
    "final_text_vector_cv = tf.keras.layers.concatenate([question_vect_cv,answer_vect_cv])\n",
    "\n",
    "print(\"Train text vector shape = \",final_text_vector_train.shape)\n",
    "print(\"CV text vector shape = \",final_text_vector_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5775, 26, 1) (304, 26, 1)\n"
     ]
    }
   ],
   "source": [
    "# concatenating and reshaping categorical and numerical data for train and cross-validation\n",
    "\n",
    "train_vect_cat_num = np.hstack([category_encoded_train,host_encoded_train,\n",
    "                                    x_train[['q_title_length','q_body_length','answer_length', 'question_title_neg',\n",
    "                                             'question_title_neu', 'question_title_pos', 'question_title_comp', \n",
    "                                             'question_body_neg','question_body_neu', 'question_body_pos', 'question_body_comp', \n",
    "                                             'answer_neg','answer_neu', 'answer_pos', 'answer_comp']]])\n",
    "\n",
    "cv_vect_cat_num = np.hstack([category_encoded_cv,host_encoded_cv,\n",
    "                                 x_cv[['q_title_length','q_body_length','answer_length', 'question_title_neg',\n",
    "                                       'question_title_neu', 'question_title_pos', 'question_title_comp', \n",
    "                                       'question_body_neg','question_body_neu', 'question_body_pos', 'question_body_comp', \n",
    "                                       'answer_neg','answer_neu', 'answer_pos', 'answer_comp']]])\n",
    "train_vect_cat_num = train_vect_cat_num[:,:,newaxis]\n",
    "cv_vect_cat_num = cv_vect_cat_num[:,:,newaxis]\n",
    "print(train_vect_cat_num.shape, cv_vect_cat_num.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(train_vect_text, train_vect_cat_num):\n",
    "    '''\n",
    "    function to create CNN Model architecture\n",
    "    '''\n",
    "\n",
    "    #fixing numpy RS\n",
    "    np.random.seed(42)\n",
    "\n",
    "    #fixing tensorflow RS\n",
    "    tensorflow.random.set_seed(32)\n",
    "    \n",
    "    #python RS\n",
    "    rn.seed(12)\n",
    "\n",
    "    #input 1\n",
    "    input1 = Input(shape=(train_vect_text.shape[1],1), name = 'input_1')\n",
    "    conv = Conv1D(filters = 32, kernel_size = 7,strides = 1, activation='relu',\n",
    "                  kernel_initializer=tensorflow.keras.initializers.he_normal(seed=43),\n",
    "                  kernel_regularizer=tensorflow.keras.regularizers.l2(0.6),\n",
    "                  input_shape=(None,train_vect_text.shape[1],1))(input1)\n",
    "    conv = MaxPooling1D()(conv)\n",
    "    \n",
    "    conv = Conv1D(16, 7, activation='relu',kernel_initializer=tensorflow.keras.initializers.he_normal(seed=43),\n",
    "                  kernel_regularizer=tensorflow.keras.regularizers.l2(0.6))(conv)\n",
    "    conv = MaxPooling1D()(conv)\n",
    "    flat_text = Flatten()(conv)\n",
    "    out_1 =  Dense(768,activation='sigmoid')(flat_text)\n",
    "    #====================================================================================================================\n",
    "\n",
    "    #input 2\n",
    "    input2 = Input(shape=(train_vect_cat_num.shape[1],1), name = 'input_2')\n",
    "    conv = Conv1D(filters = 64, kernel_size = 3,strides = 1, activation='relu',\n",
    "                  kernel_initializer=tensorflow.keras.initializers.he_normal(seed=43),\n",
    "                  kernel_regularizer=tensorflow.keras.regularizers.l2(0.2),\n",
    "                  input_shape=(None,train_vect_cat_num.shape[1],1))(input2)\n",
    "\n",
    "    conv = MaxPooling1D()(conv)\n",
    "    conv = Conv1D(32, 3,  activation='relu',kernel_initializer=tensorflow.keras.initializers.he_normal(seed=43))(conv)\n",
    "    conv = MaxPooling1D()(conv)\n",
    "    flat_rem = Flatten()(conv)\n",
    "    out_2 =  Dense(48,activation='sigmoid')(flat_rem)\n",
    "    #====================================================================================================================\n",
    "\n",
    "    final_data = concatenate([out_1,out_2])\n",
    "\n",
    "    #====================================================================================================================\n",
    "\n",
    "    output = Dense(30,activation='sigmoid',kernel_initializer=tensorflow.keras.initializers.glorot_uniform(seed=45))(final_data)\n",
    "\n",
    "    # create model with 2 inputs\n",
    "    model = Model([input1,input2], output)\n",
    "\n",
    "    model.compile(loss=tensorflow.keras.losses.binary_crossentropy,\n",
    "              optimizer=tensorflow.keras.optimizers.Adam(0.001),\n",
    "              metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1536, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 26, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 1530, 32)     256         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 24, 64)       256         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 765, 32)      0           conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 12, 64)       0           conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 759, 16)      3600        max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 10, 32)       6176        max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 379, 16)      0           conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5, 32)        0           conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_24 (Flatten)            (None, 6064)         0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_25 (Flatten)            (None, 160)          0           max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 768)          4657920     flatten_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 48)           7728        flatten_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12168 (Concatenate) (None, 816)          0           dense_36[0][0]                   \n",
      "                                                                 dense_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 30)           24510       concatenate_12168[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 4,700,446\n",
      "Trainable params: 4,700,446\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# creating a CNN model \n",
    "model = cnn_model(final_text_vector_train,train_vect_cat_num)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "172/172 - 1s - loss: 56.2657 - mae: 0.1750 - val_loss: 40.4854 - val_mae: 0.1730\n",
      "Epoch 2/35\n",
      "172/172 - 1s - loss: 32.5068 - mae: 0.1722 - val_loss: 25.8240 - val_mae: 0.1725\n",
      "Epoch 3/35\n",
      "172/172 - 1s - loss: 21.0699 - mae: 0.1730 - val_loss: 16.8953 - val_mae: 0.1741\n",
      "Epoch 4/35\n",
      "172/172 - 1s - loss: 13.8182 - mae: 0.1717 - val_loss: 11.1115 - val_mae: 0.1699\n",
      "Epoch 5/35\n",
      "172/172 - 1s - loss: 9.1021 - mae: 0.1607 - val_loss: 7.3264 - val_mae: 0.1523\n",
      "Epoch 6/35\n",
      "172/172 - 1s - loss: 6.0271 - mae: 0.1507 - val_loss: 4.8744 - val_mae: 0.1510\n",
      "Epoch 7/35\n",
      "172/172 - 1s - loss: 4.0209 - mae: 0.1477 - val_loss: 3.2739 - val_mae: 0.1518\n",
      "Epoch 8/35\n",
      "172/172 - 1s - loss: 2.7177 - mae: 0.1453 - val_loss: 2.2325 - val_mae: 0.1489\n",
      "Epoch 9/35\n",
      "172/172 - 1s - loss: 1.8706 - mae: 0.1435 - val_loss: 1.5621 - val_mae: 0.1489\n",
      "Epoch 10/35\n",
      "172/172 - 1s - loss: 1.3238 - mae: 0.1426 - val_loss: 1.1288 - val_mae: 0.1519\n",
      "Epoch 11/35\n",
      "172/172 - 1s - loss: 0.9711 - mae: 0.1415 - val_loss: 0.8489 - val_mae: 0.1483\n",
      "Epoch 12/35\n",
      "172/172 - 1s - loss: 0.7471 - mae: 0.1416 - val_loss: 0.6697 - val_mae: 0.1469\n",
      "Epoch 13/35\n",
      "172/172 - 1s - loss: 0.6048 - mae: 0.1412 - val_loss: 0.5633 - val_mae: 0.1472\n",
      "Epoch 14/35\n",
      "172/172 - 1s - loss: 0.5161 - mae: 0.1411 - val_loss: 0.4907 - val_mae: 0.1465\n",
      "Epoch 15/35\n",
      "172/172 - 1s - loss: 0.4608 - mae: 0.1406 - val_loss: 0.4477 - val_mae: 0.1442\n",
      "Epoch 16/35\n",
      "172/172 - 1s - loss: 0.4274 - mae: 0.1402 - val_loss: 0.4260 - val_mae: 0.1445\n",
      "Epoch 17/35\n",
      "172/172 - 1s - loss: 0.4074 - mae: 0.1393 - val_loss: 0.4099 - val_mae: 0.1473\n",
      "Epoch 18/35\n",
      "172/172 - 1s - loss: 0.3962 - mae: 0.1390 - val_loss: 0.4017 - val_mae: 0.1434\n",
      "Epoch 19/35\n",
      "172/172 - 1s - loss: 0.3892 - mae: 0.1380 - val_loss: 0.3967 - val_mae: 0.1456\n",
      "Epoch 20/35\n",
      "172/172 - 1s - loss: 0.3858 - mae: 0.1375 - val_loss: 0.3981 - val_mae: 0.1446\n",
      "Epoch 21/35\n",
      "172/172 - 1s - loss: 0.3844 - mae: 0.1375 - val_loss: 0.3939 - val_mae: 0.1435\n",
      "Epoch 22/35\n",
      "172/172 - 1s - loss: 0.3830 - mae: 0.1368 - val_loss: 0.3952 - val_mae: 0.1457\n",
      "Epoch 23/35\n",
      "172/172 - 1s - loss: 0.3822 - mae: 0.1365 - val_loss: 0.3939 - val_mae: 0.1433\n",
      "Epoch 24/35\n",
      "172/172 - 1s - loss: 0.3809 - mae: 0.1356 - val_loss: 0.3916 - val_mae: 0.1446\n",
      "Epoch 25/35\n",
      "172/172 - 1s - loss: 0.3801 - mae: 0.1352 - val_loss: 0.3951 - val_mae: 0.1445\n",
      "Epoch 26/35\n",
      "172/172 - 1s - loss: 0.3793 - mae: 0.1344 - val_loss: 0.3925 - val_mae: 0.1447\n",
      "Epoch 27/35\n",
      "172/172 - 1s - loss: 0.3789 - mae: 0.1341 - val_loss: 0.3925 - val_mae: 0.1423\n",
      "Epoch 28/35\n",
      "172/172 - 1s - loss: 0.3787 - mae: 0.1337 - val_loss: 0.3930 - val_mae: 0.1436\n",
      "Epoch 29/35\n",
      "172/172 - 1s - loss: 0.3774 - mae: 0.1329 - val_loss: 0.3952 - val_mae: 0.1441\n",
      "Epoch 30/35\n",
      "172/172 - 1s - loss: 0.3770 - mae: 0.1326 - val_loss: 0.3946 - val_mae: 0.1419\n",
      "Epoch 31/35\n",
      "172/172 - 1s - loss: 0.3761 - mae: 0.1318 - val_loss: 0.3932 - val_mae: 0.1440\n",
      "Epoch 32/35\n",
      "172/172 - 1s - loss: 0.3749 - mae: 0.1310 - val_loss: 0.3971 - val_mae: 0.1486\n",
      "Epoch 33/35\n",
      "172/172 - 1s - loss: 0.3744 - mae: 0.1306 - val_loss: 0.3956 - val_mae: 0.1431\n",
      "Epoch 34/35\n",
      "172/172 - 1s - loss: 0.3742 - mae: 0.1302 - val_loss: 0.3941 - val_mae: 0.1419\n",
      "Epoch 35/35\n",
      "172/172 - 1s - loss: 0.3727 - mae: 0.1291 - val_loss: 0.3928 - val_mae: 0.1428\n"
     ]
    }
   ],
   "source": [
    "# checkpoint model for best weights\n",
    "filepath=\"weights_zn.best_copy.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "# fitting model on train data\n",
    "history_model = model.fit([final_text_vector_train,train_vect_cat_num], y_train, \n",
    "                                 epochs=35,\n",
    "                                 verbose=2,batch_size=32,\n",
    "                                 callbacks = [checkpoint],\n",
    "                                 validation_split = 0.05)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's correlation on train data =  0.4076806249634812\n",
      "Spearman's correlation on cross-validation data =  0.3618157991884826\n"
     ]
    }
   ],
   "source": [
    "# loading best weights for prediction\n",
    "model.load_weights(\"weights_zn.best_copy.hdf5\")\n",
    "    \n",
    "# predicting on train data and calculating spearman correlation \n",
    "y_tr_pred = model.predict([final_text_vector_train,train_vect_cat_num])\n",
    "print(\"Spearman's correlation on train data = \",spearman(y_train,y_tr_pred))\n",
    "    \n",
    "# predicting on cross-validation data and calculating spearman correlation \n",
    "y_cv_pred = model.predict([final_text_vector_cv,cv_vect_cat_num])\n",
    "print(\"Spearman's correlation on cross-validation data = \",spearman(y_cv,y_cv_pred))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../input/google-quest-challenge/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================PREPROCESSING===================================\n",
    "    \n",
    "# preprocessing host name \n",
    "test['host'] = test['host'].apply(lambda x: x.split('.')[-2])\n",
    "test['host'] = test['host'].apply(lambda x: x.lower())\n",
    "test['host'] = test['host'].apply(lambda x: x.strip())\n",
    "    \n",
    "# preprocessing 'category' to lower-case and stripping leading and tailing spaces \n",
    "test['category'] = test['category'].apply(lambda x: x.lower())\n",
    "test['category'] = test['category'].apply(lambda x: x.strip())\n",
    "    \n",
    "#=================================FEATURE ENGINEERING=============================\n",
    "    \n",
    "# taking length of question title, question body, answer\n",
    "test['q_title_length'] = test['question_title'].apply(lambda x: len(x.split(' ')))\n",
    "test['q_body_length'] = test['question_body'].apply(lambda x: len(x.split(' ')))\n",
    "test['answer_length'] = test['answer'].apply(lambda x: len(x.split(' ')))\n",
    "#train.head()\n",
    "    \n",
    "test = get_sentiments(test,['question_title','question_body','answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding categorical data - 'category' and 'host'\n",
    "category_encoded_test = category_vectorizer.transform(test['category']).todense()\n",
    "host_encoded_test = host_vectorizer.transform(test['host']).todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting inputs for BERT\n",
    "q_input_ids_test,q_mask_test,q_seg_test,a_input_ids_test,a_mask_test,a_seg_test = get_encoded_bert_inputs(test,max_len=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting vector output (representing whole input sequence) of question from BERT for test data\n",
    "question_vect_test = get_text_vector(bert_layer,q_input_ids_test,q_mask_test,q_seg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting vector output (representing whole inpur sequence) of answer from BERT for test data\n",
    "answer_vect_test = get_text_vector(bert_layer,a_input_ids_test,a_mask_test,a_seg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating question and answer output from BERT\n",
    "final_text_vector_test = tf.keras.layers.concatenate([question_vect_test,answer_vect_test])\n",
    "\n",
    "print(\"Test text vector shape = \",final_text_vector_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating and reshaping categorical and numerical data for train and cross-validation\n",
    "\n",
    "test_vect_cat_num = np.hstack([category_encoded_test,host_encoded_test,\n",
    "                                    test[['q_title_length','q_body_length','answer_length', 'question_title_neg',\n",
    "                                             'question_title_neu', 'question_title_pos', 'question_title_comp', \n",
    "                                             'question_body_neg','question_body_neu', 'question_body_pos', 'question_body_comp', \n",
    "                                             'answer_neg','answer_neu', 'answer_pos', 'answer_comp']]])\n",
    "\n",
    "test_vect_cat_num = test_vect_cat_num[:,:,newaxis]\n",
    "\n",
    "print(test_vect_cat_num.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict([final_text_vector_test,test_vect_cat_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['qa_id'] = test['qa_id']\n",
    "for i,col in enumerate(class_labels):\n",
    "    submission[col] = y_test_pred[:,i]\n",
    "submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
