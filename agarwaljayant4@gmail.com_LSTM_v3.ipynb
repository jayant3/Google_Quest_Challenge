{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import pickle\n",
    "#from transformers import \n",
    "import numpy as np\n",
    "from numpy import zeros, newaxis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm\n",
    "import random as rn\n",
    "import tensorflow \n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#nltk.download('vader_lexicon')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D,AveragePooling1D,MaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Flatten,Embedding,Input,concatenate,Activation,Dropout,BatchNormalization,LSTM\n",
    "from tensorflow.keras import regularizers,Model\n",
    "from tensorflow.keras.regularizers import l1,l2\n",
    "from tensorflow.keras.callbacks import TensorBoard, Callback, EarlyStopping, ModelCheckpoint,LearningRateScheduler\n",
    "\n",
    "\n",
    "from matplotlib_venn import venn2, venn2_unweighted\n",
    "from matplotlib_venn import venn3, venn3_unweighted\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/47091490/4084039\n",
    "import re\n",
    "\n",
    "def preprocess_text(phrase):\n",
    "    # specific\n",
    "    phrase = str(phrase)\n",
    "    phrase = phrase.lower()\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrade = re.sub(r\"wont\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"cant\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"cannot\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"doesn't\", \"does not\", phrase)\n",
    "    \n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    \n",
    "    phrase = phrase.replace('\\\\r', ' ')\n",
    "    phrase = phrase.replace('\\\\\"', ' ')\n",
    "    phrase = phrase.replace('\\\\n', ' ')\n",
    "    phrase = phrase.replace('\\\\', ' ')\n",
    "    \n",
    "    #phrase = re.sub(r'http\\S+', '', phrase)  # for removal all http link\n",
    "    phrase = re.sub(r'[^A-Za-z0-9]', ' ',phrase) # remove everything except alphaets and numbers\n",
    "    \n",
    "    sent = ''\n",
    "    for val in phrase.split():          # removing multiple spaces between words\n",
    "        sent = sent + ' ' + val\n",
    "    sent = sent.strip()\n",
    "    \n",
    "    return sent\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiments(text_data):\n",
    "    '''\n",
    "    function to generate sentiments from text data\n",
    "    '''\n",
    "        \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    neg=[]\n",
    "    neu=[]\n",
    "    pos=[]\n",
    "    comp=[]\n",
    "    \n",
    "    for txt in text_data:\n",
    "        ss = sid.polarity_scores(txt)\n",
    "        neg.append(ss['neg'])\n",
    "        neu.append(ss['neu'])\n",
    "        pos.append(ss['pos'])\n",
    "        comp.append(ss['compound'])\n",
    "        \n",
    "    sentiment_ = dict()\n",
    "    sentiment_['neg'] = np.array(neg)[:,newaxis]\n",
    "    sentiment_['neu'] = np.array(neu)[:,newaxis]\n",
    "    sentiment_['pos'] = np.array(pos)[:,newaxis]\n",
    "    sentiment_['comp'] = np.array(comp)[:,newaxis]\n",
    "\n",
    "    return sentiment_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countVectorizer(df_train,df_cv,column):\n",
    "    '''\n",
    "    function to vectorize categorical data\n",
    "    '''\n",
    "    # one-hot encoding 'category' feature\n",
    "    vect = CountVectorizer(binary=True)\n",
    "    vect.fit(df_train[column])\n",
    "    encoded_train = vect.transform(df_train[column]).todense()\n",
    "    encoded_cv = vect.transform(df_cv[column]).todense()\n",
    "    \n",
    "    return encoded_train, encoded_cv, vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "def spearman(y_true,y_pred):\n",
    "    '''\n",
    "    function to calculate mean spearman correlation of all 30 class-labels\n",
    "    '''\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    spearman_y = []\n",
    "    for i in range(30):\n",
    "        score = spearmanr(y_true[:,i], y_pred[:,i] + np.random.normal(0, 1e-7, y_pred.shape[0]) , \n",
    "                                                                        nan_policy='omit').correlation\n",
    "                                                                        \n",
    "\n",
    "        spearman_y.append(score)\n",
    "    mean_score = np.nanmean(spearman_y)\n",
    "    return mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_LSTM(input_data):\n",
    "    '''\n",
    "    function to predict on raw data input\n",
    "    input type should be an list/array of datapoints\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #loading saved files\n",
    "    with open('../input/lstm-output/category_vectorizer', 'rb') as f:\n",
    "        category_vectorizer = pickle.load(f)\n",
    "        \n",
    "    with open('../input/lstm-output/host_vectorizer', 'rb') as f:\n",
    "        host_vectorizer = pickle.load(f)\n",
    "        \n",
    "    with open('../input/lstm-output/tokenizer_title', 'rb') as f:\n",
    "        tokenizer_title = pickle.load(f)\n",
    "        \n",
    "    with open('../input/lstm-output/tokenizer_body', 'rb') as f:\n",
    "        tokenizer_body = pickle.load(f)\n",
    "        \n",
    "    with open('../input/lstm-output/tokenizer_answer', 'rb') as f:\n",
    "        tokenizer_answer = pickle.load(f)\n",
    "        \n",
    "    with open('../input/lstm-output/max_length', 'rb') as f:\n",
    "        max_length = pickle.load(f)   \n",
    "                  \n",
    "    with open(\"../input/lstm-output/model.json\", \"r\") as json_file:\n",
    "        model_json = json_file.read()\n",
    "        \n",
    "    model = tf.keras.models.model_from_json(model_json)\n",
    "    model.load_weights(\"../input/lstm-output/weights_lstm.best_copy.hdf5\") \n",
    "    \n",
    "    input_data = np.array(input_data) # converting input to a numpy array (if it is given as a list)\n",
    "\n",
    "    #preprocessing host\n",
    "    host = pd.Series(input_data[:,10]).apply(lambda x: x.split('.')[-2])\n",
    "    host = host.apply(lambda x: x.lower())\n",
    "    host = host.apply(lambda x: x.strip())\n",
    "    #=================================================================================================\n",
    "    # preprocessing 'category'  to lower-case and stripping leading and tailing spaces \n",
    "    category = pd.Series(input_data[:,9]).apply(lambda x: x.lower())\n",
    "    category = category.apply(lambda x: x.strip())\n",
    "    #=================================================================================================\n",
    "    \n",
    "    #encoding categorical data - 'category' and 'host'\n",
    "    category_encoded = category_vectorizer.transform(category).todense()\n",
    "    host_encoded = host_vectorizer.transform(host).todense()\n",
    "\n",
    "    #=================================================================================================\n",
    "    \n",
    "    # preprocessing question title, question body and answers\n",
    "    question_title = pd.Series(input_data[:,1]).apply(lambda x: preprocess_text(x))\n",
    "    question_body = pd.Series(input_data[:,2]).apply(lambda x: preprocess_text(x))\n",
    "    answer = pd.Series(input_data[:,5]).apply(lambda x: preprocess_text(x))\n",
    "\n",
    "    #=================================================================================================\n",
    "    \n",
    "    # taking length of question title, question body, answer\n",
    "    q_title_length = np.array(question_title.apply(lambda x: len(x.split(' '))))[:,newaxis]\n",
    "    q_body_length = np.array(question_body.apply(lambda x: len(x.split(' '))))[:,newaxis]\n",
    "    answer_length = np.array(answer.apply(lambda x: len(x.split(' '))))[:,newaxis]\n",
    "    \n",
    "    #==================================================================================================\n",
    "    \n",
    "    #creating sentiments features\n",
    "    \n",
    "    title_sentiments = get_sentiments(question_title)\n",
    "    body_sentiments = get_sentiments(question_body)\n",
    "    answer_sentiments = get_sentiments(answer)\n",
    "    \n",
    "    # encoding text data\n",
    "    \n",
    "    # encoding question title to sequence and paddinh with zeros\n",
    "    encoded_title_test = tokenizer_title.texts_to_sequences(question_title)\n",
    "    padded_title_test = pad_sequences(encoded_title_test, maxlen = max_length['title'], padding = 'post')\n",
    "    \n",
    "    # encoding question body to sequence and paddinh with zeros\n",
    "    encoded_body_test = tokenizer_body.texts_to_sequences(question_body)\n",
    "    padded_body_test = pad_sequences(encoded_body_test, maxlen = max_length['body'], padding = 'post')\n",
    "    \n",
    "    # encoding answer to sequence and paddinh with zeros\n",
    "    encoded_answer_test = tokenizer_answer.texts_to_sequences(answer)\n",
    "    padded_answer_test = pad_sequences(encoded_answer_test, maxlen = max_length['answer'], padding = 'post')\n",
    "    \n",
    "    \n",
    "    x_test_cat_num  =  np.hstack([category_encoded,host_encoded,\n",
    "                        q_title_length,q_body_length,answer_length, title_sentiments['neg'],\n",
    "                              title_sentiments['neu'], title_sentiments['pos'], title_sentiments['comp'], \n",
    "                              body_sentiments['neg'],body_sentiments['neu'], body_sentiments['pos'], body_sentiments['comp'], \n",
    "                              answer_sentiments['neg'],answer_sentiments['neu'], answer_sentiments['pos'], answer_sentiments['comp']\n",
    "                             ])\n",
    "\n",
    "    x_test_cat_num = x_test_cat_num[:,:,newaxis]\n",
    "    \n",
    "    #======================================================================================================\n",
    "    \n",
    "    prediction = model.predict([padded_title_test,padded_body_test,padded_answer_test,x_test_cat_num])\n",
    "\n",
    "    prediction = np.hstack([input_data[:,0][:,newaxis],prediction]) #giving \"qa_id\" as 1st column to identify the predictions\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting on data\n",
    "train_data = pd.read_csv(\"../input/google-quest-challenge/train.csv\") #reading  data\n",
    "train_data = np.array(train_data) \n",
    "train_output = predict_LSTM(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0.9353364706039429, 0.6771138906478882, ...,\n",
       "        0.1706579029560089, 0.6841882467269897, 0.904914140701294],\n",
       "       [1, 0.9555881023406982, 0.7461124658584595, ...,\n",
       "        0.0507732629776001, 0.7087625861167908, 0.9104382991790771],\n",
       "       [2, 0.8828686475753784, 0.45747336745262146, ...,\n",
       "        0.1713494062423706, 0.6756827235221863, 0.9028689861297607],\n",
       "       ...,\n",
       "       [9645, 0.882625162601471, 0.48956966400146484, ...,\n",
       "        0.10914880037307739, 0.19130933284759521, 0.8575698137283325],\n",
       "       [9646, 0.9379218816757202, 0.6494990587234497, ...,\n",
       "        0.08006134629249573, 0.7052682638168335, 0.9089163541793823],\n",
       "       [9647, 0.9585661888122559, 0.782730221748352, ...,\n",
       "        0.053153425455093384, 0.7830450534820557, 0.9137704372406006]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing output from predict_CNN function\n",
    "train_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6079, 30)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting actual value of class-labels\n",
    "actual_train = train_data[:,11:]\n",
    "actual_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2941428947186191"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spearman's correlation on train data\n",
    "spearman(actual_train,train_output[:,1:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
