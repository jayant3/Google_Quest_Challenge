{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrKDUXAgtcWf"
   },
   "source": [
    "## Google QUEST Q&A Labeling\n",
    "\n",
    "Improving automated understanding of complex question answer content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Oilow17xtcWi"
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import pickle\n",
    "#from transformers import \n",
    "import numpy as np\n",
    "from numpy import zeros\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm\n",
    "import random as rn\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#nltk.download('vader_lexicon')\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Conv1D,AveragePooling1D,MaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Flatten,Embedding,Input,concatenate,Activation,Dropout,BatchNormalization,LSTM\n",
    "from tensorflow.keras import regularizers,Model\n",
    "from tensorflow.keras.regularizers import l1,l2\n",
    "from tensorflow.keras.callbacks import TensorBoard, Callback, EarlyStopping, ModelCheckpoint,LearningRateScheduler\n",
    "\n",
    "\n",
    "from matplotlib_venn import venn2, venn2_unweighted\n",
    "from matplotlib_venn import venn3, venn3_unweighted\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nw9r32lNtcW5",
    "outputId": "2880800f-5168-4b2a-e753-612d873b1462"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6079, 41)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_title</th>\n",
       "      <th>question_body</th>\n",
       "      <th>question_user_name</th>\n",
       "      <th>question_user_page</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_user_name</th>\n",
       "      <th>answer_user_page</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>...</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What am I losing when using extension tubes in...</td>\n",
       "      <td>After playing around with macro photography on...</td>\n",
       "      <td>ysap</td>\n",
       "      <td>https://photo.stackexchange.com/users/1024</td>\n",
       "      <td>I just got extension tubes, so here's the skin...</td>\n",
       "      <td>rfusca</td>\n",
       "      <td>https://photo.stackexchange.com/users/1917</td>\n",
       "      <td>http://photo.stackexchange.com/questions/9169/...</td>\n",
       "      <td>LIFE_ARTS</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the distinction between a city and a s...</td>\n",
       "      <td>I am trying to understand what kinds of places...</td>\n",
       "      <td>russellpierce</td>\n",
       "      <td>https://rpg.stackexchange.com/users/8774</td>\n",
       "      <td>It might be helpful to look into the definitio...</td>\n",
       "      <td>Erik Schmidt</td>\n",
       "      <td>https://rpg.stackexchange.com/users/1871</td>\n",
       "      <td>http://rpg.stackexchange.com/questions/47820/w...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Maximum protusion length for through-hole comp...</td>\n",
       "      <td>I'm working on a PCB that has through-hole com...</td>\n",
       "      <td>Joe Baker</td>\n",
       "      <td>https://electronics.stackexchange.com/users/10157</td>\n",
       "      <td>Do you even need grooves?  We make several pro...</td>\n",
       "      <td>Dwayne Reid</td>\n",
       "      <td>https://electronics.stackexchange.com/users/64754</td>\n",
       "      <td>http://electronics.stackexchange.com/questions...</td>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>...</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Can an affidavit be used in Beit Din?</td>\n",
       "      <td>An affidavit, from what i understand, is basic...</td>\n",
       "      <td>Scimonster</td>\n",
       "      <td>https://judaism.stackexchange.com/users/5151</td>\n",
       "      <td>Sending an \"affidavit\" it is a dispute between...</td>\n",
       "      <td>Y     e     z</td>\n",
       "      <td>https://judaism.stackexchange.com/users/4794</td>\n",
       "      <td>http://judaism.stackexchange.com/questions/551...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>How do you make a binary image in Photoshop?</td>\n",
       "      <td>I am trying to make a binary image. I want mor...</td>\n",
       "      <td>leigero</td>\n",
       "      <td>https://graphicdesign.stackexchange.com/users/...</td>\n",
       "      <td>Check out Image Trace in Adobe Illustrator. \\n...</td>\n",
       "      <td>q2ra</td>\n",
       "      <td>https://graphicdesign.stackexchange.com/users/...</td>\n",
       "      <td>http://graphicdesign.stackexchange.com/questio...</td>\n",
       "      <td>LIFE_ARTS</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id                                     question_title  \\\n",
       "0      0  What am I losing when using extension tubes in...   \n",
       "1      1  What is the distinction between a city and a s...   \n",
       "2      2  Maximum protusion length for through-hole comp...   \n",
       "3      3              Can an affidavit be used in Beit Din?   \n",
       "4      5       How do you make a binary image in Photoshop?   \n",
       "\n",
       "                                       question_body question_user_name  \\\n",
       "0  After playing around with macro photography on...               ysap   \n",
       "1  I am trying to understand what kinds of places...      russellpierce   \n",
       "2  I'm working on a PCB that has through-hole com...          Joe Baker   \n",
       "3  An affidavit, from what i understand, is basic...         Scimonster   \n",
       "4  I am trying to make a binary image. I want mor...            leigero   \n",
       "\n",
       "                                  question_user_page  \\\n",
       "0         https://photo.stackexchange.com/users/1024   \n",
       "1           https://rpg.stackexchange.com/users/8774   \n",
       "2  https://electronics.stackexchange.com/users/10157   \n",
       "3       https://judaism.stackexchange.com/users/5151   \n",
       "4  https://graphicdesign.stackexchange.com/users/...   \n",
       "\n",
       "                                              answer answer_user_name  \\\n",
       "0  I just got extension tubes, so here's the skin...           rfusca   \n",
       "1  It might be helpful to look into the definitio...     Erik Schmidt   \n",
       "2  Do you even need grooves?  We make several pro...      Dwayne Reid   \n",
       "3  Sending an \"affidavit\" it is a dispute between...    Y     e     z   \n",
       "4  Check out Image Trace in Adobe Illustrator. \\n...             q2ra   \n",
       "\n",
       "                                    answer_user_page  \\\n",
       "0         https://photo.stackexchange.com/users/1917   \n",
       "1           https://rpg.stackexchange.com/users/1871   \n",
       "2  https://electronics.stackexchange.com/users/64754   \n",
       "3       https://judaism.stackexchange.com/users/4794   \n",
       "4  https://graphicdesign.stackexchange.com/users/...   \n",
       "\n",
       "                                                 url   category  ...  \\\n",
       "0  http://photo.stackexchange.com/questions/9169/...  LIFE_ARTS  ...   \n",
       "1  http://rpg.stackexchange.com/questions/47820/w...    CULTURE  ...   \n",
       "2  http://electronics.stackexchange.com/questions...    SCIENCE  ...   \n",
       "3  http://judaism.stackexchange.com/questions/551...    CULTURE  ...   \n",
       "4  http://graphicdesign.stackexchange.com/questio...  LIFE_ARTS  ...   \n",
       "\n",
       "  question_well_written  answer_helpful  answer_level_of_information  \\\n",
       "0              1.000000        1.000000                     0.666667   \n",
       "1              0.888889        0.888889                     0.555556   \n",
       "2              0.777778        0.777778                     0.555556   \n",
       "3              0.888889        0.833333                     0.333333   \n",
       "4              1.000000        1.000000                     0.666667   \n",
       "\n",
       "   answer_plausible  answer_relevance  answer_satisfaction  \\\n",
       "0          1.000000          1.000000             0.800000   \n",
       "1          0.888889          0.888889             0.666667   \n",
       "2          1.000000          1.000000             0.666667   \n",
       "3          0.833333          1.000000             0.800000   \n",
       "4          1.000000          1.000000             0.800000   \n",
       "\n",
       "   answer_type_instructions  answer_type_procedure  \\\n",
       "0                       1.0               0.000000   \n",
       "1                       0.0               0.000000   \n",
       "2                       0.0               0.333333   \n",
       "3                       0.0               0.000000   \n",
       "4                       1.0               0.000000   \n",
       "\n",
       "   answer_type_reason_explanation  answer_well_written  \n",
       "0                        0.000000             1.000000  \n",
       "1                        0.666667             0.888889  \n",
       "2                        1.000000             0.888889  \n",
       "3                        1.000000             1.000000  \n",
       "4                        1.000000             1.000000  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading train data from csv file\n",
    "\n",
    "train = pd.read_csv(\"../input/google-quest-challenge/train.csv\")\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GNY_Dp85tcXB",
    "outputId": "bac4bea8-3506-4fa3-8a6b-991d5484aee1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['qa_id', 'question_title', 'question_body', 'question_user_name',\n",
       "       'question_user_page', 'answer', 'answer_user_name', 'answer_user_page',\n",
       "       'url', 'category', 'host', 'question_asker_intent_understanding',\n",
       "       'question_body_critical', 'question_conversational',\n",
       "       'question_expect_short_answer', 'question_fact_seeking',\n",
       "       'question_has_commonly_accepted_answer',\n",
       "       'question_interestingness_others', 'question_interestingness_self',\n",
       "       'question_multi_intent', 'question_not_really_a_question',\n",
       "       'question_opinion_seeking', 'question_type_choice',\n",
       "       'question_type_compare', 'question_type_consequence',\n",
       "       'question_type_definition', 'question_type_entity',\n",
       "       'question_type_instructions', 'question_type_procedure',\n",
       "       'question_type_reason_explanation', 'question_type_spelling',\n",
       "       'question_well_written', 'answer_helpful',\n",
       "       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n",
       "       'answer_satisfaction', 'answer_type_instructions',\n",
       "       'answer_type_procedure', 'answer_type_reason_explanation',\n",
       "       'answer_well_written'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LND_lyv1tcXG",
    "outputId": "7c84305a-7030-4127-fdb9-91f96f69b46f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(476, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_title</th>\n",
       "      <th>question_body</th>\n",
       "      <th>question_user_name</th>\n",
       "      <th>question_user_page</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_user_name</th>\n",
       "      <th>answer_user_page</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>host</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>Will leaving corpses lying around upset my pri...</td>\n",
       "      <td>I see questions/information online about how t...</td>\n",
       "      <td>Dylan</td>\n",
       "      <td>https://gaming.stackexchange.com/users/64471</td>\n",
       "      <td>There is no consequence for leaving corpses an...</td>\n",
       "      <td>Nelson868</td>\n",
       "      <td>https://gaming.stackexchange.com/users/97324</td>\n",
       "      <td>http://gaming.stackexchange.com/questions/1979...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>gaming.stackexchange.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>Url link to feature image in the portfolio</td>\n",
       "      <td>I am new to Wordpress. i have issue with Featu...</td>\n",
       "      <td>Anu</td>\n",
       "      <td>https://wordpress.stackexchange.com/users/72927</td>\n",
       "      <td>I think it is possible with custom fields.\\n\\n...</td>\n",
       "      <td>Irina</td>\n",
       "      <td>https://wordpress.stackexchange.com/users/27233</td>\n",
       "      <td>http://wordpress.stackexchange.com/questions/1...</td>\n",
       "      <td>TECHNOLOGY</td>\n",
       "      <td>wordpress.stackexchange.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>Is accuracy, recoil or bullet spread affected ...</td>\n",
       "      <td>To experiment I started a bot game, toggled in...</td>\n",
       "      <td>Konsta</td>\n",
       "      <td>https://gaming.stackexchange.com/users/37545</td>\n",
       "      <td>You do not have armour in the screenshots. Thi...</td>\n",
       "      <td>Damon Smithies</td>\n",
       "      <td>https://gaming.stackexchange.com/users/70641</td>\n",
       "      <td>http://gaming.stackexchange.com/questions/2154...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>gaming.stackexchange.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>Suddenly got an I/O error from my external HDD</td>\n",
       "      <td>I have used my Raspberry Pi as a torrent-serve...</td>\n",
       "      <td>robbannn</td>\n",
       "      <td>https://raspberrypi.stackexchange.com/users/17341</td>\n",
       "      <td>Your Western Digital hard drive is disappearin...</td>\n",
       "      <td>HeatfanJohn</td>\n",
       "      <td>https://raspberrypi.stackexchange.com/users/1311</td>\n",
       "      <td>http://raspberrypi.stackexchange.com/questions...</td>\n",
       "      <td>TECHNOLOGY</td>\n",
       "      <td>raspberrypi.stackexchange.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>Passenger Name - Flight Booking Passenger only...</td>\n",
       "      <td>I have bought Delhi-London return flights for ...</td>\n",
       "      <td>Amit</td>\n",
       "      <td>https://travel.stackexchange.com/users/29089</td>\n",
       "      <td>I called two persons who work for Saudia (tick...</td>\n",
       "      <td>Nean Der Thal</td>\n",
       "      <td>https://travel.stackexchange.com/users/10051</td>\n",
       "      <td>http://travel.stackexchange.com/questions/4704...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>travel.stackexchange.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id                                     question_title  \\\n",
       "0     39  Will leaving corpses lying around upset my pri...   \n",
       "1     46         Url link to feature image in the portfolio   \n",
       "2     70  Is accuracy, recoil or bullet spread affected ...   \n",
       "3    132     Suddenly got an I/O error from my external HDD   \n",
       "4    200  Passenger Name - Flight Booking Passenger only...   \n",
       "\n",
       "                                       question_body question_user_name  \\\n",
       "0  I see questions/information online about how t...              Dylan   \n",
       "1  I am new to Wordpress. i have issue with Featu...                Anu   \n",
       "2  To experiment I started a bot game, toggled in...             Konsta   \n",
       "3  I have used my Raspberry Pi as a torrent-serve...           robbannn   \n",
       "4  I have bought Delhi-London return flights for ...               Amit   \n",
       "\n",
       "                                  question_user_page  \\\n",
       "0       https://gaming.stackexchange.com/users/64471   \n",
       "1    https://wordpress.stackexchange.com/users/72927   \n",
       "2       https://gaming.stackexchange.com/users/37545   \n",
       "3  https://raspberrypi.stackexchange.com/users/17341   \n",
       "4       https://travel.stackexchange.com/users/29089   \n",
       "\n",
       "                                              answer answer_user_name  \\\n",
       "0  There is no consequence for leaving corpses an...        Nelson868   \n",
       "1  I think it is possible with custom fields.\\n\\n...            Irina   \n",
       "2  You do not have armour in the screenshots. Thi...   Damon Smithies   \n",
       "3  Your Western Digital hard drive is disappearin...      HeatfanJohn   \n",
       "4  I called two persons who work for Saudia (tick...    Nean Der Thal   \n",
       "\n",
       "                                   answer_user_page  \\\n",
       "0      https://gaming.stackexchange.com/users/97324   \n",
       "1   https://wordpress.stackexchange.com/users/27233   \n",
       "2      https://gaming.stackexchange.com/users/70641   \n",
       "3  https://raspberrypi.stackexchange.com/users/1311   \n",
       "4      https://travel.stackexchange.com/users/10051   \n",
       "\n",
       "                                                 url    category  \\\n",
       "0  http://gaming.stackexchange.com/questions/1979...     CULTURE   \n",
       "1  http://wordpress.stackexchange.com/questions/1...  TECHNOLOGY   \n",
       "2  http://gaming.stackexchange.com/questions/2154...     CULTURE   \n",
       "3  http://raspberrypi.stackexchange.com/questions...  TECHNOLOGY   \n",
       "4  http://travel.stackexchange.com/questions/4704...     CULTURE   \n",
       "\n",
       "                            host  \n",
       "0       gaming.stackexchange.com  \n",
       "1    wordpress.stackexchange.com  \n",
       "2       gaming.stackexchange.com  \n",
       "3  raspberrypi.stackexchange.com  \n",
       "4       travel.stackexchange.com  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading test data from csv file\n",
    "test = pd.read_csv(\"../input/google-quest-challenge/test.csv\")\n",
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "e5SZONwTtcXK",
    "outputId": "d41185ff-a490-40eb-f728-a7adda030633",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of class labels =  30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['question_asker_intent_understanding',\n",
       " 'question_body_critical',\n",
       " 'question_conversational',\n",
       " 'question_expect_short_answer',\n",
       " 'question_fact_seeking',\n",
       " 'question_has_commonly_accepted_answer',\n",
       " 'question_interestingness_others',\n",
       " 'question_interestingness_self',\n",
       " 'question_multi_intent',\n",
       " 'question_not_really_a_question',\n",
       " 'question_opinion_seeking',\n",
       " 'question_type_choice',\n",
       " 'question_type_compare',\n",
       " 'question_type_consequence',\n",
       " 'question_type_definition',\n",
       " 'question_type_entity',\n",
       " 'question_type_instructions',\n",
       " 'question_type_procedure',\n",
       " 'question_type_reason_explanation',\n",
       " 'question_type_spelling',\n",
       " 'question_well_written',\n",
       " 'answer_helpful',\n",
       " 'answer_level_of_information',\n",
       " 'answer_plausible',\n",
       " 'answer_relevance',\n",
       " 'answer_satisfaction',\n",
       " 'answer_type_instructions',\n",
       " 'answer_type_procedure',\n",
       " 'answer_type_reason_explanation',\n",
       " 'answer_well_written']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.read_csv(\"../input/google-quest-challenge/sample_submission.csv\")\n",
    "class_labels = list(sample.columns)[1:]\n",
    "print(\"Number of class labels = \",len(class_labels))\n",
    "class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wYMSn5rTtcXO",
    "outputId": "24dcbeab-3038-4803-b5d2-c781345f8199"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['question_title',\n",
       " 'question_body',\n",
       " 'question_user_name',\n",
       " 'question_user_page',\n",
       " 'answer',\n",
       " 'answer_user_name',\n",
       " 'answer_user_page',\n",
       " 'url',\n",
       " 'category',\n",
       " 'host']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_col = list(test.columns)\n",
    "input_col.remove('qa_id')\n",
    "input_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qa_id                                    0\n",
       "question_title                           0\n",
       "question_body                            0\n",
       "question_user_name                       0\n",
       "question_user_page                       0\n",
       "answer                                   0\n",
       "answer_user_name                         0\n",
       "answer_user_page                         0\n",
       "url                                      0\n",
       "category                                 0\n",
       "host                                     0\n",
       "question_asker_intent_understanding      0\n",
       "question_body_critical                   0\n",
       "question_conversational                  0\n",
       "question_expect_short_answer             0\n",
       "question_fact_seeking                    0\n",
       "question_has_commonly_accepted_answer    0\n",
       "question_interestingness_others          0\n",
       "question_interestingness_self            0\n",
       "question_multi_intent                    0\n",
       "question_not_really_a_question           0\n",
       "question_opinion_seeking                 0\n",
       "question_type_choice                     0\n",
       "question_type_compare                    0\n",
       "question_type_consequence                0\n",
       "question_type_definition                 0\n",
       "question_type_entity                     0\n",
       "question_type_instructions               0\n",
       "question_type_procedure                  0\n",
       "question_type_reason_explanation         0\n",
       "question_type_spelling                   0\n",
       "question_well_written                    0\n",
       "answer_helpful                           0\n",
       "answer_level_of_information              0\n",
       "answer_plausible                         0\n",
       "answer_relevance                         0\n",
       "answer_satisfaction                      0\n",
       "answer_type_instructions                 0\n",
       "answer_type_procedure                    0\n",
       "answer_type_reason_explanation           0\n",
       "answer_well_written                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking if there are any missing values\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCXk9fNHtcXS"
   },
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-AwxqIr6tcXT",
    "outputId": "c57ff134-d3e8-4fc3-9eb0-979216093b64"
   },
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
    "# plotting Wordcloud of question title\n",
    "\n",
    "comment_words = '' \n",
    "stopwords = set(STOPWORDS) \n",
    "  \n",
    "# iterate through the question title \n",
    "for val in train.question_title: \n",
    "      \n",
    "    # typecaste each val to string \n",
    "    val = str(val) \n",
    "  \n",
    "    # split the value \n",
    "    tokens = val.split() \n",
    "      \n",
    "    # Converts each token into lowercase \n",
    "    for i in range(len(tokens)): \n",
    "        tokens[i] = tokens[i].lower() \n",
    "      \n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    "  \n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='black', \n",
    "                stopwords = stopwords, \n",
    "                min_font_size = 10).generate(comment_words) \n",
    "  \n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YyVVm6eCtcXY"
   },
   "source": [
    "From the above Wordcloud, it can be said that most occuring words in Question title are `using, file, use, value, user, one, time, change, window, function, data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-Ik35qFtcXY",
    "outputId": "ff1848be-f856-46d7-eaf9-87452d2b8ca8"
   },
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
    "# plotting Wordcloud of question body\n",
    "\n",
    "comment_words = '' \n",
    "stopwords = set(STOPWORDS) \n",
    "  \n",
    "# iterate through the question body  \n",
    "for val in train.question_body: \n",
    "      \n",
    "    # typecaste each val to string \n",
    "    val = str(val) \n",
    "  \n",
    "    # split the value \n",
    "    tokens = val.split() \n",
    "      \n",
    "    # Converts each token into lowercase \n",
    "    for i in range(len(tokens)): \n",
    "        tokens[i] = tokens[i].lower() \n",
    "      \n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    "  \n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='black', \n",
    "                stopwords = stopwords, \n",
    "                min_font_size = 10).generate(comment_words) \n",
    "  \n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJMKaZF4tcXc"
   },
   "source": [
    "From the above Wordcloud, it can be said that most occuring words in Question body are `gt, lt, one, will, using, use, example,know, question, new, work, value, problem`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXom3Yk2tcXe",
    "outputId": "ada0dc6c-bf83-4c17-c4f0-abc0940857ac",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plotting Wordcloud of (question body + question title)\n",
    "\n",
    "comment_words = '' \n",
    "stopwords = set(STOPWORDS) \n",
    "\n",
    "# iterate through the question title \n",
    "for val in train.question_title: \n",
    "      \n",
    "    # typecaste each val to string \n",
    "    val = str(val) \n",
    "  \n",
    "    # split the value \n",
    "    tokens = val.split() \n",
    "      \n",
    "    # Converts each token into lowercase \n",
    "    for i in range(len(tokens)): \n",
    "        tokens[i] = tokens[i].lower() \n",
    "      \n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    "\n",
    "# iterate through the question body \n",
    "for val in train.question_body: \n",
    "      \n",
    "    # typecaste each val to string \n",
    "    val = str(val) \n",
    "  \n",
    "    # split the value \n",
    "    tokens = val.split() \n",
    "      \n",
    "    # Converts each token into lowercase \n",
    "    for i in range(len(tokens)): \n",
    "        tokens[i] = tokens[i].lower() \n",
    "      \n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    "\n",
    "\n",
    "  \n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='black', \n",
    "                stopwords = stopwords, \n",
    "                min_font_size = 10).generate(comment_words) \n",
    "  \n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7-y11hOtcXi"
   },
   "source": [
    "Some of the most occuring words in Question body and title combined are `gt, lt, one, will, use, file, want, example, know, question, using, new, work, problem`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTMYwanGtcXi",
    "outputId": "4fab5718-15e0-45c2-a6b4-15167c21835f"
   },
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
    "# plotting Wordcloud of answers\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "\n",
    "comment_words = '' \n",
    "stopwords = set(STOPWORDS) \n",
    "  \n",
    "# iterate through the csv file \n",
    "for val in train.answer: \n",
    "      \n",
    "    # typecaste each val to string \n",
    "    val = str(val) \n",
    "  \n",
    "    # split the value \n",
    "    tokens = val.split() \n",
    "      \n",
    "    # Converts each token into lowercase \n",
    "    for i in range(len(tokens)): \n",
    "        tokens[i] = tokens[i].lower() \n",
    "      \n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    "  \n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='black', \n",
    "                stopwords = stopwords, \n",
    "                min_font_size = 10).generate(comment_words) \n",
    "  \n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9FRwV1ztcXo"
   },
   "source": [
    "From the above Wordcloud, it can be said that most occuring words in Question title are `use, one, will, need, gt, lt, using, file, time, gt, lt, way, make, example, see`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F3Jze1mitcXp"
   },
   "outputs": [],
   "source": [
    "# taking set of all words in question title\n",
    "q_title_set = set([word for question in train.question_title for word in question.split(' ')])\n",
    "\n",
    "# taking set of all words in question body\n",
    "q_body_set = set([word for question in train.question_body for word in question.split(' ')])\n",
    "\n",
    "# taking set of all words in answer\n",
    "answer_set = set([word for answer in train.answer for word in answer.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CedlWEN5tcXs",
    "outputId": "2ec1d1b8-62e6-4724-b7a1-2e8c7f9f43b7"
   },
   "outputs": [],
   "source": [
    "# https://pypi.org/project/matplotlib-venn/\n",
    "# venn diagram of (set of words in Question title) vs (set of words in Question body)\n",
    "\n",
    "venn2([q_title_set,q_body_set], set_labels=(\"Question title\",\"Question body\"))\n",
    "plt.title(\"Venn diagram of (set of words in Question title) vs (set of words in Question body)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RZBo5nYtcXx"
   },
   "source": [
    "##### There is a good overlap between Question title and Question body. Out of 10492 words in Question title, 7027 are common with Question body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k4wZQwFYtcXy",
    "outputId": "3574efb4-6267-449c-b519-872bd88006fc"
   },
   "outputs": [],
   "source": [
    "# venn diagram of (set of words in answer) vs (set of words in Question body)\n",
    "\n",
    "h = venn2([answer_set,q_body_set], set_labels=(\"Answer\",\"Question body\"))\n",
    "plt.title(\"Venn diagram of (set of words in Answer) vs (set of words in Question body)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KH0-mh9etcX3"
   },
   "source": [
    "##### Out of 77578 words in Question body, 25859 words i.e. nearly 33% are common with answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1cSwuVPtcX5",
    "outputId": "44bd0032-4dd5-4ef8-cc3d-09587e4b822f"
   },
   "outputs": [],
   "source": [
    "# venn diagram of (set of words in answer) vs (set of words in Question body)\n",
    "\n",
    "h = venn2([answer_set,q_title_set], set_labels=(\"Answer\",\"Question title\"))\n",
    "plt.title(\"Venn diagram of (set of words in Answer) vs (set of words in Question title)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIaGHLCItcX9"
   },
   "source": [
    "##### Answer and Question title also have a good overlap. 90% of the words in Question title are present in Answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWoFcBKhtcX-",
    "outputId": "0fa46541-34c7-420b-c6c3-228027a8e847"
   },
   "outputs": [],
   "source": [
    "# venn diagram of (set of words in Question title) vs (set of words in Question body) vs (set of words in answer)\n",
    "venn3_unweighted([q_title_set,q_body_set,answer_set],('Question title', 'Question body', 'Answer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jI6yhDIYtcYC",
    "outputId": "72edb1ff-456c-4409-b8b8-f4e5246f7db4"
   },
   "outputs": [],
   "source": [
    "#plot of category count \n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "ax = sns.countplot(train.category, order=train.category.value_counts().index)\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Number of Questions\")\n",
    "plt.title(\"Category vs Number of Questions\")\n",
    "\n",
    "total = train.shape[0]\n",
    "\n",
    "for p in ax.patches:\n",
    "        ax.annotate('{:.1f}%'.format(100*p.get_height()/total), (p.get_x()+0.25, p.get_height()))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPjqPRBgtcYF"
   },
   "source": [
    "##### Maximum number of questions have category `TECHNOLOGY` which is 40.2% of total number of questions.\n",
    "##### 20.6% of total questions are `STACKOVERFLOW`.\n",
    "##### 15.6% questions falls under `CULTURE`.\n",
    "##### `SCIENCE` and `LIFE_ARTS` are minimum with 11.7% each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MdOslhLXtcYG"
   },
   "outputs": [],
   "source": [
    "# preprocessing host name \n",
    "\n",
    "train['host'] = train['host'].apply(lambda x: x.split('.')[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ncBvRKOltcYP",
    "outputId": "9cccb080-4bbc-472f-9782-f197a0b46cae"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot of host count \n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "ax = sns.countplot(train.host, order=train.host.value_counts().index)\n",
    "plt.xlabel(\"Host\")\n",
    "plt.ylabel(\"Number of Questions\")\n",
    "plt.title(\"Host vs Number of Questions\")\n",
    "\n",
    "total = train.shape[0]\n",
    "\n",
    "for p in ax.patches:\n",
    "        ax.annotate('{:.1f}%'.format(100*p.get_height()/total), (p.get_x()+0.25, p.get_height()))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enqwVHUdtcYU"
   },
   "source": [
    "##### The above plot shows that `stackexchange` hosted maximum questions with 68.6% of total questions.\n",
    "##### Host with 2nd highest number of questions is `stackoverflow`.\n",
    "##### Other hosts are`superuser` with 3.7%, `serverfault` with 3.5%, `askubuntu` with 2.1% and `mathoverflow` 1.3%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FUtUTgl1tcYU",
    "outputId": "768c2da4-4951-4829-e624-79fed3e17401"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_title</th>\n",
       "      <th>question_body</th>\n",
       "      <th>question_user_name</th>\n",
       "      <th>question_user_page</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_user_name</th>\n",
       "      <th>answer_user_page</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>...</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "      <th>q_title_length</th>\n",
       "      <th>q_body_length</th>\n",
       "      <th>answer_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What am I losing when using extension tubes in...</td>\n",
       "      <td>After playing around with macro photography on...</td>\n",
       "      <td>ysap</td>\n",
       "      <td>https://photo.stackexchange.com/users/1024</td>\n",
       "      <td>I just got extension tubes, so here's the skin...</td>\n",
       "      <td>rfusca</td>\n",
       "      <td>https://photo.stackexchange.com/users/1917</td>\n",
       "      <td>http://photo.stackexchange.com/questions/9169/...</td>\n",
       "      <td>LIFE_ARTS</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>139</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the distinction between a city and a s...</td>\n",
       "      <td>I am trying to understand what kinds of places...</td>\n",
       "      <td>russellpierce</td>\n",
       "      <td>https://rpg.stackexchange.com/users/8774</td>\n",
       "      <td>It might be helpful to look into the definitio...</td>\n",
       "      <td>Erik Schmidt</td>\n",
       "      <td>https://rpg.stackexchange.com/users/1871</td>\n",
       "      <td>http://rpg.stackexchange.com/questions/47820/w...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>16</td>\n",
       "      <td>144</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Maximum protusion length for through-hole comp...</td>\n",
       "      <td>I'm working on a PCB that has through-hole com...</td>\n",
       "      <td>Joe Baker</td>\n",
       "      <td>https://electronics.stackexchange.com/users/10157</td>\n",
       "      <td>Do you even need grooves?  We make several pro...</td>\n",
       "      <td>Dwayne Reid</td>\n",
       "      <td>https://electronics.stackexchange.com/users/64754</td>\n",
       "      <td>http://electronics.stackexchange.com/questions...</td>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>7</td>\n",
       "      <td>117</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Can an affidavit be used in Beit Din?</td>\n",
       "      <td>An affidavit, from what i understand, is basic...</td>\n",
       "      <td>Scimonster</td>\n",
       "      <td>https://judaism.stackexchange.com/users/5151</td>\n",
       "      <td>Sending an \"affidavit\" it is a dispute between...</td>\n",
       "      <td>Y     e     z</td>\n",
       "      <td>https://judaism.stackexchange.com/users/4794</td>\n",
       "      <td>http://judaism.stackexchange.com/questions/551...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>73</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>How do you make a binary image in Photoshop?</td>\n",
       "      <td>I am trying to make a binary image. I want mor...</td>\n",
       "      <td>leigero</td>\n",
       "      <td>https://graphicdesign.stackexchange.com/users/...</td>\n",
       "      <td>Check out Image Trace in Adobe Illustrator. \\n...</td>\n",
       "      <td>q2ra</td>\n",
       "      <td>https://graphicdesign.stackexchange.com/users/...</td>\n",
       "      <td>http://graphicdesign.stackexchange.com/questio...</td>\n",
       "      <td>LIFE_ARTS</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>80</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id                                     question_title  \\\n",
       "0      0  What am I losing when using extension tubes in...   \n",
       "1      1  What is the distinction between a city and a s...   \n",
       "2      2  Maximum protusion length for through-hole comp...   \n",
       "3      3              Can an affidavit be used in Beit Din?   \n",
       "4      5       How do you make a binary image in Photoshop?   \n",
       "\n",
       "                                       question_body question_user_name  \\\n",
       "0  After playing around with macro photography on...               ysap   \n",
       "1  I am trying to understand what kinds of places...      russellpierce   \n",
       "2  I'm working on a PCB that has through-hole com...          Joe Baker   \n",
       "3  An affidavit, from what i understand, is basic...         Scimonster   \n",
       "4  I am trying to make a binary image. I want mor...            leigero   \n",
       "\n",
       "                                  question_user_page  \\\n",
       "0         https://photo.stackexchange.com/users/1024   \n",
       "1           https://rpg.stackexchange.com/users/8774   \n",
       "2  https://electronics.stackexchange.com/users/10157   \n",
       "3       https://judaism.stackexchange.com/users/5151   \n",
       "4  https://graphicdesign.stackexchange.com/users/...   \n",
       "\n",
       "                                              answer answer_user_name  \\\n",
       "0  I just got extension tubes, so here's the skin...           rfusca   \n",
       "1  It might be helpful to look into the definitio...     Erik Schmidt   \n",
       "2  Do you even need grooves?  We make several pro...      Dwayne Reid   \n",
       "3  Sending an \"affidavit\" it is a dispute between...    Y     e     z   \n",
       "4  Check out Image Trace in Adobe Illustrator. \\n...             q2ra   \n",
       "\n",
       "                                    answer_user_page  \\\n",
       "0         https://photo.stackexchange.com/users/1917   \n",
       "1           https://rpg.stackexchange.com/users/1871   \n",
       "2  https://electronics.stackexchange.com/users/64754   \n",
       "3       https://judaism.stackexchange.com/users/4794   \n",
       "4  https://graphicdesign.stackexchange.com/users/...   \n",
       "\n",
       "                                                 url   category  ...  \\\n",
       "0  http://photo.stackexchange.com/questions/9169/...  LIFE_ARTS  ...   \n",
       "1  http://rpg.stackexchange.com/questions/47820/w...    CULTURE  ...   \n",
       "2  http://electronics.stackexchange.com/questions...    SCIENCE  ...   \n",
       "3  http://judaism.stackexchange.com/questions/551...    CULTURE  ...   \n",
       "4  http://graphicdesign.stackexchange.com/questio...  LIFE_ARTS  ...   \n",
       "\n",
       "  answer_plausible  answer_relevance  answer_satisfaction  \\\n",
       "0         1.000000          1.000000             0.800000   \n",
       "1         0.888889          0.888889             0.666667   \n",
       "2         1.000000          1.000000             0.666667   \n",
       "3         0.833333          1.000000             0.800000   \n",
       "4         1.000000          1.000000             0.800000   \n",
       "\n",
       "   answer_type_instructions  answer_type_procedure  \\\n",
       "0                       1.0               0.000000   \n",
       "1                       0.0               0.000000   \n",
       "2                       0.0               0.333333   \n",
       "3                       0.0               0.000000   \n",
       "4                       1.0               0.000000   \n",
       "\n",
       "   answer_type_reason_explanation  answer_well_written  q_title_length  \\\n",
       "0                        0.000000             1.000000              13   \n",
       "1                        0.666667             0.888889              16   \n",
       "2                        1.000000             0.888889               7   \n",
       "3                        1.000000             1.000000               8   \n",
       "4                        1.000000             1.000000               9   \n",
       "\n",
       "   q_body_length  answer_length  \n",
       "0            139            149  \n",
       "1            144             72  \n",
       "2            117            188  \n",
       "3             73            258  \n",
       "4             80             24  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking length of question title, question body, answer\n",
    "train['q_title_length'] = train['question_title'].apply(lambda x: len(x.split(' ')))\n",
    "train['q_body_length'] = train['question_body'].apply(lambda x: len(x.split(' ')))\n",
    "train['answer_length'] = train['answer'].apply(lambda x: len(x.split(' ')))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kb9--aTutcYa",
    "outputId": "d35c0815-21fb-40d4-b280-cf3e67dfbb4b"
   },
   "outputs": [],
   "source": [
    "# kde plot of length of question title\n",
    "fig = plt.figure(figsize=plt.figaspect(.3))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "sns.kdeplot(train['q_title_length'], shade=True, ax=ax1)\n",
    "plt.xlabel('Length of Question title')\n",
    "plt.grid()\n",
    "plt.title(\"PDF\")\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "sns.kdeplot(train['q_title_length'], shade=True, cumulative=True, ax=ax2)\n",
    "plt.xlabel('Length of Question title')\n",
    "plt.title('CDF')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICy0iO6itcYg"
   },
   "source": [
    "##### In the PDF plot, it can be observed that maximum number of questions titles have length around 7 or 8.\n",
    "##### CDF plot shows that 90% of questions have length less than 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4pSChH7tcYh",
    "outputId": "ed16cd30-7ed2-4f31-854b-74cdfd1c1e82"
   },
   "outputs": [],
   "source": [
    "# kde plot of length of question title\n",
    "fig = plt.figure(figsize=plt.figaspect(.3))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "sns.kdeplot(train['q_body_length'], shade=True, ax=ax1)\n",
    "plt.xlabel('Length of Question body')\n",
    "plt.grid()\n",
    "plt.title(\"PDF\")\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "sns.kdeplot(train['q_body_length'], shade=True, cumulative=True, ax=ax2)\n",
    "plt.xlabel('Length of Question body')\n",
    "plt.grid()\n",
    "plt.title('CDF')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_be3jp3otcYl"
   },
   "source": [
    "##### In the PDF plot, it can be observed that almost all questions body have length less than 500. But there are few questions body with length around 1000.\n",
    "##### CDF plot shows that more than 90% of questions have length less than 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5SS0dJ8-tcYl",
    "outputId": "3c1f002b-a524-4bc5-bcfb-e4ba565c190e"
   },
   "outputs": [],
   "source": [
    "# kde plot of length of question title\n",
    "fig = plt.figure(figsize=plt.figaspect(.3))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "sns.kdeplot(train['answer_length'], shade=True, ax=ax1)\n",
    "plt.xlabel('Length of answers')\n",
    "plt.grid()\n",
    "plt.title(\"PDF\")\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "sns.kdeplot(train['answer_length'], shade=True, cumulative=True, ax=ax2)\n",
    "plt.xlabel('Length of answers')\n",
    "plt.grid()\n",
    "plt.title('CDF')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSK-AEbbtcYu"
   },
   "source": [
    "##### In the PDF plot, it can be observed that almost all answersy have length less than 1000. But there are few questions body with length around 2000 and even 8000.\n",
    "##### CDF plot shows that more than 90% of questions have length less than 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HhiY6PoZtcYy",
    "outputId": "058f6cb4-f19b-48b4-e06b-84e3e8b96fdf"
   },
   "outputs": [],
   "source": [
    "# kde plot of question_asker_intent_understanding\n",
    "fig = plt.figure(figsize=plt.figaspect(.3))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "sns.kdeplot(train['question_asker_intent_understanding'], shade=True, ax=ax1)\n",
    "plt.xlabel('Questions asker intent')\n",
    "plt.grid()\n",
    "plt.title(\"PDF\")\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "sns.kdeplot(train['question_asker_intent_understanding'], shade=True, cumulative=True, ax=ax2)\n",
    "plt.xlabel('Questions asker intent')\n",
    "plt.grid()\n",
    "plt.title('CDF')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sn49udr-tcY1"
   },
   "source": [
    "##### Around 25% of the questions have intent score less than 0.8 i.e. most of the question's intent are clear upto good extent.\n",
    "##### Also maximum questions have intent score around 1 and 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qjZXlonrtcY2",
    "outputId": "35ce76f4-6784-418e-df7d-88b6db30beca"
   },
   "outputs": [],
   "source": [
    "# kde plot of question_type_reason_explanation\n",
    "fig = plt.figure(figsize=plt.figaspect(.3))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "sns.kdeplot(train['question_type_reason_explanation'], shade=True, ax=ax1)\n",
    "plt.xlabel('Questions reason explanation')\n",
    "plt.grid()\n",
    "plt.title(\"PDF\")\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "sns.kdeplot(train['question_type_reason_explanation'], shade=True, cumulative=True, ax=ax2)\n",
    "plt.xlabel('Questions reason explanation')\n",
    "plt.grid()\n",
    "plt.title('CDF')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGqcnx5htcY7"
   },
   "source": [
    "##### Maximum questions type have reason explanation score 0.\n",
    "##### 80% of the questions have reason explanation score less than 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EiVkRYVitcY7",
    "outputId": "a510a3cf-fc46-407b-88e3-abbf44cd2814"
   },
   "outputs": [],
   "source": [
    "# kde plot of question_well_written\n",
    "fig = plt.figure(figsize=plt.figaspect(.3))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "sns.kdeplot(train['question_well_written'], shade=True, ax=ax1)\n",
    "plt.xlabel('Questions well written')\n",
    "plt.grid()\n",
    "plt.title(\"PDF\")\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "sns.kdeplot(train['question_well_written'], shade=True, cumulative=True, ax=ax2)\n",
    "plt.xlabel('Questions well written')\n",
    "plt.grid()\n",
    "plt.title('CDF')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bcCdH2ItcY-"
   },
   "source": [
    "#### Plot shows that most of the questions are well written with score between 0.8 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QhL-NsF7tcY_",
    "outputId": "01a086ab-62b5-4c48-cb5b-747a72f21560"
   },
   "outputs": [],
   "source": [
    "# count of each score for \"question_not_really_a_question\"\n",
    "train['question_not_really_a_question'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3R5gp0vYtcZD"
   },
   "source": [
    "##### There are very few question who are not really a question. There are 3 questions having score 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDH8IGBUtcZE",
    "outputId": "8c1cdb38-ef2c-44f2-9bd2-d10e9e091516"
   },
   "outputs": [],
   "source": [
    "# kde plot of question_interestingness_others\n",
    "fig = plt.figure(figsize=plt.figaspect(.3))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "sns.kdeplot(train['question_interestingness_others'],  ax=ax1, color='green')\n",
    "sns.kdeplot(train['question_interestingness_self'], ax=ax1, color='gold')\n",
    "plt.xlabel('Questions interestingness')\n",
    "plt.grid()\n",
    "plt.title(\"PDF\")\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "sns.kdeplot(train['question_interestingness_others'], cumulative=True, ax=ax2, color = 'green')\n",
    "sns.kdeplot(train['question_interestingness_self'], cumulative=True, ax=ax2, color= 'gold')\n",
    "plt.xlabel('Questions interestingness')\n",
    "plt.grid()\n",
    "plt.title('CDF')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GglXSGRtcZJ"
   },
   "source": [
    "##### Most of  the questions have high score for question_interestingness_others, than question_interestingness_self  scores. That means most of the questions are more interesting to others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXy3PuF3tcZK",
    "outputId": "6d9bd28c-a8c7-4ec2-d06c-f81eb7db4cf6"
   },
   "outputs": [],
   "source": [
    "# kde plot of Question expect short answer\n",
    "fig = plt.figure(figsize=plt.figaspect(.3))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "sns.kdeplot(train['question_expect_short_answer'], shade=True, ax=ax1)\n",
    "plt.xlabel('Question expect short answer')\n",
    "plt.grid()\n",
    "plt.title(\"PDF\")\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "sns.kdeplot(train['question_expect_short_answer'], shade=True, cumulative=True, ax=ax2)\n",
    "plt.xlabel('Question expect short answer')\n",
    "plt.grid()\n",
    "plt.title('CDF')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IefRfz2ztcZP"
   },
   "source": [
    "##### 50% of the question_asker_intent_understanding have value less than 0.8, but most of the questions have value 1. There are few questions with value 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "325-_pXZtcZQ",
    "outputId": "2cc5d6f6-fee1-4311-ba21-0341befe76b6"
   },
   "outputs": [],
   "source": [
    "# kde plot of answer_helpful\n",
    "fig = plt.figure(figsize=plt.figaspect(.3))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "sns.kdeplot(train['answer_helpful'], shade=True, ax=ax1)\n",
    "plt.xlabel('Answer helpful rating')\n",
    "plt.grid()\n",
    "plt.title(\"PDF\")\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "sns.kdeplot(train['answer_helpful'], shade=True, cumulative=True, ax=ax2)\n",
    "plt.xlabel('Answer helpful rating')\n",
    "plt.grid()\n",
    "plt.title('CDF')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExyXIkqftcZT"
   },
   "source": [
    "##### From the above plots, it can be said that most of the answers have helpful score 1. \n",
    "##### Around 35% of the answers have answer_helpful score less than 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gffui5GHtcZU",
    "outputId": "58dd3b39-21d7-498f-c78f-f54cc1acdc5b"
   },
   "outputs": [],
   "source": [
    "# kde plot of answer_satisfaction\n",
    "fig = plt.figure(figsize=plt.figaspect(.3))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "sns.kdeplot(train['answer_satisfaction'], shade=True, ax=ax1)\n",
    "plt.xlabel('Answer satisfaction rating')\n",
    "plt.grid()\n",
    "plt.title(\"PDF\")\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "sns.kdeplot(train['answer_satisfaction'], shade=True, cumulative=True, ax=ax2)\n",
    "plt.xlabel('Answer satisfaction rating')\n",
    "plt.grid()\n",
    "plt.title('CDF')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzwgHzVTtcZX"
   },
   "source": [
    "##### Only 30% of the answers have answer_satisfaction score less than 0.8.  Most of the answers have score between 0.8 to 1. So maximum number of answers are satisfactory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vihdGGD-tcZY",
    "outputId": "dd58980c-4642-48de-c0c1-91726b1c5dc7"
   },
   "outputs": [],
   "source": [
    "# kde plot of answer_well_written\n",
    "fig = plt.figure(figsize=plt.figaspect(.3))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "sns.kdeplot(train['answer_well_written'], shade=True, ax=ax1)\n",
    "plt.xlabel('Answer well written score')\n",
    "plt.grid()\n",
    "plt.title(\"PDF\")\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "sns.kdeplot(train['answer_well_written'], shade=True, cumulative=True, ax=ax2)\n",
    "plt.xlabel('Answer well written score')\n",
    "plt.grid()\n",
    "plt.title('CDF')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-FnMEMrtcZd"
   },
   "source": [
    "##### Most of the questions have well written score of 0.9 and 1.\n",
    "##### Only around 18% of the questions have well written score less than 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZUKL0_dtcZf",
    "outputId": "d5c8797f-8a8f-4060-9d33-d8005628a128"
   },
   "outputs": [],
   "source": [
    "# BoxPlot of question_asker_intent_understanding\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.grid()\n",
    "sns.boxplot(y=train.question_asker_intent_understanding,x=train.category, orient='v')\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Question asker's intent\")\n",
    "plt.title(\"Category vs Question asker's intent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99nDtOyHtcZh"
   },
   "source": [
    "Most of the `life_arts` category questions have a good understanding of question asker's intent with a score between 0.8 to 1.\n",
    "\n",
    "`culture` category questions also have the same scores as `life_arts`\n",
    "\n",
    "Categories `science, stackoverflow, technology` have 75% of the questions asker's intent score between 0.78 to 1, but there are still 25% questions with score between 0.3 to 0.75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5YIuBo_EtcZk",
    "outputId": "87a0302d-2948-492e-d0ca-cd20f0774bee"
   },
   "outputs": [],
   "source": [
    "# BoxPlot of question_body_critical\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.grid()\n",
    "sns.boxplot(y=train.question_conversational,x=train.category, orient='v')\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Conversational Questions\")\n",
    "plt.title(\"Category vs Conversational Questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pgmrN6ztcZo"
   },
   "source": [
    "##### Can't get enough info out of above plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GiOp0I10tcZp",
    "outputId": "2d328a65-42a0-4067-fffb-8625e20a80ce"
   },
   "outputs": [],
   "source": [
    "# BoxPlot of question_expect_short_answer\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.grid()\n",
    "sns.boxplot(y=train.question_expect_short_answer,x=train.category, orient='v')\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"question_expect_short_answer\")\n",
    "plt.title(\"Category vs Questionsexpect short answers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wxNJ3setcZx"
   },
   "source": [
    "##### From above plot, it can be said that mostly questions under categories `culture, stackoverflow, technology` expects short answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJqlzHv4tcZz",
    "outputId": "34df9223-c29e-48f3-b1e2-c49ae85ea68c"
   },
   "outputs": [],
   "source": [
    "# BoxPlot of question_interestingness_others\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.grid()\n",
    "sns.boxplot(y=train.question_interestingness_others,x=train.category, orient='v')\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"question_interestingness_others\")\n",
    "plt.title(\"Category vs Questions interesting to others\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EeR9lqL4tcZ4"
   },
   "source": [
    "##### It seems `'life_arts` category questions are more interesting to others than other categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Qi-m7qUtcZ5",
    "outputId": "3540c2b2-9205-4cec-c462-cfaa1f68bc02"
   },
   "outputs": [],
   "source": [
    "# BoxPlot of question_interestingness_self\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.grid()\n",
    "sns.boxplot(y=train.question_interestingness_self,x=train.category, orient='v')\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"question_interestingness_self\")\n",
    "plt.title(\"Category vs question_interestingness_self\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQnq46R8tcZ_"
   },
   "source": [
    "Categories `life_arts` and `culture` have higher score for `question_interestingness_self` than others.\n",
    "\n",
    "`stackoverflow` have the lowest scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VHFJfRAKtcZ_",
    "outputId": "1b8210b9-77b2-4501-a046-70b823443f60"
   },
   "outputs": [],
   "source": [
    "# BoxPlot of question_well_written\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.grid()\n",
    "sns.boxplot(y=train.question_well_written,x=train.category, orient='v')\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"question_well_written\")\n",
    "plt.title(\"Category vs question_well_written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkYKltO8tcaE"
   },
   "source": [
    "`life_arts` and `culture` have more well written questions  than other categories. \n",
    "`science` also has well written questions compared to `stackoverflow` and `technology`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sj0TPMeBtcaE",
    "outputId": "2b3f113f-68a5-465a-84b1-f97cfb393ea7"
   },
   "outputs": [],
   "source": [
    "# BoxPlot of answer_well_written\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.grid()\n",
    "sns.boxplot(y=train.answer_well_written,x=train.category, orient='v')\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"answer_well_written\")\n",
    "plt.title(\"Category vs answer_well_written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LeZu7vttcaN"
   },
   "source": [
    "Answers of all categories are well written except for few outliers with value less than 0.5.\n",
    "`stackoverflow` have some answers with well written score less compared to others "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "igc6_1V0tcaO",
    "outputId": "0c355d09-ff19-4aa7-818b-17a48343d88a"
   },
   "outputs": [],
   "source": [
    "# BoxPlot of answer_satisfaction\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.grid()\n",
    "sns.boxplot(y=train.answer_satisfaction,x=train.category, orient='v')\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"answer_satisfaction\")\n",
    "plt.title(\"Category vs answer_satisfaction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SShIWgKMtcaS"
   },
   "source": [
    "So all the categories have more or less equal answer satisfaction scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-M9zm2MOtcaU"
   },
   "outputs": [],
   "source": [
    "# function to plot distribution of class-labels for each of 5 category\n",
    "def dist_plot_category(class_labels,column_name):\n",
    "    colours = ['r','g','b','m','y']\n",
    "    fig = plt.figure(figsize=(35,35))\n",
    "    for cat,color in zip(set(train[column_name]),colours):\n",
    "        #plt.colorbar()\n",
    "        \n",
    "        i=1\n",
    "        for col in class_labels:\n",
    "            fig.add_subplot(6,5,i)\n",
    "            sns.distplot(train[train['category']==cat][col],color=color,hist=False)\n",
    "            \n",
    "            i+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KXzt9VwHtcaX",
    "outputId": "a1cab2fc-c765-4fa0-cccf-4ecf58395c1b"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "dist_plot_category(class_labels=class_labels,column_name='category')\n",
    "plt.title(\"Distribution of each class label for each category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0pt7aqftcah"
   },
   "outputs": [],
   "source": [
    "# function to plot distribution of class-labels\n",
    "def dist_plot(class_labels, filter_col, filter_value,color='green'):\n",
    "    fig = plt.figure(figsize=(35,35))\n",
    "    i=1\n",
    "    j=1\n",
    "    for col in class_labels:\n",
    "        fig.add_subplot(6,5,i)\n",
    "        sns.distplot(train[train[filter_col]==filter_value][col],color=color)\n",
    "        i+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IWhU1s43tcan",
    "outputId": "3970f26f-cbde-4bac-f9a5-dd92c100aabe"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "dist_plot(class_labels,filter_col='category',filter_value='LIFE_ARTS')\n",
    "plt.title(\"Distribution of each class label for category 'LIFE_ARTS'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIzqqz57tcay",
    "outputId": "3a7ebdaf-7215-43f5-97ef-7cdfee95ef06"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "dist_plot(class_labels,filter_col='category',filter_value='CULTURE',color='blue')\n",
    "plt.title(\"Distribution of each class label for category 'CULTURE'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pI8b_IQ1tca2",
    "outputId": "e151b822-1e5f-4f9e-b9cd-62f07e759d43"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "dist_plot(class_labels,filter_col='category',filter_value='SCIENCE',color='m')\n",
    "plt.title(\"Distribution of each class label for category 'SCIENCE'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5BD-o7XAtca9",
    "outputId": "5ab7266f-f6bf-4542-c112-dffb3f4697ac"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "dist_plot(class_labels,filter_col='category',filter_value='STACKOVERFLOW',color='cyan')\n",
    "plt.title(\"Distribution of each class label for category 'STACKOVERFLOW'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_l4L9pttcbI",
    "outputId": "bbc54a9d-7222-44dd-c806-1c86497cb84e"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "dist_plot(class_labels,filter_col='category',filter_value='TECHNOLOGY',color='green')\n",
    "plt.title(\"Distribution of each class label for category 'TECHNOLOGY'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mO19hCBAtcbM"
   },
   "source": [
    "From all the category plots, `cultural` and `life_arts` have more converstaional questions than other categories.\n",
    "\n",
    "Class label `question_interestingness_self` have higher score for categories `life_arts` and `cultural` for most questions.\n",
    "\n",
    "Categories `stackoverflow` and `technology` have less reason explanation type answers, whereas `science` have more questions compared to others for reason explanation type answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_p4SAXOutcbM",
    "outputId": "a616236d-26df-4eaf-f2e7-44c5a1fbc182"
   },
   "outputs": [],
   "source": [
    "set(train.host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVVZZuwptcbU",
    "outputId": "c9f693e5-68d1-4bab-ea99-94aa46aeae15"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "dist_plot(class_labels,filter_col='host',filter_value='askubuntu',color='green')\n",
    "plt.title(\"Distribution of each class label for host 'askubuntu'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vcv-AQcOtcbY",
    "outputId": "4a917105-360e-48e1-b812-ef0fdfe49b99"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "dist_plot(class_labels,filter_col='host',filter_value='mathoverflow',color='green')\n",
    "plt.title(\"Distribution of each class label for host 'mathoverflow'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "laO2Qi_Jtcbb",
    "outputId": "ba4f7c3c-cf19-4bdf-84b5-a507ae4532e7"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "dist_plot(class_labels,filter_col='host',filter_value='serverfault',color='green')\n",
    "plt.title(\"Distribution of each class label for host 'serverfault'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-63wubWtcbf",
    "outputId": "75c3aefe-978a-4305-c66e-6744b2073109"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "dist_plot(class_labels,filter_col='host',filter_value='stackexchange',color='green')\n",
    "plt.title(\"Distribution of each class label for host 'stackexchange'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZoKIByr6tcbl",
    "outputId": "fb3a23f3-ed0d-4f96-c773-3df096d9714c"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "dist_plot(class_labels,filter_col='host',filter_value='stackoverflow',color='green')\n",
    "plt.title(\"Distribution of each class label for host 'stackoverflow'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7klDpS2btcbo"
   },
   "source": [
    "For host `stackoverflow`, most of the questions have `question_conversational` score 0 i.e most questions are not conversational.\n",
    "\n",
    "`question_well_written` score are well distributed between 0.4 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "suSYkJVstcbq",
    "outputId": "035388fd-1fc5-44c6-b2d5-a15a6c0810da"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "dist_plot(class_labels,filter_col='host',filter_value='superuser',color='green')\n",
    "plt.title(\"Distribution of each class label for host 'superuser'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUe_Zfkjtcbt"
   },
   "source": [
    "For all the hosts above, very few questions have `question_type_reason_explanation` close to 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQo0Jceftcb2"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wncp4dR1tcb3",
    "outputId": "e49d53e2-c11b-402c-c4c1-687ec5f98102"
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JqwoAGbjtccC"
   },
   "outputs": [],
   "source": [
    "# https://gist.github.com/sebleier/554280\n",
    "# we are removing the words from the stop words list: 'no', 'nor', 'not'\n",
    "stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't , gt,lt\"]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IXWtBC01tccI"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(phrase):\n",
    "    # specific\n",
    "    phrase = str(phrase)\n",
    "    phrase = phrase.lower()\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrade = re.sub(r\"wont\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"cant\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"cannot\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"doesn't\", \"does not\", phrase)\n",
    "    \n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    \n",
    "    phrase = phrase.replace('\\\\r', ' ')\n",
    "    phrase = phrase.replace('\\\\\"', ' ')\n",
    "    phrase = phrase.replace('\\\\n', ' ')\n",
    "    phrase = phrase.replace('\\\\', ' ')\n",
    "    \n",
    "    #phrase = re.sub(r'http\\S+', '', phrase)  # for removal all http link\n",
    "    phrase = re.sub(r'[^A-Za-z0-9]', ' ',phrase) # remove everything except alphaets and numbers\n",
    "    \n",
    "    senten=''\n",
    "    for val in phrase.split():           # this for loop will remove the stopwords and words having only numbers \n",
    "        pattern = re.match(r\"[0-9]\",val)\n",
    "        if (pattern) or (val in stopwords):      \n",
    "            continue\n",
    "        else:\n",
    "            senten = senten + ' ' +val\n",
    "            \n",
    "    senten = senten.strip()            # removing space from string start and end\n",
    "    return senten\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###try\n",
    "\n",
    "# https://stackoverflow.com/a/47091490/4084039\n",
    "import re\n",
    "\n",
    "def preprocess_text(phrase):\n",
    "    # specific\n",
    "    phrase = str(phrase)\n",
    "    phrase = phrase.lower()\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrade = re.sub(r\"wont\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"cant\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"cannot\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"doesn't\", \"does not\", phrase)\n",
    "    \n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    \n",
    "    phrase = phrase.replace('\\\\r', ' ')\n",
    "    phrase = phrase.replace('\\\\\"', ' ')\n",
    "    phrase = phrase.replace('\\\\n', ' ')\n",
    "    phrase = phrase.replace('\\\\', ' ')\n",
    "    \n",
    "    #phrase = re.sub(r'http\\S+', '', phrase)  # for removal all http link\n",
    "    phrase = re.sub(r'[^A-Za-z0-9]', ' ',phrase) # remove everything except alphaets and numbers\n",
    "    \n",
    "    sent = ''\n",
    "    for val in phrase.split():          # removing multiple spaces between words\n",
    "        sent = sent + ' ' + val\n",
    "    sent = sent.strip()\n",
    "    \n",
    "    return sent\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "QEB_qb-CtccQ",
    "outputId": "26ca6230-7e25-4a16-c932-b2f4fc00292f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6079\n",
      "what am i losing when using extension tubes instead of a macro lens\n",
      "==================================================\n",
      "after playing around with macro photography on the cheap read reversed lens rev lens mounted on a straight lens passive extension tubes i would like to get further with this the problems with the techniques i used is that focus is manual and aperture control is problematic at best this limited my setup to still subjects read dead insects now as spring is approaching i want to be able to shoot live insects i believe that for this autofocus and settable aperture will be of great help so one obvious but expensive option is a macro lens say ef 100mm macro however i am not really interested in yet another prime lens an alternative is the electrical extension tubes except for maximum focusing distance what am i losing when using tubes coupled with a fine lens say ef70 200 2 8 instead of a macro lens\n",
      "==================================================\n",
      "i just got extension tubes so here is the skinny what am i losing when using tubes a very considerable amount of light increasing that distance from the end of the lens to the sensor can cut your light several stops combined with the fact that you will usually shoot stopped down expect to need to increase your iso considerably the fact the macro is are usually considered very very sharp although i believe that 70 200mm 2 8 is supposed to be quite sharp the ultra low distortion typical of many macros i would not worry too much about the bokeh since the dof will still be quite limited coupled on my 50mm a full 60mm ish extension tube results in a dof of about a couple inches in front of the lens on my 70 300 its probably around 2 3 feet in front of the lens to about a foot in front of the lens\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# preprocessing question title, question body and answers\n",
    "train['question_title'] = train['question_title'].apply(lambda x: preprocess_text(x))\n",
    "train['question_body'] = train['question_body'].apply(lambda x: preprocess_text(x))\n",
    "train['answer'] = train['answer'].apply(lambda x: preprocess_text(x))\n",
    "\n",
    "print(len(train['question_title']))\n",
    "print(train['question_title'][0])\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(train['question_body'][0])\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(train['answer'][0])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "50WOF45otccd"
   },
   "outputs": [],
   "source": [
    "# preprocessing 'category' and 'host' to lower-case and stripping leading and tailing spaces \n",
    "\n",
    "train['category'] = train['category'].apply(lambda x: x.lower())\n",
    "train['category'] = train['category'].apply(lambda x: x.strip())\n",
    "\n",
    "train['host'] = train['host'].apply(lambda x: x.lower())\n",
    "train['host'] = train['host'].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rf72wlIatcci"
   },
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgtZiDyptcci"
   },
   "source": [
    "I have already created 3 features in the EDA section - \n",
    "1. `q_title_length` - Length of question title\n",
    "2. `q_body_length` - Length of question body\n",
    "3. `answer_length` - Length of answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsjPEweEtccj"
   },
   "source": [
    "#### Features - Sentiment scores of the question_title, question_body and answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "c3wp9df8tccj",
    "outputId": "d214c774-a713-49a7-ceb2-5c0cb1d9b475"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6079, 48)\n"
     ]
    }
   ],
   "source": [
    "# sentiments from question_title\n",
    "\n",
    "#nltk.download('vader_lexicon')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "neg=[]\n",
    "neu=[]\n",
    "pos=[]\n",
    "comp=[]\n",
    "for title in train.question_title:\n",
    "    ss = sid.polarity_scores(title)\n",
    "    neg.append(ss['neg'])\n",
    "    neu.append(ss['neu'])\n",
    "    pos.append(ss['pos'])\n",
    "    comp.append(ss['compound'])\n",
    "\n",
    "train['q_title_neg'] = neg\n",
    "train['q_title_neu'] = neu\n",
    "train['q_title_pos'] = pos\n",
    "train['q_title_comp'] = comp\n",
    "\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "TeizkTvTtccm",
    "outputId": "1db96715-5c9f-4b05-c5f3-99c4ef0bb754"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6079, 52)\n"
     ]
    }
   ],
   "source": [
    "# sentiments from question_body\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "neg=[]\n",
    "neu=[]\n",
    "pos=[]\n",
    "comp=[]\n",
    "for body in train.question_body:\n",
    "    ss = sid.polarity_scores(body)\n",
    "    neg.append(ss['neg'])\n",
    "    neu.append(ss['neu'])\n",
    "    pos.append(ss['pos'])\n",
    "    comp.append(ss['compound'])\n",
    "        \n",
    "train['q_body_neg'] = neg\n",
    "train['q_body_neu'] = neu\n",
    "train['q_body_pos'] = pos\n",
    "train['q_body_comp'] = comp\n",
    "\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "U-dSjo57tcco",
    "outputId": "4f410f80-70e0-491f-f65d-3b187a90dfc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6079, 56)\n"
     ]
    }
   ],
   "source": [
    "# sentiments from answer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "neg=[]\n",
    "neu=[]\n",
    "pos=[]\n",
    "comp=[]\n",
    "for body in train.answer:\n",
    "    ss = sid.polarity_scores(body)\n",
    "    neg.append(ss['neg'])\n",
    "    neu.append(ss['neu'])\n",
    "    pos.append(ss['pos'])\n",
    "    comp.append(ss['compound'])\n",
    "        \n",
    "train['q_answer_neg'] = neg\n",
    "train['q_answer_neu'] = neu\n",
    "train['q_answer_pos'] = pos\n",
    "train['q_answer_comp'] = comp\n",
    "\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnJG3Wtstccu"
   },
   "source": [
    "#### EDA on new features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "PvPO0j-Jtccu"
   },
   "outputs": [],
   "source": [
    "# function to get the percentile values by passing dataframe and column\n",
    "def percentile(data,column,min_lim=1,max_lim=101,diff=3):\n",
    "    for i in np.arange(min_lim,max_lim,diff):\n",
    "        print(f\"{i} percentile = \",np.percentile(data[column], i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Pr89lQKktccw",
    "outputId": "dd642a92-6868-4127-9039-a82e96cf6eda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 percentile =  0.0\n",
      "4 percentile =  0.0\n",
      "7 percentile =  0.0\n",
      "10 percentile =  0.0\n",
      "13 percentile =  0.0\n",
      "16 percentile =  0.0\n",
      "19 percentile =  0.0\n",
      "22 percentile =  0.0\n",
      "25 percentile =  0.0\n",
      "28 percentile =  0.0\n",
      "31 percentile =  0.0\n",
      "34 percentile =  0.0\n",
      "37 percentile =  0.0\n",
      "40 percentile =  0.0\n",
      "43 percentile =  0.0\n",
      "46 percentile =  0.0\n",
      "49 percentile =  0.0\n",
      "52 percentile =  0.0\n",
      "55 percentile =  0.0\n",
      "58 percentile =  0.0\n",
      "61 percentile =  0.0\n",
      "64 percentile =  0.0\n",
      "67 percentile =  0.0\n",
      "70 percentile =  0.0\n",
      "73 percentile =  0.0\n",
      "76 percentile =  0.0\n",
      "79 percentile =  0.0\n",
      "82 percentile =  0.0\n",
      "85 percentile =  0.1283000000000002\n",
      "88 percentile =  0.189\n",
      "91 percentile =  0.238\n",
      "94 percentile =  0.278\n",
      "97 percentile =  0.355\n",
      "100 percentile =  0.76\n"
     ]
    }
   ],
   "source": [
    "percentile(train, column='q_title_neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "o-PBmutStcc1",
    "outputId": "a5d48fef-9b7f-45da-aaf2-89cc95ad8701"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 percentile =  0.1283000000000002\n",
      "86 percentile =  0.149\n",
      "87 percentile =  0.172\n",
      "88 percentile =  0.189\n",
      "89 percentile =  0.20742000000000005\n",
      "90 percentile =  0.22\n",
      "91 percentile =  0.238\n",
      "92 percentile =  0.252\n",
      "93 percentile =  0.268\n",
      "94 percentile =  0.278\n",
      "95 percentile =  0.302\n",
      "96 percentile =  0.326\n",
      "97 percentile =  0.355\n",
      "98 percentile =  0.40187999999999924\n",
      "99 percentile =  0.470880000000001\n"
     ]
    }
   ],
   "source": [
    "percentile(train, 'q_title_neg',85,100,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VftxqZrftcc8"
   },
   "source": [
    "##### 82% of question title have negative score = 0 i.e. they are not negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tM-TAlK1tcc8",
    "outputId": "0f7105c5-d03c-455c-b55e-879a82f1f647"
   },
   "outputs": [],
   "source": [
    "# kde plot of q_title_neu\n",
    "fig = plt.figure(figsize=plt.figaspect(.3))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "sns.kdeplot(train['q_title_neu'], shade=True, ax=ax1)\n",
    "plt.xlabel('Question title neutral score')\n",
    "plt.grid()\n",
    "plt.title(\"PDF\")\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "sns.kdeplot(train['q_title_neu'], shade=True, cumulative=True, ax=ax2)\n",
    "plt.xlabel('Question title neutral score')\n",
    "plt.grid()\n",
    "plt.title('CDF')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxx7yjtJtcc-"
   },
   "source": [
    "##### Maximum number of questions have neutral score 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wWoA4GIZtcc_",
    "outputId": "32aa9a88-d7ed-489c-ec7b-084624ad9e76"
   },
   "outputs": [],
   "source": [
    "# percentiles of q_title_neu (neutral rating of question title)\n",
    "percentile(train, 'q_title_neu',1,100,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wsNf60ztcdA"
   },
   "source": [
    "##### All question titles have neutral score greater than 0 i.e. all questions are somewhat neutral.\n",
    "##### 65% of question titles have neutral score 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2lRURLTtcdB",
    "outputId": "e71622b4-d3e7-47f8-a700-a4387545713e"
   },
   "outputs": [],
   "source": [
    "percentile(train, 'q_title_pos',1,100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xouhsf-ktcdD",
    "outputId": "f91b6354-9b8a-4bf5-a894-070efdb96784"
   },
   "outputs": [],
   "source": [
    "percentile(train, 'q_title_pos',99,100.1,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgAa5tbGtcdF"
   },
   "source": [
    "##### 75% questions have positive score 0 i.e. they are not positive.\n",
    "##### Maximum positive score a question has 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNNeepYKtcdF",
    "outputId": "44278b38-fc4c-481e-9836-ae0bdaa3e226"
   },
   "outputs": [],
   "source": [
    "# kde plot of q_body_neg\n",
    "fig = plt.figure(figsize=plt.figaspect(.3))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "sns.kdeplot(train['q_body_neg'], shade=True, ax=ax1)\n",
    "plt.xlabel('Question body negative score')\n",
    "plt.grid()\n",
    "plt.title(\"PDF\")\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "sns.kdeplot(train['q_body_neg'], shade=True, cumulative=True, ax=ax2)\n",
    "plt.xlabel('Question body negative score')\n",
    "plt.grid()\n",
    "plt.title('CDF')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ODbiX_7QtcdH",
    "outputId": "02c042dd-3fa8-4849-b5be-3000615eecb5"
   },
   "outputs": [],
   "source": [
    "percentile(train, 'q_body_neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3olIYp4xtcdK"
   },
   "source": [
    "##### Maximum negative score of a question body is 0.605. Around 31% of questions have negative score 0 i.e. they are not negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tzYv0gg0tcdK",
    "outputId": "78f47bd0-8146-408c-f08f-87a73cddf386"
   },
   "outputs": [],
   "source": [
    "# kde plot of q_body_neu\n",
    "fig = plt.figure(figsize=plt.figaspect(.3))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "sns.kdeplot(train['q_body_neu'], shade=True, ax=ax1)\n",
    "plt.xlabel('Question body neutral score')\n",
    "plt.grid()\n",
    "plt.title(\"PDF\")\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "sns.kdeplot(train['q_body_neu'], shade=True, cumulative=True, ax=ax2)\n",
    "plt.xlabel('Question body neural score')\n",
    "plt.grid()\n",
    "plt.title('CDF')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndjsIeZwtcdN"
   },
   "source": [
    "##### Most of the questions body have neutral score greater than 0.8\n",
    "##### About 50% of the questions have neutral score less than 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1OrK0YCtcdN",
    "outputId": "b5d3a439-c2ab-46fe-d443-a2dd406ca25f"
   },
   "outputs": [],
   "source": [
    "percentile(train, 'q_body_neu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlM3IC7xtcdP"
   },
   "source": [
    "##### All question body have neutral score greater than 0.4\n",
    "#####  51% of the questions body have neutral score greater than 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPJoB_IPtcdP",
    "outputId": "19c3c8c8-2704-489f-ef4e-79c69bcc5f93"
   },
   "outputs": [],
   "source": [
    "# kde plot of q_body_pos\n",
    "fig = plt.figure(figsize=plt.figaspect(.3))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "sns.kdeplot(train['q_body_pos'], shade=True, ax=ax1)\n",
    "plt.xlabel('Question body positive score')\n",
    "plt.grid()\n",
    "plt.title(\"PDF\")\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "sns.kdeplot(train['q_body_pos'], shade=True, cumulative=True, ax=ax2)\n",
    "plt.xlabel('Question body positive score')\n",
    "plt.grid()\n",
    "plt.title('CDF')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "An_RjQKetcdR"
   },
   "source": [
    "##### Around 40% of the questions body have score less than 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3YSK4-qtcdR",
    "outputId": "c34c77fd-34bb-444a-c06a-5b6fbabde5c6"
   },
   "outputs": [],
   "source": [
    "percentile(train, 'q_body_pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5jnmpuOtcdZ"
   },
   "source": [
    "##### Maximum positive score of a question body is 0.667.\n",
    "##### 95% of the questions body have a positive score less than 0.3 i.e. most of the questions body are not much positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8rfv6glytcda",
    "outputId": "0e1fe6c9-47f6-4795-eca3-00b421179777"
   },
   "outputs": [],
   "source": [
    "# kde plot of q_answer_neg\n",
    "fig = plt.figure(figsize=plt.figaspect(.3))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "sns.kdeplot(train['q_answer_neg'], shade=True, ax=ax1)\n",
    "plt.xlabel('Answer negative score')\n",
    "plt.grid()\n",
    "plt.title(\"PDF\")\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "sns.kdeplot(train['q_answer_neg'], shade=True, cumulative=True, ax=ax2)\n",
    "plt.xlabel('Answer negative score')\n",
    "plt.grid()\n",
    "plt.title('CDF')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYiBy27Etcdd"
   },
   "source": [
    "##### Most of the answers have negative score 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dAprDGJ1tcdd",
    "outputId": "bae60e37-296a-4136-bc07-afd5aad773af"
   },
   "outputs": [],
   "source": [
    "percentile(train,'q_answer_neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0DL-Q7ftcdj"
   },
   "source": [
    "##### 35% of the answers have negative score 0. \n",
    "##### 75% of the answers have negative score less than 0.1, that means maximum answers have very less negative score.\n",
    "##### Maximum negative score of an answer is 0.675."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zv-sRWxCtcdj",
    "outputId": "d305e188-267e-430c-db09-5171a73bb0b8"
   },
   "outputs": [],
   "source": [
    "# kde plot of q_answer_neu\n",
    "fig = plt.figure(figsize=plt.figaspect(.3))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "sns.kdeplot(train['q_answer_neu'], shade=True, ax=ax1)\n",
    "plt.xlabel('Answer neutral score')\n",
    "plt.grid()\n",
    "plt.title(\"PDF\")\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "sns.kdeplot(train['q_answer_neu'], shade=True, cumulative=True, ax=ax2)\n",
    "plt.xlabel('Answer neutral score')\n",
    "plt.grid()\n",
    "plt.title('CDF')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBe7h81htcdl"
   },
   "source": [
    "##### Around 50% of the answers have neutral score greater than 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OlWbmTfStcdm",
    "outputId": "a7bac6c4-9b43-45b4-c11a-2b84e297e3cf"
   },
   "outputs": [],
   "source": [
    "percentile(train,'q_answer_neu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlLIpvTktcdo"
   },
   "source": [
    "Most answers are highly neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWXMltt9tcdo",
    "outputId": "ba20d0d7-485b-447b-aba7-e8082943c033"
   },
   "outputs": [],
   "source": [
    "# kde plot of q_answer_pos\n",
    "fig = plt.figure(figsize=plt.figaspect(.3))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "sns.kdeplot(train['q_answer_pos'], shade=True, ax=ax1)\n",
    "plt.xlabel('Answer positive score')\n",
    "plt.grid()\n",
    "plt.title(\"PDF\")\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "sns.kdeplot(train['q_answer_pos'], shade=True, cumulative=True, ax=ax2)\n",
    "plt.xlabel('Answer positive score')\n",
    "plt.grid()\n",
    "plt.title('CDF')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nubTL_VBtcdq"
   },
   "source": [
    "##### Around 78% of the answers have positive score upto 0.2, so most answers are not so positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1s_9Wpuotcdq",
    "outputId": "cb9bfdc4-b2ff-4401-9429-d441aa2cc058"
   },
   "outputs": [],
   "source": [
    "# percentiles of answer positive score\n",
    "percentile(train,'q_answer_pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOQxMcC8tcds"
   },
   "source": [
    "##### Maximum positive score of an answer is 0.857.\n",
    "##### Around 91% of the answer have a positive score upto 0.29 i.e. most of the answers are not much positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfUI5XD3tcds"
   },
   "source": [
    "### Splitting train data into train, cross-validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_Gt7vVwtcdt",
    "outputId": "355d27d8-0ca7-487d-9459-904ba8ef2dc0"
   },
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Arwv_9k0tcdu"
   },
   "outputs": [],
   "source": [
    "# input columns to be used for training model\n",
    "input_columns = ['question_title', 'question_body','answer','category', 'host','q_title_length', 'q_body_length',\n",
    "                 'answer_length', 'q_title_neg', 'q_title_neu', 'q_title_pos','q_title_comp', 'q_body_neg', 'q_body_neu', \n",
    "                 'q_body_pos', 'q_body_comp','q_answer_neg', 'q_answer_neu', 'q_answer_pos', 'q_answer_comp']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "PKVAwbW8tcdx",
    "outputId": "76308784-f248-446e-ca84-8f05c23159a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6079, 20) (6079, 30)\n"
     ]
    }
   ],
   "source": [
    "X = train[input_columns].copy()\n",
    "Y = train[class_labels].copy()\n",
    "\n",
    "print(X.shape,Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "pyK91eOHtcd1",
    "outputId": "a324d03a-b98d-45bf-ee34-982ed564dab6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after split\n",
      "Train shape =  (5471, 20) (5471, 30)\n",
      "Test shape =  (608, 20) (608, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_cv,y_train,y_cv = train_test_split(X,Y, test_size=0.1)\n",
    "print(\"Shape after split\")\n",
    "print(\"Train shape = \", x_train.shape, y_train.shape)\n",
    "print(\"Test shape = \", x_cv.shape, y_cv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dId08OPYtcd4"
   },
   "source": [
    "### Encoding categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "7i-Fa-Gatcd4",
    "outputId": "7d117146-4128-41c2-c514-6df891df0397"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train after encoding =  (5471, 5)\n",
      "Shape of cv after encoding =  (608, 5)\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding 'category' feature\n",
    "category_ohe = CountVectorizer(binary=True)\n",
    "category_ohe.fit(x_train['category'])\n",
    "category_encoded_train = category_ohe.transform(x_train['category']).todense()\n",
    "category_encoded_cv = category_ohe.transform(x_cv['category']).todense()\n",
    "\n",
    "print(\"Shape of train after encoding = \",category_encoded_train.shape)\n",
    "print(\"Shape of cv after encoding = \",category_encoded_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "aCo-rW-etcd8",
    "outputId": "efceced4-63f1-4dff-b5cf-b28e4daa7fa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train after encoding =  (5471, 6)\n",
      "Shape of cv after encoding =  (608, 6)\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding 'host'\n",
    "host_ohe = CountVectorizer(binary=True)\n",
    "host_ohe.fit(x_train['host'])\n",
    "host_encoded_train = host_ohe.transform(x_train['host']).todense()\n",
    "host_encoded_cv = host_ohe.transform(x_cv['host']).todense()\n",
    "\n",
    "print(\"Shape of train after encoding = \",host_encoded_train.shape)\n",
    "print(\"Shape of cv after encoding = \",host_encoded_cv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKGWxf2Htcd-"
   },
   "source": [
    "### Encoding Text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "joNU7i_UtceB"
   },
   "source": [
    "#### Using Pretrained Models: Avg W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sMBysRUetceB",
    "outputId": "64bbeb5c-505e-468d-e1c3-8a34c5f6a811"
   },
   "outputs": [],
   "source": [
    "# Reading glove vectors in python: https://stackoverflow.com/a/38230349/4084039\n",
    "def loadGloveModel(gloveFile):\n",
    "    print (\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding=\"utf8\")\n",
    "    model = {}\n",
    "    for line in tqdm(f):\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "model = loadGloveModel('glove.42B.300d.txt')\n",
    "\n",
    "# ============================\n",
    "\n",
    "words = []\n",
    "for i in train['question_title']:\n",
    "    words.extend(i.split(' '))\n",
    "\n",
    "for i in train['question_body']:\n",
    "    words.extend(i.split(' '))\n",
    "    \n",
    "for i in train['answer']:\n",
    "    words.extend(i.split(' '))\n",
    "\n",
    "print(\"all the words in the coupus\", len(words))\n",
    "words = set(words)\n",
    "print(\"the unique words in the coupus\", len(words))\n",
    "\n",
    "inter_words = set(model.keys()).intersection(words)\n",
    "print(\"The number of words that are present in both glove vectors and our coupus\", \\\n",
    "      len(inter_words),\"(\",np.round(len(inter_words)/len(words)*100,3),\"%)\")\n",
    "\n",
    "words_courpus = {}\n",
    "words_glove = set(model.keys())\n",
    "for i in words:\n",
    "    if i in words_glove:\n",
    "        words_courpus[i] = model[i]\n",
    "print(\"word 2 vec length\", len(words_courpus))\n",
    "\n",
    "# saving variables into a pickle file\n",
    "with open('glove_vectors_sw', 'wb') as f:\n",
    "    pickle.dump(words_courpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "HemlaszUtceJ"
   },
   "outputs": [],
   "source": [
    "# reading glove_vectors pickle file\n",
    "with open('../input/glove-vec/glove_vectors_sw', 'rb') as f:\n",
    "    glove_vector = pickle.load(f)\n",
    "    glove_words =  set(glove_vector.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "U4riHwc0tceL",
    "outputId": "68a3250e-4eb1-4132-c9c5-05537dc0b116",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5471/5471 [00:00<00:00, 22436.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5471\n",
      "300\n",
      "[ 1.64727692e-01 -4.59757692e-02 -1.50027923e-01 -1.90242769e-01\n",
      "  1.83442154e-01  1.65900462e-01 -3.43772462e+00  5.16896923e-01\n",
      " -2.33952308e-02 -2.90199412e-01  9.16001846e-02 -4.83976077e-02\n",
      " -9.45620000e-02 -8.88867154e-02  7.21796923e-02 -1.15775100e-01\n",
      " -3.17739462e-01 -3.76377692e-02  1.05340869e-02  3.29356154e-03\n",
      " -9.46256154e-02  2.09441654e-02  5.43830769e-03 -4.63223846e-02\n",
      "  5.12043846e-02 -2.65704615e-01  3.45151538e-02 -1.89961446e-01\n",
      " -1.08648392e-01 -2.64158762e-01 -1.07462031e-01  5.74815385e-02\n",
      "  5.30039231e-02 -1.52692385e-01 -1.67131154e-01 -6.32578846e-02\n",
      " -1.16893577e-01 -8.61210400e-02  8.01764092e-02 -2.17952308e-02\n",
      " -9.28490000e-02 -9.17992308e-03 -1.77419154e-01 -2.62596692e-01\n",
      "  1.03670769e-02 -6.16856154e-02 -2.12778692e-02 -4.37582308e-02\n",
      " -5.73906154e-02 -7.61547692e-02 -1.19100077e-01  9.02662308e-02\n",
      " -9.59820769e-02 -3.89924615e-02 -1.30807692e-01 -1.59371238e-01\n",
      " -1.09581769e-01  5.03935385e-02 -1.00709000e-01  3.60418462e-02\n",
      " -4.12824615e-02 -2.74266923e-01 -2.40425385e-02 -4.56313077e-02\n",
      "  3.94233846e-02  1.34544615e-01 -8.29784615e-03  1.31757692e-02\n",
      "  9.12720923e-02  5.65784615e-03 -8.58262308e-02  1.50465615e-01\n",
      " -1.66960000e-02  9.70584615e-03 -1.48045231e-02 -1.32570885e-01\n",
      "  1.09397231e-01  1.08185615e-01  4.50221154e-02 -2.11717000e-01\n",
      " -1.20348462e-02 -4.13213554e-01  1.01261769e-01 -8.41964154e-02\n",
      "  3.87289308e-02 -3.89815385e-03 -6.08306923e-02 -1.36296385e-01\n",
      "  1.20298846e-01 -8.22719538e-02  7.36836154e-02 -1.65745538e-01\n",
      " -1.71862692e-01 -6.63258000e-02  7.45774615e-02 -1.85037462e-01\n",
      " -2.46386923e+00  1.58556692e-02  7.19385385e-02  8.48600000e-02\n",
      "  1.76739923e-01  1.62007615e-01  1.39945815e-01 -2.98882308e-02\n",
      " -4.24919923e-02 -5.81172308e-03  7.03873077e-02 -1.36998462e-01\n",
      " -6.14083077e-02 -1.87523692e-01  6.20445231e-02  6.81833077e-02\n",
      "  4.69476923e-03  3.15419231e-02 -2.26044177e-01  8.62840346e-02\n",
      "  1.69943553e-01 -1.02357000e-02 -7.46318462e-02 -1.36315385e-02\n",
      " -1.73778892e-01 -1.05330385e-01  8.59548700e-02 -2.27655462e-01\n",
      "  1.70802846e-01  6.80170000e-02  4.21219231e-02 -1.47536892e-01\n",
      "  1.09665385e-01  2.75464077e-01 -1.22106900e-01 -8.04699846e-02\n",
      " -1.07289462e-01 -1.85560769e-01  1.24338462e-02 -7.71027385e-02\n",
      "  1.19555462e-01 -2.81319231e-02  7.98947692e-02  3.80081846e-01\n",
      "  1.44393308e-01  2.06622846e-01  2.54697692e-02 -1.77835300e-01\n",
      "  1.22519231e-02  1.22795835e-01  6.04424615e-02 -1.23432446e-01\n",
      "  2.09210685e-01  1.35580000e-01  1.26302462e-01 -1.35080692e-01\n",
      " -1.35966023e-01  4.23056154e-02  1.44076308e-01 -3.45243846e-02\n",
      "  9.57019231e-02 -1.74319846e-01 -1.64933692e-01 -7.99338462e-02\n",
      "  6.61782308e-02 -1.13523631e-01  1.48385385e-02  7.00150615e-02\n",
      "  5.36707692e-03 -6.75366154e-02  5.53454923e-02  1.28109692e-01\n",
      "  2.63642308e-02  1.31675462e-01 -9.32031385e-02 -5.28097692e-02\n",
      " -1.33366077e-01 -6.06028231e-02 -1.87879846e-01  2.34622154e-01\n",
      " -2.54639231e-02 -1.03558308e-01  2.31854231e-01 -1.67280192e-01\n",
      " -7.49223077e-02  1.64819385e-01 -1.10256462e-01 -5.64073846e-02\n",
      "  1.89538154e-01 -1.58827308e-01  2.14712462e-01  5.19890000e-02\n",
      "  3.98559231e-02  3.02851615e-02 -4.86470000e-02  9.44608615e-02\n",
      "  3.74627154e-02  1.01355692e-02  1.31384615e-01 -1.24016462e-01\n",
      "  2.60123154e-01  2.23286154e-02  2.38565692e-01 -2.05213846e-01\n",
      "  1.83582462e-01  5.64274615e-02  2.38791000e-02  1.20679692e-01\n",
      "  5.88595385e-03  1.43470692e-01  4.01266923e-02 -8.01325385e-02\n",
      "  4.65031692e-02 -1.50945846e-01  8.13411538e-02  3.11849231e-02\n",
      "  4.96689231e-02 -2.34939231e-02 -4.64816923e-02  1.39537692e-01\n",
      " -4.95676923e-03 -3.40298769e-01 -3.73246154e-02  1.89541538e-02\n",
      " -6.44716615e-02 -1.80871000e-01 -2.26318462e-01  5.14213846e-02\n",
      " -2.45240100e+00  1.89791923e-01  7.00563154e-02 -4.30790000e-02\n",
      " -8.65076923e-04  5.70132308e-02  1.38055215e-01 -3.42626154e-02\n",
      "  2.72975169e-01  3.10236154e-02 -4.94321615e-02  3.92289231e-02\n",
      " -4.90933231e-02 -4.64676154e-02 -1.28891308e-01 -1.32324246e-01\n",
      " -1.71303846e-02 -8.29775769e-02 -1.57082308e-01 -1.36586385e-01\n",
      "  1.13819231e-03  2.03448462e-02 -1.33070692e-01  6.62118154e-02\n",
      "  4.08176923e-02  5.44190000e-02 -1.62657869e-01 -1.21104746e-01\n",
      " -2.99052000e-01  1.06746362e-01  1.00240692e-01 -4.81489846e-02\n",
      "  1.57137462e-02  1.05036315e-01 -9.12648308e-02  1.82610562e-01\n",
      "  5.06899231e-02  5.27530462e-02 -3.52553077e-02 -9.41719231e-02\n",
      "  1.19624769e-01 -5.92088462e-02 -2.81895769e-01 -9.21406154e-02\n",
      "  1.08470769e-01 -1.80092308e-03  3.62268462e-02  9.15669231e-03\n",
      " -1.58571692e-02  2.32427462e-01  1.14521769e-01  1.24911538e-01\n",
      "  7.25138462e-03  1.82939308e-01  6.43563077e-02  5.17324615e-02\n",
      "  4.33964615e-01  4.57923077e-04  1.88415385e-02  4.15538462e-02\n",
      " -8.28681538e-02  1.12076000e-01 -1.64586846e-02  1.53801462e-01\n",
      "  6.89410769e-02 -1.39174231e-01 -9.82312538e-02 -1.14737231e-01\n",
      "  3.69821538e-02 -6.07038462e-03 -7.70923077e-04  6.90379231e-02\n",
      " -9.01949692e-02  1.75021538e-02  1.97756923e-02 -3.48470000e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# average Word2Vec on question title train data\n",
    "# computing average word2vec for each question title\n",
    "avg_w2v_vectors_title_train = []; # the avg-w2v for each question title is stored in this list\n",
    "for sentence in tqdm(x_train['question_title']): # for each question title\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    cnt_words = 0; # num of words with a valid vector in the question title\n",
    "    for word in sentence.split(): # for each word in a question title\n",
    "        if word in glove_words:\n",
    "            vector += glove_vector[word]\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        vector /= cnt_words\n",
    "    avg_w2v_vectors_title_train.append(vector)\n",
    "\n",
    "print(len(avg_w2v_vectors_title_train))\n",
    "print(len(avg_w2v_vectors_title_train[0]))\n",
    "print(avg_w2v_vectors_title_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Siz1OGeBtceM",
    "outputId": "f1191939-bcee-4cf5-a904-6237789e58e0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 608/608 [00:00<00:00, 22283.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608\n",
      "300\n",
      "[-1.02702500e-01 -1.24952500e-01  4.60032500e-02  4.61875000e-02\n",
      " -2.13337500e-02 -2.42090000e-01 -2.20487250e+00 -8.01335000e-02\n",
      "  8.95940000e-02 -3.05692500e-01  2.31213250e-01 -1.60452500e-01\n",
      " -3.10460500e-01 -8.12286000e-02  1.65948750e-01  7.72487500e-02\n",
      " -8.42195000e-02  1.32220000e-01  7.30650000e-02  1.97406750e-01\n",
      "  5.35622500e-01  1.05915000e-02  6.88375000e-02  1.50225000e-01\n",
      " -1.19860000e-01  8.07025000e-02  1.05769000e-01 -4.10766000e-01\n",
      "  6.94575000e-03 -1.11838500e-01  6.55072500e-02 -2.08339500e-01\n",
      " -2.30932500e-01  2.89742500e-01 -3.59192500e-02 -2.40535000e-01\n",
      "  2.53209250e-01  9.28500000e-04  2.03382750e-01 -3.29990000e-02\n",
      " -1.65735000e-01  1.34780000e-02 -1.38507500e-02 -4.03552500e-01\n",
      "  1.55301175e-01  3.35280000e-01 -1.66436750e-01 -1.85878875e-01\n",
      " -4.93915000e-01  4.26075000e-02  1.29445650e-01  1.74497500e-01\n",
      " -1.86195000e-02 -5.18800000e-03 -2.53217500e-01 -4.64127500e-02\n",
      "  3.98167500e-02 -3.43334500e-01  3.12283250e-01 -1.14772250e-02\n",
      "  3.81703000e-01 -9.85500000e-03 -1.32523750e-01  2.41763500e-01\n",
      " -2.68136500e-01  1.48408000e-01 -4.96191750e-01  1.66232500e-01\n",
      " -2.20807500e-02 -1.41540000e-01  5.90050000e-02  9.59322500e-02\n",
      " -8.66583500e-02 -1.84515000e-02 -1.33761250e-01 -3.99407500e-01\n",
      "  3.65432500e-01 -3.33777500e-01  1.72300000e-02 -1.42041250e-01\n",
      " -1.06904500e-01  9.00575000e-02 -2.72877500e-02  2.07673000e-01\n",
      "  3.26102500e-01 -3.40838750e-01 -3.38457500e-02  3.71187500e-02\n",
      " -2.80205500e-01 -6.35792500e-01 -1.72915000e-01 -1.88767500e-01\n",
      " -1.70234000e-01  9.72675000e-03 -1.24542000e-01 -4.48554500e-02\n",
      " -1.34389250e+00  1.40310000e-01  1.25831750e-01  7.62190000e-02\n",
      "  8.14522500e-02  2.00597250e-01  2.68062500e-01  1.53200750e-01\n",
      " -2.27580000e-01 -2.00960250e-01 -3.34700000e-03  4.09675000e-02\n",
      "  1.71360750e-01 -2.45436250e-02  1.08552500e-02  1.26190500e-01\n",
      " -2.15797500e-02  4.56100000e-02 -3.54800000e-03  1.11094250e-01\n",
      "  1.48459000e-01 -2.17363500e-01  2.15071250e-01  2.48055000e-01\n",
      "  5.64262500e-02  3.74577500e-01 -2.22867500e-01 -8.48394000e-02\n",
      " -1.61872500e-01 -1.40067500e-01 -1.20917275e-01 -1.04035250e-01\n",
      "  2.66715000e-02  3.06260000e-01  1.01832500e-01 -2.53607750e-02\n",
      " -4.82507500e-02 -2.02057500e-01 -2.85380000e-01 -1.12167500e-02\n",
      " -1.80098000e-01 -3.78875000e-02  1.11830000e-01  1.47500000e-04\n",
      " -1.44414500e-01 -1.22625000e-03  1.06525000e-02 -5.38735000e-02\n",
      " -5.58105000e-02 -1.37504000e-01  7.29877500e-02 -8.19050000e-02\n",
      "  3.05465000e-02 -3.09370750e-02 -1.35045250e-01 -2.70407500e-02\n",
      " -1.92052500e-01 -1.91836250e-01  8.29750000e-03 -2.81687500e-01\n",
      "  1.75778500e-01  4.91232500e-02 -9.47670000e-02  7.02772500e-02\n",
      "  7.90907500e-02 -5.05695000e-02 -1.42987500e-01  2.34409250e-01\n",
      " -1.16687250e-01 -1.25750000e-03 -1.80577250e-01  1.04053000e-01\n",
      " -3.75435250e-01 -2.60007500e-01  1.44620500e-01 -2.63420000e-01\n",
      "  2.04558750e-01  1.81205500e-01  2.48255000e-02  7.99050000e-02\n",
      "  1.86391250e-01  3.28930000e-01  2.25586750e-01 -9.55277500e-02\n",
      "  2.39069500e-01 -1.86923500e-01  3.42059500e-01 -1.89871000e-01\n",
      "  3.71140000e-01  1.80936500e-01  9.12005000e-02 -4.33157500e-01\n",
      " -7.52920000e-02 -1.03422500e-01  1.45370000e-01 -2.07050000e-01\n",
      " -2.73462500e-01  2.68356750e-01  9.38500000e-03 -2.29925000e-01\n",
      "  6.12860000e-01  6.45072500e-02 -2.15395750e-01 -4.64660000e-01\n",
      " -1.83295000e-01 -1.53319250e-01 -4.20524500e-01 -3.52947500e-02\n",
      "  9.32250000e-02  1.49456500e-01  1.89100000e-01 -3.31170000e-01\n",
      "  1.79327500e-01 -1.07928425e-01 -2.83142500e-01 -8.89352500e-02\n",
      " -3.93750000e-04  8.63125000e-02 -1.41477750e-01 -3.31052750e-01\n",
      " -5.48262500e-02 -1.10097500e-02  1.69875500e-01 -1.67995000e-01\n",
      "  1.38892500e-01  2.23928000e-01  1.02282500e-01  3.49800000e-02\n",
      " -1.88763750e+00  1.77867750e-01  9.18185000e-02 -3.53640000e-01\n",
      "  5.19260000e-02 -2.71877500e-02 -1.51522500e-01  1.34275000e-01\n",
      "  3.80075000e-02 -1.84411500e-01  1.90271250e-01  1.07765000e-01\n",
      "  8.22195000e-02  6.15350000e-02  2.85875000e-03  8.78175000e-02\n",
      " -4.64635000e-02  2.22885000e-01 -3.91695000e-02  1.84552000e-01\n",
      "  5.70075000e-02  9.20250000e-02  2.25021500e-01  1.11260750e-02\n",
      " -1.20102250e-01 -5.77825000e-02 -2.47235375e-01  7.74970000e-02\n",
      " -5.49192500e-02  2.71965000e-02 -2.09474600e-01 -3.43496000e-01\n",
      " -1.52535000e-01 -1.75095250e-01 -1.65401250e-02  8.73800000e-03\n",
      " -1.28487500e-01  8.52182500e-02 -1.49912500e-01 -6.30089250e-02\n",
      "  8.81925000e-02 -2.23835000e-01 -2.71665000e-02  4.48250000e-03\n",
      " -4.78155000e-01  1.37375250e-01 -2.73342250e-01 -2.20546500e-01\n",
      " -4.30943000e-02  2.77492250e-01 -3.90257500e-02 -5.53750000e-02\n",
      "  1.07577500e-02  5.97537500e-02 -4.41935000e-01  2.05377500e-01\n",
      "  4.69792500e-01 -1.00227500e-01  1.18938500e-01 -5.84095000e-02\n",
      " -3.11498000e-01  1.23824250e-01  2.72535000e-02 -2.43812500e-02\n",
      " -6.57990000e-02 -3.29646250e-01 -1.81062500e-01  4.88200000e-02\n",
      " -1.91752500e-01 -5.70657500e-02 -1.46721000e-01 -1.73552500e-01\n",
      " -1.39049000e-01  9.51647500e-02 -4.99195000e-02 -1.51471500e-01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# average Word2Vec on question title cross-validation data\n",
    "# computing average word2vec for each question title\n",
    "avg_w2v_vectors_title_cv = []; # the avg-w2v for each question title is stored in this list\n",
    "for sentence in tqdm(x_cv['question_title']): # for each question title\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    cnt_words = 0; # num of words with a valid vector in the question title\n",
    "    for word in sentence.split(): # for each word in a question title\n",
    "        if word in glove_words:\n",
    "            vector += glove_vector[word]\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        vector /= cnt_words\n",
    "    avg_w2v_vectors_title_cv.append(vector)\n",
    "\n",
    "print(len(avg_w2v_vectors_title_cv))\n",
    "print(len(avg_w2v_vectors_title_cv[0]))\n",
    "print(avg_w2v_vectors_title_cv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "gTRDnHHwtceO",
    "outputId": "5587c33e-d5a9-44a8-b4ce-c5d5411729cc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5471/5471 [00:01<00:00, 2951.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5471\n",
      "300\n",
      "[ 4.98794606e-02 -2.12572411e-02 -6.48565844e-02 -1.26463426e-01\n",
      "  2.03091624e-01  3.57973348e-02 -3.53638298e+00  4.79139163e-01\n",
      "  7.07632823e-02 -2.95747733e-01  5.81099929e-02  3.62829177e-02\n",
      " -7.21722624e-03 -4.04728241e-02  4.52131773e-02 -1.52853888e-01\n",
      " -1.64983765e-01 -6.29017759e-02  4.54168277e-02  1.35403305e-02\n",
      "  3.51126809e-02 -2.66981947e-02 -1.54867447e-02 -1.97913149e-02\n",
      " -5.24133074e-02 -2.12453631e-01 -4.84618730e-02 -1.35368788e-01\n",
      " -1.28808004e-01 -1.01439223e-01 -1.41787845e-01  4.09677773e-02\n",
      "  1.53688397e-02 -1.32905341e-01 -5.87119730e-02 -3.81824965e-02\n",
      " -7.55141689e-02 -1.69814813e-01  1.09284373e-01  2.21162980e-02\n",
      " -7.57650377e-02  4.12727879e-02 -9.04703525e-02 -2.01967016e-01\n",
      "  2.91163262e-03 -5.60526809e-02  1.32698546e-02 -6.87434596e-02\n",
      " -5.89876965e-02 -5.25707943e-02 -5.52308809e-02  9.06011064e-02\n",
      " -8.61252950e-02 -1.87256170e-03 -3.02115149e-02 -1.14445507e-01\n",
      "  5.68741702e-03  1.62356028e-02 -6.83021369e-02  6.16429078e-02\n",
      "  4.69765035e-02 -1.19640326e-02  1.08375702e-02 -5.66737645e-02\n",
      "  1.75067454e-02  1.77331510e-01 -2.58054475e-02  3.78032574e-02\n",
      "  7.84627759e-02 -1.56847078e-01 -1.98755284e-02  6.39445624e-02\n",
      "  5.00227383e-02  7.81848511e-03  2.95593792e-02 -1.28090365e-01\n",
      "  1.23230061e-01  1.06950116e-01  7.16157723e-02 -1.34980891e-01\n",
      " -2.27960780e-03 -5.24174424e-01 -5.55255050e-02 -1.07918355e-01\n",
      "  8.52172305e-02 -2.57447129e-02 -1.66374709e-02 -5.37152128e-02\n",
      "  6.55449149e-02 -2.05729872e-02 -6.17916667e-02 -1.31142150e-01\n",
      " -6.92242780e-02  2.20131972e-02  5.45313078e-02 -2.34957845e-01\n",
      " -2.41311511e+00 -6.21634339e-02  9.28975539e-02  4.59231085e-02\n",
      "  4.02153915e-02  1.72516592e-01  1.05596313e-01  1.08050092e-02\n",
      " -4.35283759e-02 -2.14164092e-02  3.72004433e-02 -1.58114765e-01\n",
      " -1.13090071e-01 -4.79498276e-02  6.06643972e-04  1.99214295e-02\n",
      " -5.87072404e-02  1.00664995e-01 -1.52513026e-01  2.64457876e-02\n",
      "  1.87722608e-01 -4.26937429e-02  4.11471986e-02  3.10763546e-03\n",
      " -7.38934028e-02 -3.39931915e-02  6.35322966e-02 -1.42061012e-01\n",
      "  8.50588752e-02  1.66823915e-02  1.89512816e-02 -8.17460667e-02\n",
      "  3.49814929e-02  1.67269681e-01 -1.08132626e-01 -5.07879624e-02\n",
      " -4.53798865e-02 -6.96069078e-02  1.11555035e-02  1.95926957e-02\n",
      "  2.02811208e-02 -2.77328936e-02  1.40983262e-01  3.61700517e-01\n",
      "  1.48566460e-01  1.37885805e-01  6.41738826e-02 -9.04867404e-02\n",
      " -3.99433050e-03 -1.47677394e-02  5.72734610e-02 -1.03553791e-01\n",
      "  2.71302513e-01  1.06158845e-01  4.07152057e-02 -6.66638312e-02\n",
      " -7.02928738e-02 -2.96897905e-02  7.33964383e-02 -8.24243929e-02\n",
      " -7.78152482e-04 -5.79530901e-02 -1.00239087e-01 -7.87325270e-02\n",
      "  9.05984199e-02 -1.01459836e-01 -4.62304766e-02  1.53269830e-02\n",
      "  3.34430610e-02 -4.81709688e-02  1.65594007e-02  7.56280156e-02\n",
      "  1.23483482e-02  1.50036582e-02 -8.28350376e-02  1.03818723e-03\n",
      " -3.97068536e-02  4.45578652e-03 -1.64143348e-01  2.06927110e-01\n",
      " -6.12836667e-02 -1.31018652e-01  1.33230006e-01 -1.07228063e-01\n",
      "  9.74835759e-02  1.31194990e-01 -6.60586468e-02 -6.09872589e-02\n",
      "  3.14844326e-02 -8.60437894e-02  1.61272878e-01  2.13700496e-02\n",
      "  1.12519783e-01 -3.05205603e-03 -1.12220085e-02 -1.08874461e-02\n",
      "  4.68385489e-02 -1.34166447e-03  1.28456323e-01 -8.16278340e-02\n",
      "  2.02604764e-01  2.66953475e-02  1.99264316e-01 -1.18981308e-01\n",
      "  1.60712177e-01  3.30868085e-05 -4.47019206e-02  7.91669367e-02\n",
      "  7.08767021e-02  1.02225866e-01 -2.29956333e-02 -7.93463156e-02\n",
      "  1.34106213e-01 -5.82872206e-02  1.13159416e-01  1.36536018e-02\n",
      " -9.85140560e-02  2.23364518e-02 -5.98180709e-02  9.83636085e-02\n",
      " -9.32523809e-02 -1.64663291e-01 -7.06346730e-02  2.65015872e-02\n",
      " -4.89843553e-02 -1.68583505e-01 -7.56021270e-02 -8.35452837e-03\n",
      " -2.70259230e+00  1.80302082e-01  2.42039638e-02 -4.92610791e-02\n",
      " -1.00932957e-02  4.79464113e-03  3.73665411e-02  4.36569915e-02\n",
      "  8.43523560e-02  6.13302766e-03 -9.49727092e-03  3.59117841e-02\n",
      " -4.92265035e-03 -5.76638156e-03 -1.01333922e-01 -4.71405220e-02\n",
      " -3.33042638e-02  1.04684292e-02 -1.79160489e-01 -7.37920404e-02\n",
      " -1.18318645e-02 -1.20504177e-02 -9.22756071e-02 -3.20980301e-02\n",
      " -5.86419135e-02  1.14205487e-01 -1.01794175e-01  4.37931752e-02\n",
      " -1.37332424e-01  6.21543458e-02  8.71295865e-02 -8.38517143e-02\n",
      " -5.64723830e-03 -2.43580482e-02 -7.06110667e-02  6.31516000e-02\n",
      "  3.28157730e-03  2.24491837e-02 -1.60155596e-02 -6.36787801e-02\n",
      "  6.71599170e-02 -9.25740213e-02 -1.54449752e-01 -1.27966674e-01\n",
      "  7.90861277e-02  1.01427057e-02  3.00060370e-02 -7.24573782e-02\n",
      " -8.06786113e-02  1.18685859e-01  6.35410433e-02  8.22161326e-02\n",
      "  2.85757213e-02  1.52671576e-01  1.10557525e-01 -2.36819206e-02\n",
      "  2.41664836e-01  2.58891390e-02 -5.03224894e-03  6.64795652e-02\n",
      " -2.21670241e-02  1.06810072e-01 -1.05906667e-03  6.32175135e-02\n",
      " -1.12068135e-02 -1.26174654e-01 -4.58347965e-02 -7.28085447e-02\n",
      " -8.59897444e-02  1.19473220e-02  6.39882128e-03  4.50204397e-02\n",
      " -7.41516823e-02 -6.57814399e-02  1.13112972e-01 -4.43174546e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# average Word2Vec on question body train data\n",
    "# computing average word2vec for each question body\n",
    "avg_w2v_vectors_body_train = []; # the avg-w2v for each question body is stored in this list\n",
    "for sentence in tqdm(x_train['question_body']): # for each question body\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    cnt_words = 0; # num of words with a valid vector in the question body\n",
    "    for word in sentence.split(): # for each word in a question body\n",
    "        if word in glove_words:\n",
    "            vector += glove_vector[word]\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        vector /= cnt_words\n",
    "    avg_w2v_vectors_body_train.append(vector)\n",
    "\n",
    "print(len(avg_w2v_vectors_body_train))\n",
    "print(len(avg_w2v_vectors_body_train[0]))\n",
    "print(avg_w2v_vectors_body_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "IifwtoK8tceP",
    "outputId": "36fa2000-943e-4597-8310-0b26c42ccd79",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 608/608 [00:00<00:00, 3160.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608\n",
      "300\n",
      "[-4.77066206e-02 -3.67971991e-02 -7.95059230e-02 -1.45002847e-01\n",
      "  1.14980955e-01 -6.46524503e-02 -3.53642534e+00  3.09715303e-01\n",
      "  6.57084185e-03 -3.66188867e-01  1.11071671e-01 -4.56330904e-02\n",
      " -9.22942303e-03 -5.77239489e-02  2.74830337e-03 -8.23516256e-02\n",
      " -1.15281087e-01 -1.48810499e-02  8.16607815e-02  7.59273249e-02\n",
      "  1.17264192e-01 -6.63509849e-02  3.98944310e-02 -3.58635242e-02\n",
      " -4.68213350e-02 -1.14332487e-01 -1.20661500e-02 -1.41242496e-01\n",
      " -4.21282062e-02  2.79500882e-03 -1.52101680e-01 -8.57789326e-03\n",
      "  1.05387602e-01 -2.28425044e-02 -5.07341489e-02 -8.58146067e-04\n",
      "  1.16952084e-02 -7.90263039e-02  1.77858287e-01 -1.01621338e-01\n",
      "  8.52448365e-03  6.53265354e-02 -3.54712899e-02 -2.10438234e-01\n",
      "  6.77647191e-03  1.24100820e-02  3.55606202e-02 -1.64803879e-01\n",
      " -7.43726140e-02  1.05413230e-02 -3.92168803e-02 -1.13082225e-02\n",
      " -3.11190880e-02 -7.60509051e-02 -4.59721067e-03 -8.48952081e-02\n",
      "  5.44735753e-02 -6.78040983e-02  8.33929607e-03  5.35119488e-02\n",
      "  8.96809708e-02 -3.26371346e-02 -1.45076539e-02  3.93778034e-03\n",
      "  1.92628428e-02  1.71768372e-01  6.08340449e-04  1.24668060e-01\n",
      "  8.05885972e-02 -2.09401326e-01 -8.70482657e-02  4.41879388e-02\n",
      " -2.61268596e-03 -7.83478047e-02 -4.33754753e-02 -1.37863198e-01\n",
      "  2.34334717e-01  9.68801517e-03 -4.65867882e-02 -5.77723972e-02\n",
      " -4.66669724e-02 -3.63963851e-01 -1.27466903e-01 -9.15471369e-02\n",
      "  6.22037556e-02 -6.25391180e-03  3.25945582e-02 -6.93548990e-02\n",
      " -5.35244944e-03 -1.03422789e-01  2.22854735e-02 -9.44820921e-02\n",
      " -7.01275000e-02  9.69716534e-02  4.22631972e-02 -2.62922981e-01\n",
      " -2.05392118e+00 -6.64999991e-02  1.65215421e-01  6.20243224e-02\n",
      "  3.18391045e-02  1.16227825e-01  1.01185129e-01  1.91711556e-02\n",
      " -7.99863343e-02 -1.50422370e-01  6.88524062e-02 -1.73615128e-01\n",
      "  2.15647635e-02 -6.39165770e-02  4.31943431e-02  3.44261910e-04\n",
      " -5.49903073e-02  9.17189354e-02 -1.05007607e-01  1.02883273e-01\n",
      "  1.84664470e-01 -9.02973112e-02  6.47622169e-02  3.99231073e-02\n",
      " -4.10044461e-02  1.91732213e-02  4.93140313e-02 -3.43361006e-02\n",
      "  8.75929213e-03 -1.22423246e-01  2.62510374e-02 -1.19480562e-01\n",
      "  1.58869133e-02  2.36444916e-02 -9.96006647e-02 -6.56607798e-03\n",
      " -1.33605000e-02 -9.34964162e-02  2.24707876e-02  3.66262242e-02\n",
      "  8.41912197e-02  8.23829315e-03  1.37556258e-01  3.23494804e-01\n",
      "  5.73392185e-02  9.38488483e-02  1.10569787e-01  4.98293124e-03\n",
      "  3.77124775e-03 -1.58864055e-01  6.81678764e-02 -1.14242017e-01\n",
      "  1.90043096e-01  1.30349347e-01 -1.84062331e-02 -2.51464149e-02\n",
      " -5.70465084e-02 -9.43382434e-02  5.33183140e-02 -5.47797876e-02\n",
      "  3.17252015e-02 -1.39304949e-02 -1.22845281e-01 -6.50704461e-02\n",
      "  1.12145629e-01  1.89281966e-03 -9.19692596e-02 -2.39701966e-02\n",
      "  8.20462649e-03  3.42909225e-02 -1.20687966e-02  5.48641961e-02\n",
      "  9.12533640e-02 -1.32018468e-01 -3.38035427e-02 -5.23837315e-02\n",
      " -6.30301135e-02  1.25322830e-02 -7.08517096e-02  2.51989008e-01\n",
      " -6.97091092e-02  5.76043938e-02  1.50065169e-02 -1.25302784e-01\n",
      "  1.65336027e-01  2.47798115e-02  4.81706348e-03 -9.49014455e-02\n",
      "  2.49780562e-03 -1.06714684e-01  5.02585517e-02 -7.82412247e-02\n",
      "  1.11252898e-01 -5.21647079e-02  5.85607084e-02 -1.15152978e-01\n",
      "  2.07317764e-02 -3.46754185e-02  5.77989815e-02 -6.14194511e-02\n",
      "  2.26369779e-01 -6.22539382e-02  6.63738927e-02 -1.32256775e-01\n",
      "  1.56902281e-01 -6.23357408e-02 -2.02601054e-01  7.18009092e-02\n",
      "  7.07072534e-03  1.34319766e-01 -2.21438382e-02 -6.09910660e-02\n",
      "  2.52758713e-01 -3.28038933e-02  6.43818393e-02 -6.52292034e-02\n",
      " -4.59278272e-02 -1.07467117e-01  2.77199202e-02  2.56669101e-03\n",
      " -7.71441943e-02 -6.67623691e-02  2.05517770e-02 -8.31256871e-02\n",
      " -2.40492163e-02 -7.59013916e-02  4.28827360e-02 -5.07708034e-03\n",
      " -2.77712550e+00  8.56875213e-02  8.92909101e-03 -1.97997607e-02\n",
      "  8.12972534e-02 -4.92892691e-02  1.37135919e-02 -4.75527079e-03\n",
      "  2.73188567e-02 -6.51307652e-02 -2.32983427e-02  1.24997836e-01\n",
      "  2.77016388e-02  8.85908028e-02 -1.52024623e-01  2.97286612e-02\n",
      " -1.31063184e-01  6.39625247e-02 -3.32858117e-01  1.10941671e-01\n",
      "  4.85277827e-02 -1.42819112e-02  2.18327376e-02  2.08650769e-02\n",
      " -1.29643195e-01  8.79328967e-02 -2.04362107e-02 -8.00601466e-02\n",
      " -2.38902489e-02 -8.71454315e-03  1.10436934e-01 -5.65092970e-02\n",
      "  4.86397107e-02 -6.49746899e-02 -6.86339062e-02  2.06001000e-02\n",
      " -3.83611596e-02 -7.35246236e-02 -2.54913944e-02 -1.05995813e-01\n",
      "  3.83609421e-02 -3.71734230e-02 -1.32919715e-01 -6.94276781e-02\n",
      "  3.14605067e-02 -2.98664079e-02  5.74708781e-02 -1.67226355e-01\n",
      " -1.54932611e-01 -6.12219814e-02 -7.61740000e-03  2.56653663e-02\n",
      "  1.32262013e-01  1.59223477e-01 -6.69903608e-02 -9.32483146e-05\n",
      "  5.66816663e-01 -9.25820618e-02  3.94652315e-02  7.56193596e-02\n",
      " -3.99228028e-02  6.39662639e-02 -1.21096764e-02  3.17876793e-02\n",
      " -8.26543157e-02 -8.80103876e-02 -1.52076283e-02 -7.86459315e-02\n",
      " -1.09271469e-01 -5.06193961e-02 -1.51604178e-02 -1.16009213e-03\n",
      " -2.91489337e-02 -5.35628266e-02  2.61097140e-02 -2.67730425e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# average Word2Vec on question body test data\n",
    "# computing average word2vec for each question body\n",
    "avg_w2v_vectors_body_cv = []; # the avg-w2v for each question body is stored in this list\n",
    "for sentence in tqdm(x_cv['question_body']): # for each question body\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    cnt_words = 0; # num of words with a valid vector in the question body\n",
    "    for word in sentence.split(): # for each word in a question body\n",
    "        if word in glove_words:\n",
    "            vector += glove_vector[word]\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        vector /= cnt_words\n",
    "    avg_w2v_vectors_body_cv.append(vector)\n",
    "\n",
    "print(len(avg_w2v_vectors_body_cv))\n",
    "print(len(avg_w2v_vectors_body_cv[0]))\n",
    "print(avg_w2v_vectors_body_cv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "eE4pA0rKtceS",
    "outputId": "c497f960-77af-4d89-9277-f4cc12ccd81f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5471/5471 [00:01<00:00, 3066.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5471\n",
      "300\n",
      "[ 8.61519512e-03 -9.29324024e-04 -1.57759990e-01 -1.36909321e-01\n",
      "  1.91203122e-01 -3.76685244e-03 -3.64999366e+00  4.91381598e-01\n",
      "  3.52693829e-02 -3.36133029e-01  5.57536098e-02  5.93754634e-03\n",
      "  4.50884146e-03 -1.15222299e-01  2.01714793e-02 -1.22932144e-01\n",
      " -8.16006524e-02 -6.26017598e-02  7.27306699e-02  1.49325854e-03\n",
      "  7.17951346e-02  1.26463567e-02 -1.30942682e-02 -9.47087195e-03\n",
      " -5.97328570e-02 -9.47539878e-02  1.13816122e-02 -1.31201437e-01\n",
      " -6.42167343e-02 -9.77926390e-02 -1.74240971e-01  3.89259756e-03\n",
      "  3.13054195e-02 -1.26902694e-01 -5.44201341e-02  2.56190756e-02\n",
      " -6.98066195e-02 -1.44711662e-01  1.20145143e-01  4.18859268e-02\n",
      "  5.63222805e-03  7.82686366e-02 -8.16191805e-02 -1.60154920e-01\n",
      " -1.90247841e-02 -1.22264598e-01  5.08936636e-02 -8.67424707e-02\n",
      " -6.85268744e-02 -6.23022976e-02 -2.68359268e-03  9.68623695e-02\n",
      " -3.75606122e-02 -3.89778171e-03  1.58844573e-02 -1.31056734e-01\n",
      " -2.08623000e-02  4.93684756e-02 -7.02991268e-02 -4.82461220e-03\n",
      "  2.77998329e-02 -2.52380524e-02  5.30194476e-02 -6.85306000e-02\n",
      "  1.91411432e-02  1.92793279e-01  1.29896427e-02  5.94819146e-02\n",
      "  7.94188976e-02 -1.90419280e-01 -7.94341256e-02  1.13704672e-01\n",
      "  4.78546110e-02 -4.99478390e-02  4.21514329e-02 -1.25464341e-01\n",
      "  1.43932215e-01  5.84334512e-02  9.14651756e-02 -8.27046780e-02\n",
      " -3.68656354e-02 -4.93569927e-01 -8.61005000e-02 -1.45773597e-01\n",
      "  8.77642146e-02 -7.61606341e-03 -7.31858415e-03 -5.01099012e-02\n",
      "  8.49251341e-02 -3.39414037e-02  1.18951215e-02 -9.66298610e-02\n",
      " -3.98424390e-02  3.40648423e-02  6.42755793e-02 -2.22430780e-01\n",
      " -2.31479341e+00 -1.04101288e-01  1.36407354e-01  6.51309390e-02\n",
      "  2.84640427e-02  1.44197268e-01  1.43867341e-01  5.96631061e-02\n",
      " -4.24200805e-02 -9.72904878e-02  9.83239402e-02 -9.94089073e-02\n",
      " -7.48953148e-02 -9.78458408e-02  4.83725390e-02  2.52861639e-02\n",
      " -3.85719146e-02  1.13804268e-01 -1.77271052e-01  4.52084268e-02\n",
      "  1.67773643e-01 -9.80136451e-02  2.34459146e-02  6.60472902e-02\n",
      " -8.72261561e-02 -5.55373293e-02  4.76920027e-02 -1.52591021e-01\n",
      " -1.11122280e-02 -4.71062683e-02  7.55548341e-03 -1.28749948e-01\n",
      "  3.55989622e-02  1.80963832e-01 -1.44611793e-02 -4.00974256e-02\n",
      " -8.83948171e-02 -8.34293098e-02  2.13666585e-02  3.38268915e-02\n",
      "  6.64659341e-02  6.79723049e-04  1.61090096e-01  2.69008705e-01\n",
      "  7.30414780e-02  1.66722366e-01  1.15922358e-01 -1.17736573e-02\n",
      " -5.19253280e-02 -3.80069683e-02  7.43353780e-02 -1.33245756e-01\n",
      "  2.20470526e-01  1.81388732e-01  1.42300976e-02  6.68073171e-03\n",
      " -1.00806000e-02 -8.93248894e-02  2.66597073e-02 -5.77467061e-02\n",
      "  1.90132537e-02 -2.98342061e-02 -1.26827905e-01 -1.57135463e-02\n",
      "  1.29694366e-01 -4.76318780e-02 -4.95025829e-02  7.32289683e-03\n",
      "  2.89192561e-03 -2.75457305e-02  1.36834976e-02  1.12662085e-01\n",
      "  7.53215561e-02 -5.31104305e-02 -7.67696415e-02 -2.88970378e-02\n",
      " -1.04867649e-01 -8.46520122e-02 -1.53131433e-01  2.44146195e-01\n",
      " -7.07599024e-02 -1.59847296e-01  1.09530132e-01 -4.89312463e-02\n",
      "  1.81832540e-01  5.23275488e-02 -7.47672744e-02 -6.10570976e-02\n",
      "  6.79826829e-03 -1.37580798e-01  1.47084798e-01  9.06647561e-03\n",
      "  5.70978000e-02  3.52921402e-02  1.49191201e-02 -1.40648537e-02\n",
      "  5.02816402e-02 -7.15160763e-02  5.36464756e-02 -3.75016927e-02\n",
      "  2.34704909e-01  3.32495317e-02  1.52366838e-01 -6.33064017e-02\n",
      "  1.48172016e-01 -4.03024485e-02 -1.10618721e-01  6.85415862e-02\n",
      "  5.53432337e-02  1.87305385e-01 -1.65506976e-02 -3.67820732e-02\n",
      "  2.37807854e-01 -3.52441415e-02  2.27027524e-01 -2.62748549e-02\n",
      " -1.23749183e-01 -4.92010732e-03  1.25790244e-02  1.01997773e-01\n",
      " -4.94894756e-02 -9.05235817e-02 -3.62493539e-02 -3.96574817e-02\n",
      " -5.92829854e-02 -1.49552463e-01 -3.39048268e-02 -3.04185317e-02\n",
      " -2.64199146e+00  1.90504211e-01  5.15834195e-02 -5.19685854e-02\n",
      "  1.39602951e-02 -4.21857317e-02 -3.57483659e-03  1.76967707e-02\n",
      "  7.08380634e-02 -2.42455268e-02 -9.04359457e-02  7.26064832e-02\n",
      "  5.85775366e-03  9.02906537e-02 -1.46252218e-01 -1.84833866e-02\n",
      " -9.48305244e-02  3.30811041e-02 -2.41863622e-01  1.20048244e-02\n",
      "  2.27981951e-02 -8.66050110e-02 -5.81475732e-02  7.18740976e-03\n",
      " -3.24230866e-02  1.26319162e-01 -7.84764061e-02 -5.74908695e-02\n",
      " -1.18360030e-01 -3.56968111e-02  1.02697502e-01 -7.37096585e-02\n",
      "  9.15082439e-03 -2.11353546e-02 -1.15939276e-01  6.88263451e-02\n",
      "  4.82585927e-02  2.02511171e-02  3.02521951e-04 -8.04879793e-02\n",
      "  5.32304780e-02 -9.58593402e-02 -1.76387691e-01 -1.29122050e-01\n",
      "  9.99773573e-02 -3.51438427e-02  7.20106929e-02 -1.39364150e-01\n",
      " -6.63552280e-02  2.93916244e-02  5.67465963e-02  5.12482671e-02\n",
      "  3.67764634e-02  1.47520351e-01  2.45819720e-02 -5.46113695e-02\n",
      "  3.34646380e-01 -9.11036585e-03  1.90015854e-02  1.39466671e-01\n",
      "  1.03476537e-02  1.31820837e-01  1.44658549e-02  4.58649415e-02\n",
      " -9.45942561e-02 -1.53627000e-01 -4.95027780e-02 -1.34020901e-01\n",
      " -7.02239427e-02  4.66195202e-02  1.98057756e-02  3.85421220e-02\n",
      " -8.11218293e-03 -8.07257551e-02  8.78739110e-02 -7.42232134e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# average Word2Vec on answer train data\n",
    "# computing average word2vec for each answer\n",
    "avg_w2v_vectors_answer_train = []; # the avg-w2v for each answer is stored in this list\n",
    "for sentence in tqdm(x_train['answer']): # for each answer\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    cnt_words = 0; # num of words with a valid vector in the answer\n",
    "    for word in sentence.split(): # for each word in a answer\n",
    "        if word in glove_words:\n",
    "            vector += glove_vector[word]\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        vector /= cnt_words\n",
    "    avg_w2v_vectors_answer_train.append(vector)\n",
    "\n",
    "print(len(avg_w2v_vectors_answer_train))\n",
    "print(len(avg_w2v_vectors_answer_train[0]))\n",
    "print(avg_w2v_vectors_answer_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "n8-4zEUwtceU",
    "outputId": "1f0b1bed-3190-496d-ae92-05494747dc66",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 608/608 [00:00<00:00, 3166.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608\n",
      "300\n",
      "[-5.90135045e-02 -6.43529929e-03 -8.41833184e-02 -1.74959924e-01\n",
      "  1.02585126e-01 -7.85644298e-02 -3.49469454e+00  4.01480475e-01\n",
      "  1.28683979e-02 -3.81835355e-01  8.86141752e-02 -7.62583740e-02\n",
      "  6.22462695e-02 -3.03424220e-02 -7.40294255e-03 -1.04028867e-01\n",
      " -1.24215296e-01 -8.60593901e-03  4.91942333e-02  1.15728033e-01\n",
      "  7.18986411e-02 -5.01357899e-02  8.25676242e-02 -1.39601227e-02\n",
      "  8.49892638e-03 -3.14943057e-02 -6.70346092e-02 -7.82360993e-02\n",
      " -4.02521726e-02 -3.75388057e-02 -1.57652179e-01 -1.89744319e-02\n",
      "  2.12747312e-02 -8.38442028e-02 -4.91979851e-02  2.38825617e-02\n",
      " -2.34785525e-02 -7.62183594e-02  4.86225095e-02 -8.01462860e-02\n",
      "  5.34124071e-02  9.24265135e-02 -2.56620184e-02 -1.94077223e-01\n",
      "  3.88792582e-02  5.36566525e-02 -9.50208085e-03 -1.69287803e-01\n",
      " -1.96257175e-01  3.07174075e-02 -5.92287285e-02  2.67197624e-02\n",
      " -2.21099369e-02 -4.11945199e-02 -8.15581560e-04 -1.29040941e-01\n",
      "  1.03935818e-01 -5.78184979e-02  3.66920567e-04  8.67393340e-02\n",
      "  8.20887660e-02  3.61144113e-03 -3.86534184e-02 -7.15561397e-02\n",
      "  2.40248326e-02  2.02418565e-01  5.53088163e-02  1.42899975e-01\n",
      "  9.99931028e-02 -9.85557000e-02 -1.08222355e-01  1.14926590e-01\n",
      " -1.12098952e-01 -9.56049043e-02 -5.40264206e-02 -1.61023360e-01\n",
      "  1.71924468e-01 -1.02945115e-02  7.00946078e-02 -9.19293376e-02\n",
      " -1.08755113e-01 -2.46547930e-01 -1.53507686e-01 -9.60674163e-02\n",
      "  9.80984461e-02  2.32621277e-02 -4.51185248e-03 -2.55505858e-02\n",
      " -3.54390894e-02 -3.68011889e-02  6.68731106e-03 -1.43872504e-01\n",
      " -3.56543957e-02  9.99987877e-02  7.11972305e-02 -3.33712921e-01\n",
      " -2.13285620e+00 -1.19135480e-01  1.17395495e-01  3.87087234e-02\n",
      "  4.32834752e-02  1.09204879e-01  1.69018643e-01  5.34695255e-02\n",
      " -1.49169470e-01 -1.23295443e-01  1.05825640e-01 -1.70531430e-01\n",
      " -1.02650468e-01 -5.90700034e-02  2.90192305e-02  1.21971709e-02\n",
      " -2.88137199e-02  7.06514440e-02 -2.08324592e-01  2.31677635e-02\n",
      "  1.83274886e-01 -9.15350326e-02  9.45778043e-02  9.30724497e-02\n",
      " -1.67737937e-01  4.39197684e-02  9.95468501e-02 -1.73573755e-01\n",
      "  2.03773879e-02 -8.25925319e-02  6.65262506e-02 -1.36571787e-01\n",
      "  2.30356091e-02  8.79068542e-02 -9.35596716e-02 -6.58881957e-02\n",
      " -2.19707259e-02 -1.01378894e-01 -2.59954482e-02  9.46032270e-02\n",
      "  5.91320021e-02 -1.09674059e-02  2.04782552e-01  3.02846765e-01\n",
      "  7.92476007e-02  1.70877794e-02  1.27476946e-01  1.78160199e-02\n",
      " -5.08383802e-02 -1.91017273e-02  5.17034504e-03 -1.00930248e-01\n",
      "  2.31096582e-01  2.20164118e-01 -2.10235064e-02 -1.74583569e-02\n",
      " -6.24398057e-02 -1.19016163e-01  9.58357539e-02 -1.31522302e-01\n",
      " -1.83061638e-02 -4.95136986e-02 -7.99654560e-02 -3.96708851e-02\n",
      "  1.24494445e-01 -1.33694376e-02 -1.27472972e-01  5.54065730e-02\n",
      " -1.05976609e-01 -5.50712234e-02 -1.95985149e-02  1.17096921e-01\n",
      "  1.17541527e-01 -1.51995035e-03 -2.70322410e-02 -3.87209426e-02\n",
      " -1.04648501e-01  8.41753922e-02 -9.66942915e-02  2.09609908e-01\n",
      "  2.31029389e-02 -1.72462199e-03  1.02480388e-01 -1.19153436e-01\n",
      "  2.11725973e-01 -1.98118560e-02 -6.05100326e-02 -8.96950589e-02\n",
      "  2.76888085e-02 -1.58937566e-01  2.85018128e-02 -3.54472333e-02\n",
      "  1.49344131e-01 -7.96180298e-02  4.77337014e-02 -6.53717801e-02\n",
      "  7.94719986e-02 -3.95725394e-02  9.50709149e-02 -1.59921090e-01\n",
      "  2.37909916e-01 -1.95915745e-02  1.32863169e-01 -2.42720014e-02\n",
      "  1.19288111e-01  1.83712335e-02 -1.23084319e-01  9.13653610e-02\n",
      "  4.90404952e-02  1.06123499e-01  9.68589645e-03 -3.06783007e-02\n",
      "  1.72164936e-01 -2.47812957e-02 -4.02784440e-02 -8.75065248e-03\n",
      " -6.27378156e-02 -6.45598645e-02  2.96850000e-02  2.06201291e-02\n",
      " -9.51690766e-02 -1.23407384e-01 -3.40400638e-03 -7.12515865e-02\n",
      " -1.20343241e-02 -8.01387461e-02  5.12776508e-02 -6.73279078e-02\n",
      " -2.80391702e+00  1.40645545e-01 -3.12444482e-02 -4.59588156e-02\n",
      "  7.53175262e-02 -2.37340922e-02  4.05235248e-02  4.19790539e-02\n",
      "  3.09605872e-02 -4.75787901e-02 -7.89956922e-02  9.37635866e-02\n",
      "  3.69289736e-02 -3.95100355e-03 -1.52955092e-01 -4.42385404e-02\n",
      " -1.72719557e-01  1.07381517e-01 -1.99572306e-01  7.65753560e-02\n",
      "  1.39642241e-02  1.15236950e-03  6.66606787e-02  4.94289376e-03\n",
      " -1.22056364e-01  2.76761305e-02 -8.07566709e-02 -6.99699433e-02\n",
      " -4.84028213e-02 -3.29269474e-02  5.30327447e-02 -9.49838553e-03\n",
      " -1.93006929e-02 -3.25520702e-02 -1.49125282e-01  5.18238589e-02\n",
      " -1.69532220e-02 -6.51569539e-02  4.42230894e-02 -1.69708674e-01\n",
      "  9.33141481e-02 -8.10260064e-02 -1.25696232e-01  7.67347660e-03\n",
      "  1.07902953e-01 -6.64821844e-03  2.73466894e-02 -2.39188621e-01\n",
      " -1.71164337e-01  1.63073177e-02 -1.78029617e-02  2.40378411e-02\n",
      "  8.77485092e-02  1.69955531e-01 -4.50430638e-02 -7.42916617e-02\n",
      "  6.01559616e-01 -6.94223241e-02  7.22288085e-03  3.75951135e-02\n",
      " -1.70396525e-02  8.09460007e-02  4.06817071e-02  5.25897248e-03\n",
      " -4.69192078e-02 -1.76630567e-01 -7.29141688e-02 -1.58313723e-01\n",
      " -8.68615426e-02 -1.15704957e-02  1.70554454e-02 -8.25708695e-03\n",
      "  1.62731631e-03 -1.51371351e-01  5.62455532e-02 -5.69766922e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# average Word2Vec on answer train data\n",
    "# computing average word2vec for each answer\n",
    "avg_w2v_vectors_answer_cv = []; # the avg-w2v for each answer is stored in this list\n",
    "for sentence in tqdm(x_cv['answer']): # for each answer\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    cnt_words = 0; # num of words with a valid vector in the answer\n",
    "    for word in sentence.split(): # for each word in a answer\n",
    "        if word in glove_words:\n",
    "            vector += glove_vector[word]\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        vector /= cnt_words\n",
    "    avg_w2v_vectors_answer_cv.append(vector)\n",
    "\n",
    "print(len(avg_w2v_vectors_answer_cv))\n",
    "print(len(avg_w2v_vectors_answer_cv[0]))\n",
    "print(avg_w2v_vectors_answer_cv[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNbGDks5tceW"
   },
   "source": [
    "List of final features to be used - \n",
    "\n",
    "Text features - \n",
    "1. avg_w2v_vectors_title (Question title)\n",
    "2. avg_w2v_vectors_body (Question title)\n",
    "3. avg_w2v_vectors_answer (Answer)\n",
    "\n",
    "Categorical features - \n",
    "\n",
    "1. category_encoded (Category)\n",
    "2. host_encoded (Host)\n",
    "\n",
    "Numerical features\n",
    "\n",
    "1. q_title_length (Length of question title)\n",
    "2. q_body_length (Length of question body)\n",
    "3. answer_length (Length of Answer)\n",
    "4. q_title_neg  (Negative score of Question title)\n",
    "5. q_title_neu  (Neutral score of Question title)\n",
    "6. q_title_pos  (Positive score of Question title)\n",
    "7. q_title_comp (Compound score of Question title)\n",
    "8. q_body_neg   (Negative score of Question body)\n",
    "9. q_body_neu   (Neutral score of Question body)\n",
    "10. q_body_pos  (Positive score of Question body)\n",
    "11. q_body_comp (Compound score of Question body)\n",
    "12. q_answer_neg (Negative score of Answer)\n",
    "13. q_answer_neu (Positive score of Answer)\n",
    "14. q_answer_pos (Neutral score of Answer)\n",
    "15. q_answer_comp (Compound score of Answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "dsC495mCtceX",
    "outputId": "92cf5eb8-ef33-4eee-d4e3-46b352bdd531"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question_title', 'question_body', 'answer', 'category', 'host',\n",
       "       'q_title_length', 'q_body_length', 'answer_length', 'q_title_neg',\n",
       "       'q_title_neu', 'q_title_pos', 'q_title_comp', 'q_body_neg',\n",
       "       'q_body_neu', 'q_body_pos', 'q_body_comp', 'q_answer_neg',\n",
       "       'q_answer_neu', 'q_answer_pos', 'q_answer_comp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "XdbZXfwMtcea",
    "outputId": "170dedeb-6c8e-4add-e052-4c92b314779c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train encoded data:  (5471, 926)\n",
      "Shape of cross-validation encoded data:  (608, 926)\n"
     ]
    }
   ],
   "source": [
    "# concatenating encoded vectors of train data\n",
    "\n",
    "train_vect = np.hstack([avg_w2v_vectors_title_train,avg_w2v_vectors_body_train,\n",
    "                        avg_w2v_vectors_answer_train,category_encoded_train,host_encoded_train,\n",
    "                        x_train[['q_title_length','q_body_length','answer_length', 'q_title_neg',\n",
    "                                 'q_title_neu', 'q_title_pos', 'q_title_comp', 'q_body_neg',\n",
    "                                 'q_body_neu', 'q_body_pos', 'q_body_comp', 'q_answer_neg',\n",
    "                                 'q_answer_neu', 'q_answer_pos', 'q_answer_comp']]])\n",
    "\n",
    "\n",
    "cv_vect = np.hstack([avg_w2v_vectors_title_cv,avg_w2v_vectors_body_cv,\n",
    "                        avg_w2v_vectors_answer_cv,category_encoded_cv,host_encoded_cv,\n",
    "                        x_cv[['q_title_length','q_body_length','answer_length', 'q_title_neg',\n",
    "                                 'q_title_neu', 'q_title_pos', 'q_title_comp', 'q_body_neg',\n",
    "                                 'q_body_neu', 'q_body_pos', 'q_body_comp', 'q_answer_neg',\n",
    "                                 'q_answer_neu', 'q_answer_pos', 'q_answer_comp']]])\n",
    "\n",
    "print(\"Shape of train encoded data: \",train_vect.shape)\n",
    "print(\"Shape of cross-validation encoded data: \",cv_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "alCG-lbgtcec",
    "outputId": "63c825c3-2aba-421c-c71c-0c7c5bac0ea4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train encoded data:  (5471, 900) (5471, 26)\n",
      "Shape of cross-validation encoded data:  (608, 900) (608, 26)\n"
     ]
    }
   ],
   "source": [
    "# concatenating encoded vectors of train data\n",
    "\n",
    "train_vect_text = np.hstack([avg_w2v_vectors_title_train,avg_w2v_vectors_body_train,\n",
    "                        avg_w2v_vectors_answer_train])\n",
    "\n",
    "train_vect_cat_num = np.hstack([category_encoded_train,host_encoded_train,\n",
    "                        x_train[['q_title_length','q_body_length','answer_length', 'q_title_neg',\n",
    "                                 'q_title_neu', 'q_title_pos', 'q_title_comp', 'q_body_neg',\n",
    "                                 'q_body_neu', 'q_body_pos', 'q_body_comp', 'q_answer_neg',\n",
    "                                 'q_answer_neu', 'q_answer_pos', 'q_answer_comp']]])\n",
    "\n",
    "cv_vect_text = np.hstack([avg_w2v_vectors_title_cv,avg_w2v_vectors_body_cv,\n",
    "                        avg_w2v_vectors_answer_cv])\n",
    "\n",
    "cv_vect_cat_num = np.hstack([category_encoded_cv,host_encoded_cv,\n",
    "                        x_cv[['q_title_length','q_body_length','answer_length', 'q_title_neg',\n",
    "                                 'q_title_neu', 'q_title_pos', 'q_title_comp', 'q_body_neg',\n",
    "                                 'q_body_neu', 'q_body_pos', 'q_body_comp', 'q_answer_neg',\n",
    "                                 'q_answer_neu', 'q_answer_pos', 'q_answer_comp']]])\n",
    "\n",
    "print(\"Shape of train encoded data: \",train_vect_text.shape,train_vect_cat_num.shape)\n",
    "print(\"Shape of cross-validation encoded data: \",cv_vect_text.shape,cv_vect_cat_num.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "7C-585QBtcee",
    "outputId": "96a3478d-7a1f-4e50-80a7-27992d0533e8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 476/476 [00:00<00:00, 21647.55it/s]\n",
      "100%|██████████| 476/476 [00:00<00:00, 3232.79it/s]\n",
      "100%|██████████| 476/476 [00:00<00:00, 3202.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(476, 926)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# preparing test data\n",
    "\n",
    "# reading test data from csv file\n",
    "test = pd.read_csv(\"../input/google-quest-challenge/test.csv\")\n",
    "\n",
    "#preprocessing host\n",
    "test['host'] = test['host'].apply(lambda x: x.split('.')[-2])\n",
    "test['host'] = test['host'].apply(lambda x: x.lower())\n",
    "test['host'] = test['host'].apply(lambda x: x.strip())\n",
    "#=================================================================================================\n",
    "# preprocessing 'category'  to lower-case and stripping leading and tailing spaces \n",
    "test['category'] = test['category'].apply(lambda x: x.lower())\n",
    "test['category'] = test['category'].apply(lambda x: x.strip())\n",
    "#=================================================================================================\n",
    "\n",
    "category_encoded_test = category_ohe.transform(test['category']).todense()\n",
    "host_encoded_test = host_ohe.transform(test['host']).todense()\n",
    "\n",
    "#=================================================================================================\n",
    "# taking length of question title, question body, answer\n",
    "test['q_title_length'] = test['question_title'].apply(lambda x: len(x.split(' ')))\n",
    "test['q_body_length'] = test['question_body'].apply(lambda x: len(x.split(' ')))\n",
    "test['answer_length'] = test['answer'].apply(lambda x: len(x.split(' ')))\n",
    "#=================================================================================================\n",
    "# preprocessing question title, question body and answers\n",
    "test['question_title'] = test['question_title'].apply(lambda x: preprocess_text(x))\n",
    "test['question_body'] = test['question_body'].apply(lambda x: preprocess_text(x))\n",
    "test['answer'] = test['answer'].apply(lambda x: preprocess_text(x))\n",
    "#=================================================================================================\n",
    "# stemming\n",
    "#test['question_title'] = test['question_title'].apply(lambda x : ' '.join([porter.stem(t) for t in x.split(' ')]))\n",
    "#test['question_body'] = test['question_body'].apply(lambda x : ' '.join([porter.stem(t) for t in x.split(' ')]))\n",
    "#test['answer'] = test['answer'].apply(lambda x : ' '.join([porter.stem(t) for t in x.split(' ')]))\n",
    "\n",
    "#=================================================================================================\n",
    "# sentiments from question_title\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "neg=[]\n",
    "neu=[]\n",
    "pos=[]\n",
    "comp=[]\n",
    "for title in test.question_title:\n",
    "    ss = sid.polarity_scores(title)\n",
    "    neg.append(ss['neg'])\n",
    "    neu.append(ss['neu'])\n",
    "    pos.append(ss['pos'])\n",
    "    comp.append(ss['compound'])\n",
    "\n",
    "test['q_title_neg'] = neg\n",
    "test['q_title_neu'] = neu\n",
    "test['q_title_pos'] = pos\n",
    "test['q_title_comp'] = comp\n",
    "#=================================================================================================\n",
    "\n",
    "# sentiments from question_body\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "neg=[]\n",
    "neu=[]\n",
    "pos=[]\n",
    "comp=[]\n",
    "for body in test.question_body:\n",
    "    ss = sid.polarity_scores(body)\n",
    "    neg.append(ss['neg'])\n",
    "    neu.append(ss['neu'])\n",
    "    pos.append(ss['pos'])\n",
    "    comp.append(ss['compound'])\n",
    "        \n",
    "test['q_body_neg'] = neg\n",
    "test['q_body_neu'] = neu\n",
    "test['q_body_pos'] = pos\n",
    "test['q_body_comp'] = comp\n",
    "#===================================================================================================\n",
    "\n",
    "# sentiments from answer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "neg=[]\n",
    "neu=[]\n",
    "pos=[]\n",
    "comp=[]\n",
    "for body in test.answer:\n",
    "    ss = sid.polarity_scores(body)\n",
    "    neg.append(ss['neg'])\n",
    "    neu.append(ss['neu'])\n",
    "    pos.append(ss['pos'])\n",
    "    comp.append(ss['compound'])\n",
    "        \n",
    "test['q_answer_neg'] = neg\n",
    "test['q_answer_neu'] = neu\n",
    "test['q_answer_pos'] = pos\n",
    "test['q_answer_comp'] = comp\n",
    "\n",
    "#=========================================================================================\n",
    "\n",
    "# average Word2Vec on question title test data\n",
    "# computing average word2vec for each question title\n",
    "avg_w2v_vectors_title_test = []; # the avg-w2v for each question title is stored in this list\n",
    "for sentence in tqdm(test['question_title']): # for each question title\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    cnt_words = 0; # num of words with a valid vector in the question title\n",
    "    for word in sentence.split(): # for each word in a question title\n",
    "        if word in glove_words:\n",
    "            vector += glove_vector[word]\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        vector /= cnt_words\n",
    "    avg_w2v_vectors_title_test.append(vector)\n",
    "\n",
    "#=========================================================================================\n",
    "\n",
    "# average Word2Vec on question body test data\n",
    "# computing average word2vec for each question body\n",
    "avg_w2v_vectors_body_test = []; # the avg-w2v for each question body is stored in this list\n",
    "for sentence in tqdm(test['question_body']): # for each question body\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    cnt_words = 0; # num of words with a valid vector in the question body\n",
    "    for word in sentence.split(): # for each word in a question body\n",
    "        if word in glove_words:\n",
    "            vector += glove_vector[word]\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        vector /= cnt_words\n",
    "    avg_w2v_vectors_body_test.append(vector)\n",
    "\n",
    "#=========================================================================================\n",
    "\n",
    "# average Word2Vec on answer test data\n",
    "# computing average word2vec for each answer\n",
    "avg_w2v_vectors_answer_test = []; # the avg-w2v for each answer is stored in this list\n",
    "for sentence in tqdm(test['answer']): # for each answer\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    cnt_words = 0; # num of words with a valid vector in the answer\n",
    "    for word in sentence.split(): # for each word in a answer\n",
    "        if word in glove_words:\n",
    "            vector += glove_vector[word]\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        vector /= cnt_words\n",
    "    avg_w2v_vectors_answer_test.append(vector)\n",
    "\n",
    "#=========================================================================================\n",
    "\n",
    "test_vect = np.hstack([avg_w2v_vectors_title_test,avg_w2v_vectors_body_test,\n",
    "                        avg_w2v_vectors_answer_test,category_encoded_test,host_encoded_test,\n",
    "                        test[['q_title_length','q_body_length','answer_length', 'q_title_neg',\n",
    "                                 'q_title_neu', 'q_title_pos', 'q_title_comp', 'q_body_neg',\n",
    "                                 'q_body_neu', 'q_body_pos', 'q_body_comp', 'q_answer_neg',\n",
    "                                 'q_answer_neu', 'q_answer_pos', 'q_answer_comp']]])\n",
    "\n",
    "\n",
    "\n",
    "print(test_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "pV0hbu9Btceg",
    "outputId": "07b9ac00-5b9d-4b4c-dfbc-fa6b1539cb16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(476, 900) (476, 26)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_vect_text = np.hstack([avg_w2v_vectors_title_test,avg_w2v_vectors_body_test,\n",
    "                        avg_w2v_vectors_answer_test])\n",
    "\n",
    "test_vect_cat_num = np.hstack([category_encoded_test,host_encoded_test,\n",
    "                        test[['q_title_length','q_body_length','answer_length', 'q_title_neg',\n",
    "                                 'q_title_neu', 'q_title_pos', 'q_title_comp', 'q_body_neg',\n",
    "                                 'q_body_neu', 'q_body_pos', 'q_body_comp', 'q_answer_neg',\n",
    "                                 'q_answer_neu', 'q_answer_pos', 'q_answer_comp']]])\n",
    "\n",
    "print(test_vect_text.shape,test_vect_cat_num.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "dEUPmFzPtcex",
    "outputId": "d8d7ea86-7489-4762-da03-32333c1296f5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5471, 926, 1)\n",
      "(608, 926, 1)\n"
     ]
    }
   ],
   "source": [
    "# reshaping data for convolution layer\n",
    "from numpy import zeros, newaxis\n",
    "\n",
    "train_vect = train_vect[:,:,newaxis]\n",
    "cv_vect = cv_vect[:,:,newaxis]\n",
    "print(train_vect.shape)\n",
    "print(cv_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "hA3SWv1xtce1",
    "outputId": "20cebb87-812a-4d62-baf6-1ae49fdd9b8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(476, 926, 1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshaping data for convolution layer\n",
    "test_vect = test_vect[:,:,newaxis]\n",
    "test_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Bk8wVU6Dtce9"
   },
   "outputs": [],
   "source": [
    "# reshaping data for convolution layer\n",
    "from numpy import zeros, newaxis\n",
    "\n",
    "train_vect_text = train_vect_text[:,:,newaxis]\n",
    "train_vect_cat_num = train_vect_cat_num[:,:,newaxis]\n",
    "cv_vect_text = cv_vect_text[:,:,newaxis]\n",
    "cv_vect_cat_num = cv_vect_cat_num[:,:,newaxis]\n",
    "test_vect_text = test_vect_text[:,:,newaxis]\n",
    "test_vect_cat_num = test_vect_cat_num[:,:,newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "LYtO7AWOtcfX"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "def spearman(y_true,y_pred):\n",
    "    '''\n",
    "    function to calculate mean spearman correlation of all 30 class-labels\n",
    "    '''\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    spearman_y = []\n",
    "    for i in range(30):\n",
    "        score = spearmanr(y_true[:,i], y_pred[:,i] + np.random.normal(0, 1e-7, y_pred.shape[0]) , \n",
    "                                                                        nan_policy='omit').correlation\n",
    "                                                                        \n",
    "\n",
    "        spearman_y.append(score)\n",
    "    mean_score = np.nanmean(spearman_y)\n",
    "    return mean_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ENCODING FOR LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded question Train data:\n",
      "[21925, 569, 1908, 118, 21, 22, 1287, 3, 79, 45, 143, 7, 238, 2, 23, 411, 206, 18, 1, 1249, 569, 7, 4, 203, 41, 1, 87, 6234, 4, 569, 138, 21, 133, 58, 932, 7, 238, 530, 819, 6, 3974, 564, 496, 28, 570, 18, 1, 187, 514, 2, 16, 1908, 11, 21, 22, 1287, 3, 44, 367, 7, 238, 97, 140, 3184, 6, 1908, 11, 28, 2199, 3, 79, 45, 367, 7, 238, 97, 140, 530, 196, 2, 23, 460, 47, 846, 5, 1, 276, 3, 1117, 20, 333, 2996, 1908, 5766, 258, 846, 41, 1, 87, 9412, 4, 238, 143, 1185, 1908, 28, 608, 6, 62, 28, 79, 524, 3, 11, 143, 7, 238, 115, 2289, 50, 1908, 570, 39, 50, 71, 26, 1, 87, 9412, 4, 564, 11, 21, 22, 1287, 3, 79, 45, 143, 7, 238, 1, 238, 143, 48, 22, 772, 4141, 115, 2477, 160, 15, 188, 139]\n",
      "============================================================\n",
      "Encoded question CV data:\n",
      "[8971, 634, 539, 5487, 2, 16, 4, 878, 11709, 18309, 11444, 9748, 20, 8972, 682, 41, 2, 213, 9, 77, 362, 992, 1351, 30, 1770, 18310, 2, 21, 1106, 34, 1424, 42, 495, 30, 51, 70, 4721, 992, 33, 45, 511, 3, 162, 21, 13, 449, 384, 80, 539, 1, 5487, 30, 473, 61, 1207, 1, 5101, 2, 1049, 9, 3, 1, 11709, 383, 707, 1157, 32, 1053, 401, 33, 17, 337, 6, 62, 1045, 57, 11, 1, 4, 63, 2655, 55, 680, 62, 11049, 9, 6, 1052, 57, 18, 17, 65, 167, 2, 166, 1106, 1, 8971, 634, 1207, 1, 5487, 49, 234, 16, 44, 684, 18, 34, 12, 310, 22, 160, 1185, 474, 2, 16, 648, 750, 11, 41, 12, 239, 495, 634, 2484, 11, 9, 5, 82, 11370, 41, 1, 690, 5, 239, 1260, 97, 140, 599, 2601, 12, 2733, 57, 3, 1204, 473, 12, 64, 132, 3, 27, 20, 1, 495, 1945, 30, 51, 70, 4721, 1945, 11, 5, 200, 52, 1, 690, 6, 13, 992, 468, 4721, 3, 945, 1, 2788, 1613, 4, 8971, 239, 495, 634]\n",
      "============================================================\n",
      "Padded question Train data:\n",
      "[21925   569  1908 ...     0     0     0]\n",
      "============================================================\n",
      "Encoded title Train data:\n",
      "[4750, 276, 1363, 69, 13, 30, 1655, 1, 104, 48, 187, 7, 233]\n",
      "============================================================\n",
      "Encoded title CV data:\n",
      "[3850, 313, 419, 3851]\n",
      "============================================================\n",
      "Padded Title Train data:\n",
      "[4750  276 1363   69   13   30 1655    1  104   48  187    7  233    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0]\n",
      "============================================================\n",
      "Encoded body Train data:\n",
      "[2, 21, 385, 211, 19, 1, 1240, 604, 7, 4, 207, 42, 1, 90, 5838, 4, 604, 133, 25, 130, 59, 962, 7, 237, 504, 797, 6, 3884, 597, 495, 28, 571, 19, 1, 178, 505, 2, 16, 1973, 11, 25, 24, 1258, 3, 44, 362, 7, 237, 93, 134, 3084, 6, 1973, 11, 28, 2211, 3, 78, 45, 362, 7, 237, 93, 134, 504, 199, 2, 21, 426, 47, 806, 5, 1, 343, 3, 1226, 20, 315, 3194, 1973, 7012, 258, 806, 42, 1, 90, 8823, 4, 237, 141, 1164, 1973, 28, 602, 6, 62, 28, 78, 537, 3, 11, 141, 7, 237, 108, 2409, 49, 1973, 571, 39, 49, 72, 26, 1, 90, 8823, 4, 597, 11, 25, 24, 1258, 3, 78, 45, 141, 7, 237, 1, 237, 141, 46, 24, 755, 3885, 108, 2466, 155, 15, 188, 140]\n",
      "============================================================\n",
      "Encoded body CV data:\n",
      "[2, 16, 4, 868, 11141, 17662, 10875, 10469, 20, 8409, 633, 42, 2, 217, 9, 77, 345, 1159, 1468, 30, 1729, 17663, 2, 25, 1027, 36, 1415, 40, 582, 30, 48, 70, 4422, 1159, 32, 45, 510, 3, 168, 25, 13, 416, 375, 83, 548, 1, 5850, 30, 440, 61, 1229, 1, 4760, 2, 976, 9, 3, 1, 11141, 384, 693, 1086, 31, 1000, 410, 32, 17, 341, 6, 62, 973, 55, 11, 1, 4, 66, 2774, 53, 687, 62, 10482, 9, 6, 1052, 55, 19, 17, 67, 159, 2, 161, 1027, 1, 11142, 665, 1229, 1, 5850, 54, 219, 16, 44, 650, 19, 36, 12, 297, 24, 155, 1164, 445, 2, 16, 610, 708, 11, 42, 12, 254, 582, 665, 2362, 11, 9, 5, 82, 10797, 42, 1, 727, 5, 254, 1233, 93, 134, 611, 2870, 12, 2639, 55, 3, 1124, 440, 12, 63, 127, 3, 27, 20, 1, 582, 1964, 30, 48, 70, 4422, 1964, 11, 5, 196, 52, 1, 727, 6, 13, 1159, 457, 4422, 3, 995, 1, 2635, 1518, 4, 11142, 254, 582, 665]\n",
      "============================================================\n",
      "Padded body Train data:\n",
      "[  2  21 385 ...   0   0   0]\n",
      "============================================================\n",
      "Encoded Answer Train data:\n",
      "[12, 37, 147, 19, 1, 21160, 598, 7, 64, 475, 1332, 10, 15, 3430, 2, 1, 1312, 1224, 1332, 16, 12, 534, 1162, 1, 262, 598, 37, 976, 2, 3, 2114, 6, 1, 90, 284, 98, 126, 16, 3, 136, 6553, 1064, 90, 7, 43, 2764, 2, 589, 1, 866, 235, 506, 37, 281, 8, 1065, 1, 313, 144, 2, 866, 14, 4, 11, 64, 2612, 9, 4, 55, 111, 10174, 13, 1, 136, 18, 45, 23, 2, 189, 44, 186, 295]\n",
      "============================================================\n",
      "Encoded Answer CV data:\n",
      "[22, 1346, 3830, 85, 17, 3110, 74, 22, 1346, 28718, 504, 50, 1, 4297, 32427, 945, 3277, 186, 9, 14, 774, 8, 1, 479, 1658, 6, 1, 1346, 40, 4, 3, 3830, 10, 4, 1277, 2, 3830, 1, 504, 855, 1, 857, 50, 9, 458, 3110, 74, 504, 8, 1, 28718, 139, 391, 1, 4526, 9, 27, 1617, 3830, 1306, 114, 305, 21, 22, 444, 1, 857, 8, 1, 383, 70, 5, 27, 17, 249, 2, 1600, 9, 276, 161, 391, 5, 4626, 25, 30, 1, 9569, 6, 1, 857, 13, 3, 4183, 3197, 6459, 69, 6, 1, 2538, 5, 20, 9972, 9, 19, 3, 4864, 23272, 25, 969, 6, 1308, 1, 711, 457, 85, 17, 49, 5, 23, 16, 11, 9, 27, 34, 274, 252, 5, 32, 379, 6, 49, 5, 15, 342, 13, 711, 247]\n",
      "============================================================\n",
      "Padded answer Train data:\n",
      "[ 12  37 147 ...   0   0   0]\n",
      "============================================================\n",
      "Encoded category Train data:\n",
      "[1]\n",
      "============================================================\n",
      "Encoded category CV data:\n",
      "[5]\n",
      "============================================================\n",
      "Padded category data:\n",
      "[1]\n",
      "============================================================\n",
      "Encoded Host Train data:\n",
      "[1]\n",
      "============================================================\n",
      "Encoded Host CV data:\n",
      "[1]\n",
      "============================================================\n",
      "Padded host data:\n",
      "[1]\n",
      "============================================================\n",
      "Shape of train : (5471, 26)\n",
      "Shape of cv : (608, 26)\n",
      "Encoded answer test data:\n",
      "[   48  2462 22616 ...     0     0     0]\n",
      "============================================================\n",
      "Encoded title test data:\n",
      "[74, 907, 23]\n",
      "============================================================\n",
      "Encoded body test data:\n",
      "[2, 130, 263, 314, 761, 73, 38, 3, 43, 2934, 7, 21935, 23, 2, 21, 13, 269, 36, 1, 4895, 3, 291, 95, 4917, 311, 28, 2, 68, 11, 2, 25, 56, 4, 3, 443, 95, 6, 11, 62, 46, 2311, 24, 1220, 833, 8, 4, 159, 2, 16, 379, 61, 178, 95, 4917, 311, 12, 54, 13, 339, 3, 16, 44, 1155, 4895, 27, 62, 14753, 30, 7699, 93, 134, 2607, 1, 427, 2745, 36, 4286, 1741, 54, 103, 95, 5647, 1168, 258, 5, 12, 127, 11, 5, 13, 1878, 502, 30, 5, 2338, 1891, 3150, 49, 226, 1, 510, 2025, 3, 24, 819, 332, 1, 1762, 5, 492, 15, 4286, 6, 27, 13, 4658, 44, 679, 19, 12, 49, 2, 21, 1493, 5, 1, 38, 7, 325, 2934, 7, 21935, 537, 3, 9, 23, 13, 122, 9, 4184, 8, 1, 113, 510, 49, 2, 16, 527, 11, 5, 537, 5, 11, 1148, 25, 6604, 23233, 21935, 15, 1561, 23, 37, 5, 80, 1155, 4895, 15, 12174, 21935, 47, 28, 36, 2, 3611, 3, 118, 77, 20, 33, 122, 13, 61, 1572, 95, 8, 1]\n",
      "============================================================\n",
      "Encoded answer test data:\n",
      "[40  4 61 ...  0  0  0]\n",
      "============================================================\n",
      "(476, 26)\n",
      "(5471, 26, 1)\n",
      "(608, 26, 1)\n",
      "(476, 26, 1)\n"
     ]
    }
   ],
   "source": [
    "def max_len(text_array):\n",
    "    length_of_text = []\n",
    "    for text in text_array:\n",
    "        length_of_text.append(len(text.split()))\n",
    "    return(max(length_of_text))\n",
    "\n",
    "#================================================================\n",
    "\n",
    "#Text data encoding\n",
    "\n",
    "# combining question title and body for input to LSTM model-2\n",
    "x_train['question'] = [x_train['question_title'][i] + ' ' + x_train['question_body'][i] for i in x_train.index]\n",
    "x_cv['question'] = [x_cv['question_title'][i] + ' ' + x_cv['question_body'][i] for i in x_cv.index]\n",
    "\n",
    "# tokenizing question_title + question_body\n",
    "tokenizer_q = Tokenizer()\n",
    "tokenizer_q.fit_on_texts(x_train['question'])\n",
    "vocab_size_q=len(tokenizer_q.word_index) + 1\n",
    "\n",
    "# encoding to sequence\n",
    "encoded_q_train = tokenizer_q.texts_to_sequences(x_train['question'])\n",
    "encoded_q_cv = tokenizer_q.texts_to_sequences(x_cv['question'])\n",
    "print(\"Encoded question Train data:\")\n",
    "print(encoded_q_train[0])\n",
    "print(\"=\"*60)\n",
    "print(\"Encoded question CV data:\")\n",
    "print(encoded_q_cv[0])\n",
    "print(\"=\"*60)\n",
    "\n",
    "#===========================================================\n",
    "\n",
    "from numpy import zeros\n",
    "max_length_q = max_len(x_train['question'])\n",
    "\n",
    "# post padding with zeros\n",
    "padded_q_train = pad_sequences(encoded_q_train, maxlen = max_length_q, padding = 'post')\n",
    "padded_q_cv = pad_sequences(encoded_q_cv, maxlen = max_length_q, padding = 'post')\n",
    "print(\"Padded question Train data:\")\n",
    "print(padded_q_train[0])\n",
    "print(\"=\"*60)\n",
    "\n",
    "# creating a weight matrix for words in training docs\n",
    "embedding_matrix_q = zeros((vocab_size_q, 300))\n",
    "for word, i in tokenizer_q.word_index.items():\n",
    "    embedding_vector = glove_vector.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_q[i] = embedding_vector\n",
    "\n",
    "# tokenizing question_title\n",
    "tokenizer_title = Tokenizer()\n",
    "tokenizer_title.fit_on_texts(x_train['question_title'])\n",
    "vocab_size_title = len(tokenizer_title.word_index) + 1\n",
    "\n",
    "# encoding to sequence\n",
    "encoded_title_train = tokenizer_title.texts_to_sequences(x_train['question_title'])\n",
    "encoded_title_cv = tokenizer_title.texts_to_sequences(x_cv['question_title'])\n",
    "print(\"Encoded title Train data:\")\n",
    "print(encoded_title_train[0])\n",
    "print(\"=\"*60)\n",
    "print(\"Encoded title CV data:\")\n",
    "print(encoded_title_cv[0])\n",
    "print(\"=\"*60)\n",
    "\n",
    "# encoding and padding zeros to question_title\n",
    "\n",
    "max_length_title = max_len(x_train['question_title'])\n",
    "\n",
    "# post padding with zeros\n",
    "padded_title_train = pad_sequences(encoded_title_train, maxlen = max_length_title, padding = 'post')\n",
    "padded_title_cv = pad_sequences(encoded_title_cv, maxlen = max_length_title, padding = 'post')\n",
    "print(\"Padded Title Train data:\")\n",
    "print(padded_title_train[0])\n",
    "print(\"=\"*60)\n",
    "\n",
    "# creating a weight matrix for words in training docs\n",
    "embedding_matrix_title = zeros((vocab_size_title, 300))\n",
    "for word, i in tokenizer_title.word_index.items():\n",
    "    embedding_vector = glove_vector.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_title[i] = embedding_vector\n",
    "\n",
    "# tokeizing question_body\n",
    "tokenizer_body = Tokenizer()\n",
    "tokenizer_body.fit_on_texts(x_train['question_body'])\n",
    "vocab_size_body = len(tokenizer_body.word_index) + 1\n",
    "\n",
    "# encoding to sequence\n",
    "encoded_body_train = tokenizer_body.texts_to_sequences(x_train['question_body'])\n",
    "encoded_body_cv = tokenizer_body.texts_to_sequences(x_cv['question_body'])\n",
    "print(\"Encoded body Train data:\")\n",
    "print(encoded_body_train[0])\n",
    "print(\"=\"*60)\n",
    "print(\"Encoded body CV data:\")\n",
    "print(encoded_body_cv[0])\n",
    "print(\"=\"*60)\n",
    "\n",
    "# encoding and padding zeros to question_body\n",
    "max_length_body = max_len(x_train['question_body'])\n",
    "\n",
    "# post padding with zeros\n",
    "padded_body_train = pad_sequences(encoded_body_train, maxlen = max_length_body, padding = 'post')\n",
    "padded_body_cv = pad_sequences(encoded_body_cv, maxlen = max_length_body, padding = 'post')\n",
    "print(\"Padded body Train data:\")\n",
    "print(padded_body_train[0])\n",
    "print(\"=\"*60)\n",
    "\n",
    "# creating a weight matrix for words in training docs\n",
    "embedding_matrix_body = zeros((vocab_size_body, 300))\n",
    "for word, i in tokenizer_body.word_index.items():\n",
    "    embedding_vector = glove_vector.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_body[i] = embedding_vector\n",
    "\n",
    "# tokenzing answers\n",
    "tokenizer_answer = Tokenizer()\n",
    "tokenizer_answer.fit_on_texts(x_train['answer'])\n",
    "vocab_size_answer = len(tokenizer_answer.word_index) + 1\n",
    "\n",
    "# encoding to sequence\n",
    "encoded_answer_train = tokenizer_answer.texts_to_sequences(x_train['answer'])\n",
    "encoded_answer_cv = tokenizer_answer.texts_to_sequences(x_cv['answer'])\n",
    "print(\"Encoded Answer Train data:\")\n",
    "print(encoded_answer_train[0])\n",
    "print(\"=\"*60)\n",
    "print(\"Encoded Answer CV data:\")\n",
    "print(encoded_answer_cv[0])\n",
    "print(\"=\"*60)\n",
    "\n",
    "# padding zeros to answers\n",
    "max_length_answer = max_len(x_train['answer'])\n",
    "\n",
    "# post padding with zeros\n",
    "padded_answer_train = pad_sequences(encoded_answer_train, maxlen = max_length_answer, padding = 'post')\n",
    "padded_answer_cv = pad_sequences(encoded_answer_cv, maxlen = max_length_answer, padding = 'post')\n",
    "print(\"Padded answer Train data:\")\n",
    "print(padded_answer_train[0])\n",
    "print(\"=\"*60)\n",
    "\n",
    "# creating a weight matrix for words in training docs\n",
    "embedding_matrix_answer = zeros((vocab_size_answer, 300))\n",
    "for word, i in tokenizer_body.word_index.items():\n",
    "    embedding_vector = glove_vector.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_answer[i] = embedding_vector\n",
    "\n",
    "#### Categorical data encoding\n",
    "\n",
    "# tokenizing category\n",
    "t1 = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n')\n",
    "t1.fit_on_texts(x_train['category'])\n",
    "vocab_size_category = len(t1.word_index) + 1\n",
    "\n",
    "# encoding to sequence\n",
    "encoded_category_train = t1.texts_to_sequences(x_train['category'])\n",
    "encoded_category_cv = t1.texts_to_sequences(x_cv['category'])\n",
    "print(\"Encoded category Train data:\")\n",
    "print(encoded_category_train[0])\n",
    "print(\"=\"*60)\n",
    "print(\"Encoded category CV data:\")\n",
    "print(encoded_category_cv[0])\n",
    "print(\"=\"*60)\n",
    "\n",
    "max_length_category = max_len(x_train['category'])\n",
    "\n",
    "# post padding with zeros\n",
    "encoded_category_train = pad_sequences(encoded_category_train, maxlen = max_length_category, padding = 'post')\n",
    "encoded_category_cv = pad_sequences(encoded_category_cv, maxlen = max_length_category, padding = 'post')\n",
    "print(\"Padded category data:\")\n",
    "print(encoded_category_train[0])\n",
    "print(\"=\"*60)\n",
    "\n",
    "# tokenizing host\n",
    "t2 = Tokenizer()\n",
    "t2.fit_on_texts(x_train['host'])\n",
    "vocab_size_host = len(t2.word_index) + 1\n",
    "\n",
    "# encoding to sequence\n",
    "encoded_host_train = t2.texts_to_sequences(x_train['host'])\n",
    "encoded_host_cv = t2.texts_to_sequences(x_cv['host'])\n",
    "print(\"Encoded Host Train data:\")\n",
    "print(encoded_host_train[0])\n",
    "print(\"=\"*60)\n",
    "print(\"Encoded Host CV data:\")\n",
    "print(encoded_host_cv[0])\n",
    "print(\"=\"*60)\n",
    "\n",
    "max_length_host = max_len(x_train['host'])\n",
    "\n",
    "# post padding with zeros\n",
    "encoded_host_train = pad_sequences(encoded_host_train, maxlen = max_length_host, padding = 'post')\n",
    "encoded_host_cv = pad_sequences(encoded_host_cv, maxlen = max_length_host, padding = 'post')\n",
    "print(\"Padded host data:\")\n",
    "print(encoded_host_train[0])\n",
    "print(\"=\"*60)\n",
    "\n",
    "x_train.columns\n",
    "\n",
    "# concatenating train and cv vectors\n",
    "x_train_cat_num = np.hstack([category_encoded_train,host_encoded_train,\n",
    "                             x_train[['q_title_length', 'q_body_length', 'answer_length', 'q_title_neg',\n",
    "                                      'q_title_neu', 'q_title_pos', 'q_title_comp', 'q_body_neg',\n",
    "                                      'q_body_neu', 'q_body_pos', 'q_body_comp', 'q_answer_neg',\n",
    "                                      'q_answer_neu', 'q_answer_pos', 'q_answer_comp']]])\n",
    "\n",
    "x_cv_cat_num = np.hstack([category_encoded_cv,host_encoded_cv,\n",
    "                             x_cv[['q_title_length', 'q_body_length', 'answer_length', 'q_title_neg',\n",
    "                                      'q_title_neu', 'q_title_pos', 'q_title_comp', 'q_body_neg',\n",
    "                                      'q_body_neu', 'q_body_pos', 'q_body_comp', 'q_answer_neg',\n",
    "                                      'q_answer_neu', 'q_answer_pos', 'q_answer_comp']]])\n",
    "\n",
    "print(\"Shape of train :\",x_train_cat_num.shape)\n",
    "print(\"Shape of cv :\",x_cv_cat_num.shape)\n",
    "\n",
    "# preparing test vectors\n",
    "\n",
    "# reading test data from csv file\n",
    "test = pd.read_csv(\"../input/google-quest-challenge/test.csv\")\n",
    "\n",
    "#preprocessing host\n",
    "test['host'] = test['host'].apply(lambda x: x.split('.')[-2])\n",
    "test['host'] = test['host'].apply(lambda x: x.lower())\n",
    "test['host'] = test['host'].apply(lambda x: x.strip())\n",
    "#=================================================================================================\n",
    "# preprocessing 'category'  to lower-case and stripping leading and tailing spaces \n",
    "test['category'] = test['category'].apply(lambda x: x.lower())\n",
    "test['category'] = test['category'].apply(lambda x: x.strip())\n",
    "#=================================================================================================\n",
    "\n",
    "#=================================================================================================\n",
    "# taking length of question title, question body, answer\n",
    "test['q_title_length'] = test['question_title'].apply(lambda x: len(x.split(' ')))\n",
    "test['q_body_length'] = test['question_body'].apply(lambda x: len(x.split(' ')))\n",
    "test['answer_length'] = test['answer'].apply(lambda x: len(x.split(' ')))\n",
    "#=================================================================================================\n",
    "# preprocessing question title, question body and answers\n",
    "test['question_title'] = test['question_title'].apply(lambda x: preprocess_text(x))\n",
    "test['question_body'] = test['question_body'].apply(lambda x: preprocess_text(x))\n",
    "test['answer'] = test['answer'].apply(lambda x: preprocess_text(x))\n",
    "\n",
    "#=================================================================================================\n",
    "\n",
    "test['question'] = [test['question_title'][i] + ' ' + test['question_body'][i] for i in test.index]\n",
    "\n",
    "#=================================================================================================\n",
    "# sentiments from question_title\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "neg=[]\n",
    "neu=[]\n",
    "pos=[]\n",
    "comp=[]\n",
    "for title in test.question_title:\n",
    "    ss = sid.polarity_scores(title)\n",
    "    neg.append(ss['neg'])\n",
    "    neu.append(ss['neu'])\n",
    "    pos.append(ss['pos'])\n",
    "    comp.append(ss['compound'])\n",
    "\n",
    "test['q_title_neg'] = neg\n",
    "test['q_title_neu'] = neu\n",
    "test['q_title_pos'] = pos\n",
    "test['q_title_comp'] = comp\n",
    "#=================================================================================================\n",
    "\n",
    "# sentiments from question_body\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "neg=[]\n",
    "neu=[]\n",
    "pos=[]\n",
    "comp=[]\n",
    "for body in test.question_body:\n",
    "    ss = sid.polarity_scores(body)\n",
    "    neg.append(ss['neg'])\n",
    "    neu.append(ss['neu'])\n",
    "    pos.append(ss['pos'])\n",
    "    comp.append(ss['compound'])\n",
    "        \n",
    "test['q_body_neg'] = neg\n",
    "test['q_body_neu'] = neu\n",
    "test['q_body_pos'] = pos\n",
    "test['q_body_comp'] = comp\n",
    "#===================================================================================================\n",
    "\n",
    "# sentiments from answer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "neg=[]\n",
    "neu=[]\n",
    "pos=[]\n",
    "comp=[]\n",
    "for body in test.answer:\n",
    "    ss = sid.polarity_scores(body)\n",
    "    neg.append(ss['neg'])\n",
    "    neu.append(ss['neu'])\n",
    "    pos.append(ss['pos'])\n",
    "    comp.append(ss['compound'])\n",
    "        \n",
    "test['q_answer_neg'] = neg\n",
    "test['q_answer_neu'] = neu\n",
    "test['q_answer_pos'] = pos\n",
    "test['q_answer_comp'] = comp\n",
    "\n",
    "#=========================================================================================\n",
    "\n",
    "# encoding question to sequence\n",
    "encoded_q_test = tokenizer_q.texts_to_sequences(test['question'])\n",
    "padded_q_test = pad_sequences(encoded_q_test, maxlen = max_length_q, padding = 'post')\n",
    "print(\"Encoded answer test data:\")\n",
    "print(padded_q_test[0])\n",
    "print(\"=\"*60)\n",
    "\n",
    "#======================================================\n",
    "\n",
    "# encoding title to sequence\n",
    "encoded_title_test = tokenizer_title.texts_to_sequences(test['question_title'])\n",
    "\n",
    "print(\"Encoded title test data:\")\n",
    "print(encoded_title_test[0])\n",
    "print(\"=\"*60)\n",
    "\n",
    "#====================================================\n",
    "\n",
    "# encoding title to sequence\n",
    "encoded_body_test = tokenizer_body.texts_to_sequences(test['question_body'])\n",
    "\n",
    "print(\"Encoded body test data:\")\n",
    "print(encoded_body_test[0])\n",
    "print(\"=\"*60)\n",
    "\n",
    "#====================================================\n",
    "\n",
    "# encoding answer to sequence\n",
    "encoded_answer_test = tokenizer_answer.texts_to_sequences(test['answer'])\n",
    "padded_answer_test = pad_sequences(encoded_answer_test, maxlen = max_length_answer, padding = 'post')\n",
    "print(\"Encoded answer test data:\")\n",
    "print(padded_answer_test[0])\n",
    "print(\"=\"*60)\n",
    "\n",
    "#====================================================\n",
    "\n",
    "encoded_category_test = t1.texts_to_sequences(test['category'])\n",
    "encoded_category_test = pad_sequences(encoded_category_test, maxlen = max_length_category, padding = 'post')\n",
    "\n",
    "#====================================================\n",
    "\n",
    "encoded_host_test = t2.texts_to_sequences(test['host'])\n",
    "encoded_host_test = pad_sequences(encoded_host_test, maxlen = max_length_host, padding = 'post')\n",
    "\n",
    "#====================================================\n",
    "\n",
    "x_test_cat_num = np.hstack([category_encoded_test,host_encoded_test,\n",
    "                             test[['q_title_length', 'q_body_length', 'answer_length', 'q_title_neg',\n",
    "                                      'q_title_neu', 'q_title_pos', 'q_title_comp', 'q_body_neg',\n",
    "                                      'q_body_neu', 'q_body_pos', 'q_body_comp', 'q_answer_neg',\n",
    "                                      'q_answer_neu', 'q_answer_pos', 'q_answer_comp']]])\n",
    "\n",
    "print(x_test_cat_num.shape)\n",
    "\n",
    "\n",
    "# reshaping data for convolution layer\n",
    "from numpy import zeros, newaxis\n",
    "\n",
    "x_train_cat_num = x_train_cat_num[:,:,newaxis]\n",
    "x_cv_cat_num = x_cv_cat_num[:,:,newaxis]\n",
    "x_test_cat_num = x_test_cat_num[:,:,newaxis]\n",
    "print(x_train_cat_num.shape)\n",
    "print(x_cv_cat_num.shape)\n",
    "print(x_test_cat_num.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning single variable for all inputs to model\n",
    "train_2 = [padded_title_train,padded_body_train,padded_answer_train,x_train_cat_num]\n",
    "cv_2 = [padded_title_cv,padded_body_cv,padded_answer_cv,x_cv_cat_num]\n",
    "test_2 = [encoded_title_test,encoded_body_test,encoded_answer_test,x_test_cat_num]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Remaining_input (InputLayer)    [(None, 26, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_input (InputLayer)        [(None, 31)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "body_input (InputLayer)         [(None, 2041)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "answer_input (InputLayer)       [(None, 2464)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 12, 32)       128         Remaining_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 31, 300)      2103000     title_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 2041, 300)    8341200     body_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 2464, 300)    10980300    answer_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 6, 32)        0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 31, 20)       25680       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 2041, 50)     70200       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 2464, 50)     70200       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 4, 32)        3104        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 620)          0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 102050)       0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 123200)       0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 128)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 225998)       0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 30)           6779970     concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 28,373,782\n",
      "Trainable params: 6,949,282\n",
      "Non-trainable params: 21,424,500\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# LSTM - 1\n",
    "\n",
    "# clearing the graph of tensorflow\n",
    "tensorflow.keras.backend.clear_session()\n",
    "\n",
    "# clearing the graph of tensorflow\n",
    "tensorflow.keras.backend.clear_session()\n",
    "\n",
    "##fixing numpy RS\n",
    "np.random.seed(42)\n",
    "\n",
    "##fixing tensorflow RS\n",
    "tf.random.set_seed(32)\n",
    "\n",
    "##python RS\n",
    "rn.seed(12)\n",
    "\n",
    "\n",
    "#input 1\n",
    "input1 = Input(shape=(max_length_title,), name = 'title_input')\n",
    "embed_layer1 = Embedding(vocab_size_title, 300, weights=[embedding_matrix_title], input_length=max_length_title,trainable=False) # embedding layer\n",
    "embedded_title = embed_layer1(input1)\n",
    "# passing embedded layer output to LSTM\n",
    "lstm_title = LSTM(20,kernel_regularizer = tensorflow.keras.regularizers.l1_l2(0.05,0.01),\n",
    "                  kernel_initializer=tf.keras.initializers.glorot_uniform(seed=26),\n",
    "                 recurrent_initializer=tf.keras.initializers.orthogonal(seed=54),\n",
    "                 bias_initializer=tf.keras.initializers.zeros(),\n",
    "                  return_sequences=True)(embedded_title)\n",
    "flat_title = Flatten()(lstm_title)\n",
    "\n",
    "#====================================================================================================================\n",
    "\n",
    "#input 2\n",
    "input2 = Input(shape=(max_length_body,), name = 'body_input')\n",
    "embed_layer2 = Embedding(vocab_size_body, 300,weights=[embedding_matrix_body], input_length=max_length_body,trainable=False) # embedding layer\n",
    "embedded_body = embed_layer2(input2)\n",
    "# passing embedded layer output to LSTM\n",
    "lstm_body = LSTM(50,kernel_initializer=tf.keras.initializers.glorot_uniform(seed=26),\n",
    "                 recurrent_initializer=tf.keras.initializers.orthogonal(seed=54),\n",
    "                 bias_initializer=tf.keras.initializers.zeros(),\n",
    "                 kernel_regularizer = tensorflow.keras.regularizers.l1_l2(0.05,0.01),\n",
    "                 return_sequences=True)(embedded_body)\n",
    "flat_body = Flatten()(lstm_body)\n",
    "\n",
    "#====================================================================================================================\n",
    "#input 3\n",
    "input3 = Input(shape=(max_length_answer,), name = 'answer_input')\n",
    "embed_layer3 = Embedding(vocab_size_answer, 300, weights=[embedding_matrix_answer],  input_length=max_length_answer,trainable=False) # embedding layer\n",
    "embedded_answer = embed_layer3(input3)\n",
    "# passing embedded layer output to LSTM\n",
    "lstm_answer = LSTM(50,kernel_initializer=tf.keras.initializers.glorot_uniform(seed=26),\n",
    "                   recurrent_initializer=tf.keras.initializers.orthogonal(seed=54),\n",
    "                   bias_initializer=tf.keras.initializers.zeros(),\n",
    "                   kernel_regularizer = tensorflow.keras.regularizers.l1_l2(0.05,0.01),\n",
    "                   return_sequences=True)(embedded_answer)\n",
    "flat_answer = Flatten()(lstm_answer)\n",
    "\n",
    "#====================================================================================================================\n",
    "\n",
    "#input 4\n",
    "input4 = Input(shape=(26,1), name = 'Remaining_input')\n",
    "conv = Conv1D(filters = 32, kernel_size = 3,strides = 2, activation='relu',\n",
    "              kernel_initializer=tf.keras.initializers.he_normal(seed=43),\n",
    "              kernel_regularizer = tensorflow.keras.regularizers.l2(0.1),input_shape=(None,26,1))(input4)\n",
    "conv= MaxPooling1D()(conv)\n",
    "conv = Conv1D(32,3, kernel_initializer='he_normal', activation='relu')(conv)\n",
    "flat_remaining = Flatten()(conv)\n",
    "\n",
    "#====================================================================================================================\n",
    "\n",
    "final_data = concatenate([flat_title,flat_body,flat_answer,flat_remaining])\n",
    "\n",
    "#====================================================================================================================\n",
    "\n",
    "output = Dense(30,activation='sigmoid',kernel_initializer=tensorflow.keras.initializers.glorot_uniform(seed=45))(final_data)\n",
    "\n",
    "# create model with seven inputs\n",
    "model_3 = Model([input1,input2,input3,input4], output)\n",
    "\n",
    "model_3.compile(loss=tensorflow.keras.losses.binary_crossentropy,\n",
    "              optimizer=tensorflow.keras.optimizers.Adam(),\n",
    "              metrics=['mae'])\n",
    "\n",
    "print(model_3.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "39/39 [==============================] - 15s 384ms/step - loss: 296.5928 - mae: 0.1978 - val_loss: 183.3537 - val_mae: 0.1747\n",
      "Epoch 2/50\n",
      "39/39 [==============================] - 14s 351ms/step - loss: 110.3018 - mae: 0.1658 - val_loss: 48.5853 - val_mae: 0.1629\n",
      "Epoch 3/50\n",
      "39/39 [==============================] - 14s 349ms/step - loss: 22.4227 - mae: 0.1674 - val_loss: 9.1678 - val_mae: 0.1739\n",
      "Epoch 4/50\n",
      "39/39 [==============================] - 14s 347ms/step - loss: 7.4104 - mae: 0.1679 - val_loss: 6.6026 - val_mae: 0.1721\n",
      "Epoch 5/50\n",
      "39/39 [==============================] - 14s 353ms/step - loss: 6.3773 - mae: 0.1704 - val_loss: 6.1520 - val_mae: 0.1744\n",
      "Epoch 6/50\n",
      "39/39 [==============================] - 14s 359ms/step - loss: 5.9822 - mae: 0.1696 - val_loss: 5.8127 - val_mae: 0.1648\n",
      "Epoch 7/50\n",
      "39/39 [==============================] - 14s 352ms/step - loss: 5.6595 - mae: 0.1659 - val_loss: 5.4959 - val_mae: 0.1624\n",
      "Epoch 8/50\n",
      "39/39 [==============================] - 14s 350ms/step - loss: 5.3627 - mae: 0.1653 - val_loss: 5.2066 - val_mae: 0.1657\n",
      "Epoch 9/50\n",
      "39/39 [==============================] - 14s 350ms/step - loss: 5.0856 - mae: 0.1635 - val_loss: 4.9469 - val_mae: 0.1634\n",
      "Epoch 10/50\n",
      "39/39 [==============================] - 14s 357ms/step - loss: 4.8263 - mae: 0.1643 - val_loss: 4.6987 - val_mae: 0.1655\n",
      "Epoch 11/50\n",
      "39/39 [==============================] - 14s 352ms/step - loss: 4.5843 - mae: 0.1634 - val_loss: 4.4647 - val_mae: 0.1675\n",
      "Epoch 12/50\n",
      "39/39 [==============================] - 14s 351ms/step - loss: 4.3566 - mae: 0.1630 - val_loss: 4.2454 - val_mae: 0.1695\n",
      "Epoch 13/50\n",
      "39/39 [==============================] - 14s 350ms/step - loss: 4.1471 - mae: 0.1623 - val_loss: 4.0416 - val_mae: 0.1638\n",
      "Epoch 14/50\n",
      "39/39 [==============================] - 14s 358ms/step - loss: 3.9540 - mae: 0.1627 - val_loss: 3.8606 - val_mae: 0.1651\n",
      "Epoch 15/50\n",
      "39/39 [==============================] - 14s 351ms/step - loss: 3.7659 - mae: 0.1615 - val_loss: 3.6715 - val_mae: 0.1589\n",
      "Epoch 16/50\n",
      "39/39 [==============================] - 14s 350ms/step - loss: 3.5979 - mae: 0.1613 - val_loss: 3.5114 - val_mae: 0.1631\n",
      "Epoch 17/50\n",
      "39/39 [==============================] - 14s 351ms/step - loss: 3.4372 - mae: 0.1609 - val_loss: 3.3533 - val_mae: 0.1643\n",
      "Epoch 18/50\n",
      "39/39 [==============================] - 14s 356ms/step - loss: 3.2869 - mae: 0.1598 - val_loss: 3.2173 - val_mae: 0.1648\n",
      "Epoch 19/50\n",
      "39/39 [==============================] - 14s 350ms/step - loss: 3.1507 - mae: 0.1610 - val_loss: 3.0723 - val_mae: 0.1622\n",
      "Epoch 20/50\n",
      "39/39 [==============================] - 14s 349ms/step - loss: 3.0182 - mae: 0.1600 - val_loss: 2.9551 - val_mae: 0.1605\n",
      "Epoch 21/50\n",
      "39/39 [==============================] - 14s 351ms/step - loss: 2.9012 - mae: 0.1613 - val_loss: 2.8312 - val_mae: 0.1612\n",
      "Epoch 22/50\n",
      "39/39 [==============================] - 14s 357ms/step - loss: 2.7794 - mae: 0.1597 - val_loss: 2.7296 - val_mae: 0.1580\n",
      "Epoch 23/50\n",
      "39/39 [==============================] - 14s 352ms/step - loss: 2.6765 - mae: 0.1591 - val_loss: 2.6277 - val_mae: 0.1641\n",
      "Epoch 24/50\n",
      "39/39 [==============================] - 14s 355ms/step - loss: 2.5766 - mae: 0.1588 - val_loss: 2.5256 - val_mae: 0.1584\n",
      "Epoch 25/50\n",
      "39/39 [==============================] - 14s 351ms/step - loss: 2.4847 - mae: 0.1589 - val_loss: 2.4404 - val_mae: 0.1630\n",
      "Epoch 26/50\n",
      "39/39 [==============================] - 14s 352ms/step - loss: 2.3959 - mae: 0.1592 - val_loss: 2.3534 - val_mae: 0.1593\n",
      "Epoch 27/50\n",
      "39/39 [==============================] - 14s 361ms/step - loss: 2.3160 - mae: 0.1585 - val_loss: 2.2761 - val_mae: 0.1637\n",
      "Epoch 28/50\n",
      "39/39 [==============================] - 14s 347ms/step - loss: 2.2419 - mae: 0.1588 - val_loss: 2.2065 - val_mae: 0.1617\n",
      "Epoch 29/50\n",
      "39/39 [==============================] - 14s 351ms/step - loss: 2.1679 - mae: 0.1590 - val_loss: 2.1300 - val_mae: 0.1641\n",
      "Epoch 30/50\n",
      "39/39 [==============================] - 14s 348ms/step - loss: 2.1002 - mae: 0.1580 - val_loss: 2.0793 - val_mae: 0.1573\n",
      "Epoch 31/50\n",
      "39/39 [==============================] - 14s 361ms/step - loss: 2.0438 - mae: 0.1592 - val_loss: 2.0125 - val_mae: 0.1589\n",
      "Epoch 32/50\n",
      "39/39 [==============================] - 14s 349ms/step - loss: 1.9801 - mae: 0.1585 - val_loss: 1.9508 - val_mae: 0.1580\n",
      "Epoch 33/50\n",
      "39/39 [==============================] - 14s 351ms/step - loss: 1.9280 - mae: 0.1575 - val_loss: 1.9099 - val_mae: 0.1605\n",
      "Epoch 34/50\n",
      "39/39 [==============================] - 14s 349ms/step - loss: 1.8775 - mae: 0.1582 - val_loss: 1.8519 - val_mae: 0.1571\n",
      "Epoch 35/50\n",
      "39/39 [==============================] - 14s 357ms/step - loss: 1.8290 - mae: 0.1579 - val_loss: 1.8006 - val_mae: 0.1589\n",
      "Epoch 36/50\n",
      "39/39 [==============================] - 14s 350ms/step - loss: 1.7881 - mae: 0.1573 - val_loss: 1.7700 - val_mae: 0.1644\n",
      "Epoch 37/50\n",
      "39/39 [==============================] - 14s 354ms/step - loss: 1.7436 - mae: 0.1574 - val_loss: 1.7270 - val_mae: 0.1569\n",
      "Epoch 38/50\n",
      "39/39 [==============================] - 14s 352ms/step - loss: 1.7128 - mae: 0.1582 - val_loss: 1.6890 - val_mae: 0.1572\n",
      "Epoch 39/50\n",
      "39/39 [==============================] - 14s 356ms/step - loss: 1.6700 - mae: 0.1570 - val_loss: 1.6521 - val_mae: 0.1566\n",
      "Epoch 40/50\n",
      "39/39 [==============================] - 14s 356ms/step - loss: 1.6362 - mae: 0.1562 - val_loss: 1.6172 - val_mae: 0.1581\n",
      "Epoch 41/50\n",
      "39/39 [==============================] - 14s 349ms/step - loss: 1.6086 - mae: 0.1567 - val_loss: 1.5932 - val_mae: 0.1594\n",
      "Epoch 42/50\n",
      "39/39 [==============================] - 14s 354ms/step - loss: 1.5796 - mae: 0.1571 - val_loss: 1.5688 - val_mae: 0.1585\n",
      "Epoch 43/50\n",
      "39/39 [==============================] - 14s 350ms/step - loss: 1.5505 - mae: 0.1567 - val_loss: 1.5371 - val_mae: 0.1598\n",
      "Epoch 44/50\n",
      "39/39 [==============================] - 14s 360ms/step - loss: 1.5272 - mae: 0.1569 - val_loss: 1.5162 - val_mae: 0.1575\n",
      "Epoch 45/50\n",
      "39/39 [==============================] - 14s 353ms/step - loss: 1.5046 - mae: 0.1564 - val_loss: 1.4955 - val_mae: 0.1643\n",
      "Epoch 46/50\n",
      "39/39 [==============================] - 14s 349ms/step - loss: 1.4814 - mae: 0.1563 - val_loss: 1.4751 - val_mae: 0.1564\n",
      "Epoch 47/50\n",
      "39/39 [==============================] - 14s 352ms/step - loss: 1.4632 - mae: 0.1562 - val_loss: 1.4493 - val_mae: 0.1589\n",
      "Epoch 48/50\n",
      "39/39 [==============================] - 14s 362ms/step - loss: 1.4443 - mae: 0.1560 - val_loss: 1.4334 - val_mae: 0.1616\n",
      "Epoch 49/50\n",
      "39/39 [==============================] - 14s 354ms/step - loss: 1.4272 - mae: 0.1562 - val_loss: 1.4146 - val_mae: 0.1552\n",
      "Epoch 50/50\n",
      "39/39 [==============================] - 13s 341ms/step - loss: 1.4104 - mae: 0.1554 - val_loss: 1.4238 - val_mae: 0.1587\n"
     ]
    }
   ],
   "source": [
    "# checkpoint path and function to save best trained weights\n",
    "filepath=\"weights_lstm2.best_copy.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "# fitting model on training data\n",
    "history_model_3 = model_3.fit( train_2, y_train, \n",
    "                               epochs=50,\n",
    "                               verbose=1,\n",
    "                               batch_size=128,\n",
    "                               callbacks=[checkpoint],\n",
    "                               validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation on train data =  0.2742513533849081\n",
      "Spearman correlation on cross-validation data =  0.2490241595235072\n"
     ]
    }
   ],
   "source": [
    "#loading best saved weights \n",
    "model_3.load_weights(\"weights_lstm2.best_copy.hdf5\")\n",
    "\n",
    "# predicting on train data\n",
    "y_tr_pred = model_3.predict(train_2)\n",
    "print(\"Spearman correlation on train data = \",spearman(y_train,y_tr_pred))\n",
    "\n",
    "# predicting on cv data\n",
    "y_cv_pred = model_3.predict(cv_2)\n",
    "print(\"Spearman correlation on cross-validation data = \",spearman(y_cv,y_cv_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM WITH LAYER NORMALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title_input (InputLayer)        [(None, 31)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "body_input (InputLayer)         [(None, 2041)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "answer_input (InputLayer)       [(None, 2464)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Remaining_input (InputLayer)    [(None, 26, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 31, 300)      2103000     title_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 2041, 300)    8341200     body_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 2464, 300)    10980300    answer_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 24, 32)       128         Remaining_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 31, 10)       12440       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 2041, 10)     12440       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 2464, 10)     12440       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 12, 32)       0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 31, 10)       20          lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 2041, 10)     20          lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 2464, 10)     20          lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 10, 32)       3104        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 310)          0           layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 20410)        0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 24640)        0           layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 320)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 45680)        0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 30)           1370430     concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 22,835,542\n",
      "Trainable params: 1,411,042\n",
      "Non-trainable params: 21,424,500\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# LSTM - 2\n",
    "\n",
    "# clearing the graph of tensorflow\n",
    "tensorflow.keras.backend.clear_session()\n",
    "\n",
    "##fixing numpy RS\n",
    "np.random.seed(42)\n",
    "\n",
    "##fixing tensorflow RS\n",
    "tf.random.set_seed(32)\n",
    "\n",
    "##python RS\n",
    "rn.seed(12)\n",
    "\n",
    "#input 1\n",
    "input1 = Input(shape=(max_length_title,), name = 'title_input')\n",
    "embed_layer1 = Embedding(vocab_size_title, 300, weights=[embedding_matrix_title], input_length=max_length_title,trainable=False) # embedding layer\n",
    "embedded_title = embed_layer1(input1)\n",
    "# passing embedded layer output to LSTM\n",
    "lstm_title = LSTM(10, kernel_initializer=tf.keras.initializers.glorot_uniform(seed=26),\n",
    "                 recurrent_initializer=tf.keras.initializers.orthogonal(seed=54),\n",
    "                 bias_initializer=tf.keras.initializers.zeros(),\n",
    "                  kernel_regularizer = tensorflow.keras.regularizers.l2(0.1),return_sequences=True)(embedded_title)\n",
    "norm_1 = tf.keras.layers.LayerNormalization()(lstm_title)\n",
    "flat_title = Flatten()(norm_1)\n",
    "\n",
    "#====================================================================================================================\n",
    "\n",
    "#input 2\n",
    "input2 = Input(shape=(max_length_body,), name = 'body_input')\n",
    "embed_layer2 = Embedding(vocab_size_body, 300,weights=[embedding_matrix_body], input_length=max_length_body,trainable=False) # embedding layer\n",
    "embedded_body = embed_layer2(input2)\n",
    "# passing embedded layer output to LSTM\n",
    "lstm_body = LSTM(10, kernel_initializer=tf.keras.initializers.glorot_uniform(seed=26),\n",
    "                 recurrent_initializer=tf.keras.initializers.orthogonal(seed=54),\n",
    "                 bias_initializer=tf.keras.initializers.zeros(),\n",
    "                 kernel_regularizer = tensorflow.keras.regularizers.l2(0.1),return_sequences=True)(embedded_body)\n",
    "norm_2 = tf.keras.layers.LayerNormalization()(lstm_body)\n",
    "flat_body = Flatten()(norm_2)\n",
    "\n",
    "#====================================================================================================================\n",
    "#input 3\n",
    "input3 = Input(shape=(max_length_answer,), name = 'answer_input')\n",
    "embed_layer3 = Embedding(vocab_size_answer, 300, weights=[embedding_matrix_answer],  input_length=max_length_answer,trainable=False) # embedding layer\n",
    "embedded_answer = embed_layer3(input3)\n",
    "# passing embedded layer output to LSTM\n",
    "lstm_answer = LSTM(10, kernel_initializer=tf.keras.initializers.glorot_uniform(seed=26),\n",
    "                 recurrent_initializer=tf.keras.initializers.orthogonal(seed=54),\n",
    "                 bias_initializer=tf.keras.initializers.zeros(),\n",
    "                   kernel_regularizer = tensorflow.keras.regularizers.l2(0.1),return_sequences=True)(embedded_answer)\n",
    "norm_3 = tf.keras.layers.LayerNormalization()(lstm_answer)\n",
    "flat_answer = Flatten()(norm_3)\n",
    "\n",
    "#====================================================================================================================\n",
    "\n",
    "#input 4\n",
    "input4 = Input(shape=(26,1), name = 'Remaining_input')\n",
    "conv = Conv1D(filters = 32, kernel_size = 3,strides = 1, activation='relu',kernel_initializer=tf.keras.initializers.he_normal(seed=43),\n",
    "              kernel_regularizer = tensorflow.keras.regularizers.l2(0.1),input_shape=(None,26,1))(input4)\n",
    "conv= MaxPooling1D()(conv)\n",
    "conv = Conv1D(32,3, kernel_initializer=tf.keras.initializers.he_normal(seed=43), activation='relu')(conv)\n",
    "flat_remaining = Flatten()(conv)\n",
    "\n",
    "#====================================================================================================================\n",
    "\n",
    "final_data = concatenate([flat_title,flat_body,flat_answer,flat_remaining])\n",
    "\n",
    "#====================================================================================================================\n",
    "\n",
    "output = Dense(30,activation='sigmoid',kernel_initializer=tf.keras.initializers.glorot_uniform(seed=45))(final_data)\n",
    "\n",
    "# create model with seven inputs\n",
    "model_3 = Model([input1,input2,input3,input4], output)\n",
    "\n",
    "model_3.compile(loss=tensorflow.keras.losses.binary_crossentropy,\n",
    "              optimizer=tensorflow.keras.optimizers.Adam(),\n",
    "              metrics=['mae'])\n",
    "\n",
    "print(model_3.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "39/39 [==============================] - 15s 379ms/step - loss: 21.3795 - mae: 0.2011 - val_loss: 14.9085 - val_mae: 0.1869\n",
      "Epoch 2/40\n",
      "39/39 [==============================] - 14s 354ms/step - loss: 11.6154 - mae: 0.1685 - val_loss: 9.0392 - val_mae: 0.1652\n",
      "Epoch 3/40\n",
      "39/39 [==============================] - 14s 356ms/step - loss: 7.7525 - mae: 0.1585 - val_loss: 6.7471 - val_mae: 0.1590\n",
      "Epoch 4/40\n",
      "39/39 [==============================] - 14s 350ms/step - loss: 6.2116 - mae: 0.1503 - val_loss: 5.7913 - val_mae: 0.1602\n",
      "Epoch 5/40\n",
      "39/39 [==============================] - 14s 353ms/step - loss: 5.5146 - mae: 0.1463 - val_loss: 5.2926 - val_mae: 0.1594\n",
      "Epoch 6/40\n",
      "39/39 [==============================] - 14s 346ms/step - loss: 5.0870 - mae: 0.1418 - val_loss: 4.9285 - val_mae: 0.1506\n",
      "Epoch 7/40\n",
      "39/39 [==============================] - 14s 358ms/step - loss: 4.7499 - mae: 0.1387 - val_loss: 4.6182 - val_mae: 0.1522\n",
      "Epoch 8/40\n",
      "39/39 [==============================] - 13s 344ms/step - loss: 4.4454 - mae: 0.1355 - val_loss: 4.3298 - val_mae: 0.1527\n",
      "Epoch 9/40\n",
      "39/39 [==============================] - 14s 352ms/step - loss: 4.1644 - mae: 0.1323 - val_loss: 4.0653 - val_mae: 0.1467\n",
      "Epoch 10/40\n",
      "39/39 [==============================] - 14s 347ms/step - loss: 3.9052 - mae: 0.1298 - val_loss: 3.8192 - val_mae: 0.1505\n",
      "Epoch 11/40\n",
      "39/39 [==============================] - 14s 353ms/step - loss: 3.6620 - mae: 0.1274 - val_loss: 3.5900 - val_mae: 0.1563\n",
      "Epoch 12/40\n",
      "39/39 [==============================] - 13s 344ms/step - loss: 3.4354 - mae: 0.1253 - val_loss: 3.3719 - val_mae: 0.1536\n",
      "Epoch 13/40\n",
      "39/39 [==============================] - 14s 351ms/step - loss: 3.2249 - mae: 0.1234 - val_loss: 3.1753 - val_mae: 0.1542\n",
      "Epoch 14/40\n",
      "39/39 [==============================] - 14s 349ms/step - loss: 3.0259 - mae: 0.1209 - val_loss: 2.9896 - val_mae: 0.1466\n",
      "Epoch 15/40\n",
      "39/39 [==============================] - 14s 353ms/step - loss: 2.8401 - mae: 0.1181 - val_loss: 2.8118 - val_mae: 0.1491\n",
      "Epoch 16/40\n",
      "39/39 [==============================] - 14s 357ms/step - loss: 2.6688 - mae: 0.1168 - val_loss: 2.6502 - val_mae: 0.1541\n",
      "Epoch 17/40\n",
      "39/39 [==============================] - 14s 349ms/step - loss: 2.5084 - mae: 0.1154 - val_loss: 2.5004 - val_mae: 0.1480\n",
      "Epoch 18/40\n",
      "39/39 [==============================] - 14s 347ms/step - loss: 2.3576 - mae: 0.1135 - val_loss: 2.3533 - val_mae: 0.1514\n",
      "Epoch 19/40\n",
      "39/39 [==============================] - 14s 347ms/step - loss: 2.2161 - mae: 0.1113 - val_loss: 2.2235 - val_mae: 0.1511\n",
      "Epoch 20/40\n",
      "39/39 [==============================] - 14s 354ms/step - loss: 2.0859 - mae: 0.1104 - val_loss: 2.1045 - val_mae: 0.1567\n",
      "Epoch 21/40\n",
      "39/39 [==============================] - 13s 346ms/step - loss: 1.9617 - mae: 0.1081 - val_loss: 1.9838 - val_mae: 0.1529\n",
      "Epoch 22/40\n",
      "39/39 [==============================] - 13s 345ms/step - loss: 1.8479 - mae: 0.1074 - val_loss: 1.8814 - val_mae: 0.1487\n",
      "Epoch 23/40\n",
      "39/39 [==============================] - 14s 347ms/step - loss: 1.7419 - mae: 0.1063 - val_loss: 1.7767 - val_mae: 0.1534\n",
      "Epoch 24/40\n",
      "39/39 [==============================] - 14s 356ms/step - loss: 1.6395 - mae: 0.1041 - val_loss: 1.6837 - val_mae: 0.1523\n",
      "Epoch 25/40\n",
      "39/39 [==============================] - 14s 349ms/step - loss: 1.5466 - mae: 0.1032 - val_loss: 1.5982 - val_mae: 0.1549\n",
      "Epoch 26/40\n",
      "39/39 [==============================] - 14s 350ms/step - loss: 1.4595 - mae: 0.1026 - val_loss: 1.5142 - val_mae: 0.1526\n",
      "Epoch 27/40\n",
      "39/39 [==============================] - 14s 350ms/step - loss: 1.3773 - mae: 0.1012 - val_loss: 1.4391 - val_mae: 0.1537\n",
      "Epoch 28/40\n",
      "39/39 [==============================] - 14s 352ms/step - loss: 1.3013 - mae: 0.1002 - val_loss: 1.3686 - val_mae: 0.1550\n",
      "Epoch 29/40\n",
      "39/39 [==============================] - 14s 358ms/step - loss: 1.2287 - mae: 0.0989 - val_loss: 1.3021 - val_mae: 0.1547\n",
      "Epoch 30/40\n",
      "39/39 [==============================] - 14s 352ms/step - loss: 1.1613 - mae: 0.0973 - val_loss: 1.2428 - val_mae: 0.1516\n",
      "Epoch 31/40\n",
      "39/39 [==============================] - 14s 350ms/step - loss: 1.1014 - mae: 0.0978 - val_loss: 1.1870 - val_mae: 0.1556\n",
      "Epoch 32/40\n",
      "39/39 [==============================] - 13s 345ms/step - loss: 1.0439 - mae: 0.0973 - val_loss: 1.1318 - val_mae: 0.1519\n",
      "Epoch 33/40\n",
      "39/39 [==============================] - 14s 357ms/step - loss: 0.9893 - mae: 0.0965 - val_loss: 1.0888 - val_mae: 0.1594\n",
      "Epoch 34/40\n",
      "39/39 [==============================] - 14s 346ms/step - loss: 0.9375 - mae: 0.0956 - val_loss: 1.0314 - val_mae: 0.1567\n",
      "Epoch 35/40\n",
      "39/39 [==============================] - 14s 353ms/step - loss: 0.8872 - mae: 0.0932 - val_loss: 0.9941 - val_mae: 0.1571\n",
      "Epoch 36/40\n",
      "39/39 [==============================] - 14s 347ms/step - loss: 0.8433 - mae: 0.0927 - val_loss: 0.9503 - val_mae: 0.1614\n",
      "Epoch 37/40\n",
      "39/39 [==============================] - 14s 360ms/step - loss: 0.8024 - mae: 0.0926 - val_loss: 0.9132 - val_mae: 0.1553\n",
      "Epoch 38/40\n",
      "39/39 [==============================] - 13s 344ms/step - loss: 0.7620 - mae: 0.0905 - val_loss: 0.8771 - val_mae: 0.1556\n",
      "Epoch 39/40\n",
      "39/39 [==============================] - 14s 349ms/step - loss: 0.7266 - mae: 0.0904 - val_loss: 0.8494 - val_mae: 0.1580\n",
      "Epoch 40/40\n",
      "39/39 [==============================] - 14s 348ms/step - loss: 0.6947 - mae: 0.0902 - val_loss: 0.8159 - val_mae: 0.1596\n"
     ]
    }
   ],
   "source": [
    "# checkpoint path and function to save best trained weights\n",
    "filepath=\"weights_lstm3.best_copy.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "#fitting model on train data\n",
    "history_model_3 = model_3.fit( train_2, y_train, \n",
    "                               epochs=40,\n",
    "                               verbose=1,\n",
    "                               batch_size=128,\n",
    "                               callbacks=[checkpoint],\n",
    "                               #validation_data=(cv_2, y_cv))\n",
    "                               validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation on train data =  0.5987056890707704\n",
      "Spearman correlation on cross-validation data =  0.2148912579909973\n"
     ]
    }
   ],
   "source": [
    "model_3.load_weights(\"weights_lstm3.best_copy.hdf5\")\n",
    "\n",
    "y_tr_pred = model_3.predict(train_2)\n",
    "print(\"Spearman correlation on train data = \",spearman(y_train,y_tr_pred))\n",
    "\n",
    "y_cv_pred = model_3.predict(cv_2)\n",
    "print(\"Spearman correlation on cross-validation data = \",spearman(y_cv,y_cv_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSTM Model without Layer Normalization has Spearman correlation score on train data = 0.274 and on cross-validation data = 0.249.\n",
    "\n",
    "##### LSTM Model with Layer Normalization has Spearman correlation score on train data = 0.598 and on cross-validation data = 0.214, that means it is overfittig.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
