{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import pickle\n",
    "#from transformers import \n",
    "import numpy as np\n",
    "from numpy import zeros, newaxis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm\n",
    "import random as rn\n",
    "from tqdm import tqdm\n",
    "import tensorflow \n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#nltk.download('vader_lexicon')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D,AveragePooling1D,MaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Flatten,Embedding,Input,concatenate,Activation,Dropout,BatchNormalization,LSTM\n",
    "from tensorflow.keras import regularizers,Model\n",
    "from tensorflow.keras.regularizers import l1,l2\n",
    "from tensorflow.keras.callbacks import TensorBoard, Callback, EarlyStopping, ModelCheckpoint,LearningRateScheduler\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiments(text_data):\n",
    "    '''\n",
    "    function to generate sentiments from text data\n",
    "    '''\n",
    "        \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    neg=[]\n",
    "    neu=[]\n",
    "    pos=[]\n",
    "    comp=[]\n",
    "    \n",
    "    for txt in text_data:\n",
    "        ss = sid.polarity_scores(txt)\n",
    "        neg.append(ss['neg'])\n",
    "        neu.append(ss['neu'])\n",
    "        pos.append(ss['pos'])\n",
    "        comp.append(ss['compound'])\n",
    "        \n",
    "    sentiment_ = dict()\n",
    "    sentiment_['neg'] = np.array(neg)[:,newaxis]\n",
    "    sentiment_['neu'] = np.array(neu)[:,newaxis]\n",
    "    sentiment_['pos'] = np.array(pos)[:,newaxis]\n",
    "    sentiment_['comp'] = np.array(comp)[:,newaxis]\n",
    "\n",
    "    return sentiment_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "def spearman(y_true,y_pred):\n",
    "    '''\n",
    "    function to calculate mean spearman correlation of all 30 class-labels\n",
    "    '''\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    spearman_y = []\n",
    "    for i in range(30):\n",
    "        score = spearmanr(y_true[:,i], y_pred[:,i] + np.random.normal(0, 1e-7, y_pred.shape[0]) , \n",
    "                                                                        nan_policy='omit').correlation\n",
    "                                                                        \n",
    "\n",
    "        spearman_y.append(score)\n",
    "    mean_score = np.nanmean(spearman_y)\n",
    "    return mean_score\n",
    "\n",
    "#=====================================================================================================================\n",
    "\n",
    "# BERT tokenizer (pretrained)\n",
    "tokenizer = BertTokenizer.from_pretrained('../input/bert-base-uncased3/bert_en_uncased_L-12_H-768_A-12_2/assets/vocab.txt')\n",
    "\n",
    "def encode_text(text,max_len):\n",
    "    \n",
    "    '''\n",
    "    function to encode text for input to ALBERT\n",
    "    '''\n",
    "    \n",
    "    encoded_dict = tokenizer.encode_plus(text, None, max_length=max_len, pad_to_max_length=True,\n",
    "                                       add_special_tokens=True)\n",
    "    return encoded_dict\n",
    "    \n",
    "#================================================================================\n",
    "\n",
    "def get_encoded_bert_inputs(title,body,answer,max_len):\n",
    "    '''\n",
    "    function to encode text data into BERT input form\n",
    "    '''\n",
    "    q_input_ids = []\n",
    "    q_masks = []\n",
    "    q_segments = []\n",
    "    a_input_ids = []\n",
    "    a_masks = []\n",
    "    a_segments = []\n",
    "    \n",
    "    # question encoding\n",
    "    for i in range(len(title)):\n",
    "        q_text = title[i] + \" [SEP] \" + body[i]\n",
    "        q_encoded_dict = encode_text(q_text,max_len)\n",
    "        q_input_ids.append(q_encoded_dict['input_ids'])\n",
    "        q_masks.append(q_encoded_dict['attention_mask'])\n",
    "        q_segments.append(q_encoded_dict['token_type_ids'])\n",
    "    \n",
    "    # answer encoding\n",
    "    for i in range(len(answer)):\n",
    "        a_text = answer[i]\n",
    "        a_encoded_dict = encode_text(a_text,max_len)\n",
    "        a_input_ids.append(a_encoded_dict['input_ids'])\n",
    "        a_masks.append(a_encoded_dict['attention_mask'])\n",
    "        a_segments.append(a_encoded_dict['token_type_ids'])\n",
    "    \n",
    "    return q_input_ids, q_masks, q_segments, a_input_ids, a_masks, a_segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum_of_word_vec(input_vec):\n",
    "    # function to get sum of vectors for each word in a question/answer\n",
    "    v = np.zeros(shape=(1,768))\n",
    "    num_of_words = input_vec.shape[1]\n",
    "    for j in range(num_of_words):\n",
    "        v = v + input_vec[0,j,:]\n",
    "    return v\n",
    "\n",
    "#=========================================================================\n",
    "\n",
    "def get_text_vector(bert_layer,input_ids,mask,seg):\n",
    "    '''\n",
    "    function to get BERT output that represents whole input sequence\n",
    "    function takes the encoded output for each word and then sums up to get the final vector for each datapoint \n",
    "    '''\n",
    "    \n",
    "    for i in tqdm(range(len(input_ids))):\n",
    "        pool, seq = bert_layer([input_ids[i:i+1],mask[i:i+1],seg[i:i+1]])\n",
    "        \n",
    "        seq = get_sum_of_word_vec(seq)\n",
    "        \n",
    "        if i>0:\n",
    "            \n",
    "            final_text_seq = tf.keras.layers.concatenate([final_text_seq,seq],axis=0)\n",
    "        \n",
    "        else:\n",
    "\n",
    "            final_text_seq = seq\n",
    "            \n",
    "    return final_text_seq    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_data):\n",
    "    '''\n",
    "    function to predict on raw data input\n",
    "    input type should be an list/array of datapoints\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #loading saved files \n",
    "    \n",
    "    with open('../input/bert-files/category_vectorizer', 'rb') as f:\n",
    "        category_vectorizer = pickle.load(f)\n",
    "        \n",
    "    with open('../input/bert-files/host_vectorizer', 'rb') as f:\n",
    "        host_vectorizer = pickle.load(f)\n",
    "        \n",
    "    with open(\"../input/bert-files/model.json\", \"r\") as json_file:\n",
    "        model_json = json_file.read()\n",
    "        \n",
    "    model = tf.keras.models.model_from_json(model_json)\n",
    "    model.load_weights(\"../input/bert-files/weights_final4.best_copy.hdf5\")\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('../input/bert-base-uncased3/bert_en_uncased_L-12_H-768_A-12_2/assets/vocab.txt')\n",
    "    \n",
    "    bert_layer = hub.KerasLayer(\"../input/bert-base-uncased3/bert_en_uncased_L-12_H-768_A-12_2\", trainable=False)\n",
    "    \n",
    "    input_data = np.array(input_data)  # converting input to a numpy array (if it is given as a list)\n",
    "    \n",
    "    #preprocessing host\n",
    "    host = pd.Series(input_data[:,10]).apply(lambda x: x.split('.')[-2])\n",
    "    host = host.apply(lambda x: x.lower())\n",
    "    host = host.apply(lambda x: x.strip())\n",
    "    #=================================================================================================\n",
    "    # preprocessing 'category'  to lower-case and stripping leading and tailing spaces \n",
    "    category = pd.Series(input_data[:,9]).apply(lambda x: x.lower())\n",
    "    category = category.apply(lambda x: x.strip())\n",
    "    #=================================================================================================\n",
    "    \n",
    "    #encoding categorical data - 'category' and 'host'\n",
    "    category_encoded = category_vectorizer.transform(category).todense()\n",
    "    host_encoded = host_vectorizer.transform(host).todense()\n",
    "    \n",
    "    #=================================================================================================\n",
    "    \n",
    "    # getting question title, question body and answers from input array \n",
    "    question_title = pd.Series(input_data[:,1])\n",
    "    question_body = pd.Series(input_data[:,2])\n",
    "    answer = pd.Series(input_data[:,5])\n",
    "\n",
    "    #=================================================================================================\n",
    "    \n",
    "    # taking length of question title, question body, answer\n",
    "    q_title_length = np.array(question_title.apply(lambda x: len(x.split(' '))))[:,newaxis]\n",
    "    q_body_length = np.array(question_body.apply(lambda x: len(x.split(' '))))[:,newaxis]\n",
    "    answer_length = np.array(answer.apply(lambda x: len(x.split(' '))))[:,newaxis]\n",
    "    \n",
    "    #creating sentiments features\n",
    "    title_sentiments = get_sentiments(question_title)\n",
    "    body_sentiments = get_sentiments(question_body)\n",
    "    answer_sentiments = get_sentiments(answer)\n",
    "    \n",
    "    \n",
    "    #=================================================================================================\n",
    "    \n",
    "    # getting inputs for BERT\n",
    "    q_input_ids_test,q_mask_test,q_seg_test,a_input_ids_test,a_mask_test,a_seg_test = get_encoded_bert_inputs(question_title,question_body,answer,max_len=450)\n",
    "    \n",
    "    # getting vector output (representing whole input sequence) of (question_title+question_body) from BERT \n",
    "    question_vect_test = get_text_vector(bert_layer,q_input_ids_test,q_mask_test,q_seg_test)\n",
    "    \n",
    "    # getting vector output (representing whole inpur sequence) of answer from BERT for test data\n",
    "    answer_vect_test = get_text_vector(bert_layer,a_input_ids_test,a_mask_test,a_seg_test)\n",
    "    \n",
    "    # concatenating question and answer output from BERT\n",
    "    final_text_vector = tf.keras.layers.concatenate([question_vect_test,answer_vect_test])\n",
    "    #print(\"Test text vector shape = \",final_text_vector_test.shape)\n",
    "\n",
    "    vect_cat_num = np.hstack([category_encoded,host_encoded,\n",
    "                        q_title_length,q_body_length,answer_length, title_sentiments['neg'],\n",
    "                              title_sentiments['neu'], title_sentiments['pos'], title_sentiments['comp'], \n",
    "                              body_sentiments['neg'],body_sentiments['neu'], body_sentiments['pos'], body_sentiments['comp'], \n",
    "                              answer_sentiments['neg'],answer_sentiments['neu'], answer_sentiments['pos'], answer_sentiments['comp']\n",
    "                             ])\n",
    "    \n",
    "    \n",
    "    # reshaping data for convolution layer\n",
    "    vect_cat_num = vect_cat_num[:,:,newaxis]\n",
    "    \n",
    "    #==============================================================================================================\n",
    "    \n",
    "    prediction = model.predict([final_text_vector,vect_cat_num]) \n",
    "    prediction = np.hstack([input_data[:,0][:,newaxis],prediction]) #giving \"qa_id\" as 1st column to identify the predictions\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6079/6079 [13:05<00:00,  7.74it/s]\n",
      "100%|██████████| 6079/6079 [12:50<00:00,  7.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# prediction on train data\n",
    "train_data = pd.read_csv(\"../input/google-quest-challenge/train.csv\")\n",
    "train_data = np.array(train_data)\n",
    "train_output = predict(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6079, 30)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting actual value of class-labels\n",
    "actual_train = train_data[:,11:]\n",
    "actual_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4021423208910162"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spearman's correlation on train data\n",
    "spearman(actual_train,train_output[:,1:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
