{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import pickle\n",
    "#from transformers import \n",
    "import numpy as np\n",
    "from numpy import zeros, newaxis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm\n",
    "import random as rn\n",
    "import tensorflow \n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#nltk.download('vader_lexicon')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D,AveragePooling1D,MaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Flatten,Embedding,Input,concatenate,Activation,Dropout,BatchNormalization,LSTM\n",
    "from tensorflow.keras import regularizers,Model\n",
    "from tensorflow.keras.regularizers import l1,l2\n",
    "from tensorflow.keras.callbacks import TensorBoard, Callback, EarlyStopping, ModelCheckpoint,LearningRateScheduler\n",
    "\n",
    "\n",
    "from matplotlib_venn import venn2, venn2_unweighted\n",
    "from matplotlib_venn import venn3, venn3_unweighted\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/47091490/4084039\n",
    "import re\n",
    "\n",
    "def preprocess_text(phrase):\n",
    "    # specific\n",
    "    phrase = str(phrase)\n",
    "    phrase = phrase.lower()\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrade = re.sub(r\"wont\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"cant\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"cannot\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"doesn't\", \"does not\", phrase)\n",
    "    \n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    \n",
    "    phrase = phrase.replace('\\\\r', ' ')\n",
    "    phrase = phrase.replace('\\\\\"', ' ')\n",
    "    phrase = phrase.replace('\\\\n', ' ')\n",
    "    phrase = phrase.replace('\\\\', ' ')\n",
    "    \n",
    "    #phrase = re.sub(r'http\\S+', '', phrase)  # for removal all http link\n",
    "    phrase = re.sub(r'[^A-Za-z0-9]', ' ',phrase) # remove everything except alphaets and numbers\n",
    "    \n",
    "    sent = ''\n",
    "    for val in phrase.split():          # removing multiple spaces between words\n",
    "        sent = sent + ' ' + val\n",
    "    sent = sent.strip()\n",
    "    \n",
    "    return sent\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiments(text_data):\n",
    "    '''\n",
    "    function to generate sentiments from text data\n",
    "    '''\n",
    "        \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    neg=[]\n",
    "    neu=[]\n",
    "    pos=[]\n",
    "    comp=[]\n",
    "    \n",
    "    for txt in text_data:\n",
    "        ss = sid.polarity_scores(txt)\n",
    "        neg.append(ss['neg'])\n",
    "        neu.append(ss['neu'])\n",
    "        pos.append(ss['pos'])\n",
    "        comp.append(ss['compound'])\n",
    "        \n",
    "    sentiment_ = dict()\n",
    "    sentiment_['neg'] = np.array(neg)[:,newaxis]\n",
    "    sentiment_['neu'] = np.array(neu)[:,newaxis]\n",
    "    sentiment_['pos'] = np.array(pos)[:,newaxis]\n",
    "    sentiment_['comp'] = np.array(comp)[:,newaxis]\n",
    "\n",
    "    return sentiment_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(train_vect_text, train_vect_cat_num):\n",
    "    '''\n",
    "    function to create CNN Model architecture\n",
    "    '''\n",
    "\n",
    "    #fixing numpy RS\n",
    "    np.random.seed(42)\n",
    "\n",
    "    #fixing tensorflow RS\n",
    "    tensorflow.random.set_seed(32)\n",
    "    \n",
    "    #python RS\n",
    "    rn.seed(12)\n",
    "\n",
    "    #input 1\n",
    "    input1 = Input(shape=(train_vect_text.shape[1],1), name = 'input_1')\n",
    "    conv = Conv1D(filters = 32, kernel_size = 11,strides = 3, activation='relu',\n",
    "                  kernel_initializer=tensorflow.keras.initializers.he_normal(seed=43),\n",
    "                  kernel_regularizer=tensorflow.keras.regularizers.l2(0.1),\n",
    "                  input_shape=(None,train_vect_text.shape[1],1))(input1)\n",
    "    conv = MaxPooling1D()(conv)\n",
    "    conv = Conv1D(32, 11, activation='relu',kernel_initializer=tensorflow.keras.initializers.he_normal(seed=43))(conv)\n",
    "    conv = MaxPooling1D()(conv)\n",
    "    flat_text = Flatten()(conv)\n",
    "    out_1 =  Dense(30,activation='sigmoid')(flat_text)\n",
    "    #====================================================================================================================\n",
    "\n",
    "    #input 2\n",
    "    input2 = Input(shape=(train_vect_cat_num.shape[1],1), name = 'input_2')\n",
    "    conv = Conv1D(filters = 64, kernel_size = 3,strides = 2, activation='relu',\n",
    "                  kernel_initializer=tensorflow.keras.initializers.he_normal(seed=43),\n",
    "                  kernel_regularizer=tensorflow.keras.regularizers.l2(0.1),\n",
    "                  input_shape=(None,train_vect_cat_num.shape[1],1))(input2)\n",
    "\n",
    "    conv = MaxPooling1D()(conv)\n",
    "    conv = Conv1D(32, 3,  activation='relu',kernel_initializer=tensorflow.keras.initializers.he_normal(seed=43))(conv)\n",
    "    conv = MaxPooling1D()(conv)\n",
    "    flat_rem = Flatten()(conv)\n",
    "    out_2 =  Dense(30,activation='sigmoid')(flat_rem)\n",
    "    #====================================================================================================================\n",
    "\n",
    "    final_data = concatenate([out_1,out_2])\n",
    "\n",
    "    #====================================================================================================================\n",
    "\n",
    "    output = Dense(30,activation='sigmoid',kernel_initializer=tensorflow.keras.initializers.glorot_uniform(seed=45))(final_data)\n",
    "\n",
    "    # create model with 2 inputs\n",
    "    model = Model([input1,input2], output)\n",
    "\n",
    "    model.compile(loss=tensorflow.keras.losses.binary_crossentropy,\n",
    "              optimizer=tensorflow.keras.optimizers.Adam(0.001),\n",
    "              metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading glove vectors in python: https://stackoverflow.com/a/38230349/4084039\n",
    "def loadGloveModel(gloveFile):\n",
    "    print (\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding=\"utf8\")\n",
    "    model = {}\n",
    "    for line in tqdm(f):\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countVectorizer(df_train,df_cv,column):\n",
    "    '''\n",
    "    function to vectorize categorical data\n",
    "    '''\n",
    "    # one-hot encoding 'category' feature\n",
    "    vect = CountVectorizer(binary=True)\n",
    "    vect.fit(df_train[column])\n",
    "    encoded_train = vect.transform(df_train[column]).todense()\n",
    "    encoded_cv = vect.transform(df_cv[column]).todense()\n",
    "    \n",
    "    return encoded_train, encoded_cv, vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_w2v(text_data,glove_vector,glove_words):\n",
    "    '''\n",
    "    function to vectorize text data into average word2vec\n",
    "    '''    \n",
    "    # computing average word2vec for each question title\n",
    "    avg_w2v_vectors = []; # the avg-w2v for each question title is stored in this list\n",
    "    for sentence in tqdm(text_data): # for each question title\n",
    "        vector = np.zeros(300) # as word vectors are of zero length\n",
    "        cnt_words = 0; # num of words with a valid vector in the question title\n",
    "        for word in sentence.split(): # for each word in a question title\n",
    "            if word in glove_words:\n",
    "                vector += glove_vector[word]\n",
    "                cnt_words += 1\n",
    "        if cnt_words != 0:\n",
    "            vector /= cnt_words\n",
    "        avg_w2v_vectors.append(vector)\n",
    "        \n",
    "    return avg_w2v_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "def spearman(y_true,y_pred):\n",
    "    '''\n",
    "    function to calculate mean spearman correlation of all 30 class-labels\n",
    "    '''\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    spearman_y = []\n",
    "    for i in range(30):\n",
    "        score = spearmanr(y_true[:,i], y_pred[:,i] + np.random.normal(0, 1e-7, y_pred.shape[0]) , \n",
    "                                                                        nan_policy='omit').correlation\n",
    "                                                                        \n",
    "\n",
    "        spearman_y.append(score)\n",
    "    mean_score = np.nanmean(spearman_y)\n",
    "    return mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List/array input\n",
    "def predict_CNN(input_data):\n",
    "    '''\n",
    "    function to predict on raw data input\n",
    "    input type should be an list/array of datapoints\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #loading saved files \n",
    "    # reading glove_vectors pickle file\n",
    "    with open('glove_vectors', 'rb') as f:\n",
    "        glove_vector = pickle.load(f)\n",
    "        glove_words =  set(glove_vector.keys())\n",
    "    \n",
    "    with open('category_vectorizer', 'rb') as f:\n",
    "        category_vectorizer = pickle.load(f)\n",
    "        \n",
    "    with open('host_vectorizer', 'rb') as f:\n",
    "        host_vectorizer = pickle.load(f)\n",
    "        \n",
    "    with open(\"model.json\", \"r\") as json_file:\n",
    "        model_json = json_file.read()\n",
    "        \n",
    "    model = tf.keras.models.model_from_json(model_json)\n",
    "    model.load_weights(\"weights_cnn.best_copy.hdf5\")\n",
    "    \n",
    "    input_data = np.array(input_data) # converting input to a numpy array (if it is given as a list)\n",
    "\n",
    "    #preprocessing host\n",
    "    host = pd.Series(input_data[:,10]).apply(lambda x: x.split('.')[-2])\n",
    "    host = host.apply(lambda x: x.lower())\n",
    "    host = host.apply(lambda x: x.strip())\n",
    "    #=================================================================================================\n",
    "    # preprocessing 'category'  to lower-case and stripping leading and tailing spaces \n",
    "    category = pd.Series(input_data[:,9]).apply(lambda x: x.lower())\n",
    "    category = category.apply(lambda x: x.strip())\n",
    "    #=================================================================================================\n",
    "    \n",
    "    #encoding categorical data - 'category' and 'host'\n",
    "    category_encoded = category_vectorizer.transform(category).todense()\n",
    "    host_encoded = host_vectorizer.transform(host).todense()\n",
    "\n",
    "    #=================================================================================================\n",
    "    \n",
    "    # preprocessing question title, question body and answers\n",
    "    question_title = pd.Series(input_data[:,1]).apply(lambda x: preprocess_text(x))\n",
    "    question_body = pd.Series(input_data[:,2]).apply(lambda x: preprocess_text(x))\n",
    "    answer = pd.Series(input_data[:,5]).apply(lambda x: preprocess_text(x))\n",
    "\n",
    "    #=================================================================================================\n",
    "    \n",
    "    # taking length of question title, question body, answer\n",
    "    q_title_length = np.array(question_title.apply(lambda x: len(x.split(' '))))[:,newaxis]\n",
    "    q_body_length = np.array(question_body.apply(lambda x: len(x.split(' '))))[:,newaxis]\n",
    "    answer_length = np.array(answer.apply(lambda x: len(x.split(' '))))[:,newaxis]\n",
    "    \n",
    "    #==================================================================================================\n",
    "    \n",
    "    #creating sentiments features\n",
    "    \n",
    "    title_sentiments = get_sentiments(question_title)\n",
    "    body_sentiments = get_sentiments(question_body)\n",
    "    answer_sentiments = get_sentiments(answer)\n",
    "    \n",
    "    # encoding text data\n",
    "    avg_w2v_vectors_title = get_average_w2v(question_title,glove_vector,glove_words)\n",
    "    avg_w2v_vectors_body = get_average_w2v(question_body,glove_vector,glove_words)\n",
    "    avg_w2v_vectors_answer = get_average_w2v(answer,glove_vector,glove_words)\n",
    "\n",
    "   #=========================================================================================\n",
    "\n",
    "    #concatenating final encoded data\n",
    "    \n",
    "    vect_text = np.hstack([avg_w2v_vectors_title,avg_w2v_vectors_body,\n",
    "                        avg_w2v_vectors_answer])\n",
    "\n",
    "    vect_cat_num = np.hstack([category_encoded,host_encoded,\n",
    "                        q_title_length,q_body_length,answer_length, title_sentiments['neg'],\n",
    "                              title_sentiments['neu'], title_sentiments['pos'], title_sentiments['comp'], \n",
    "                              body_sentiments['neg'],body_sentiments['neu'], body_sentiments['pos'], body_sentiments['comp'], \n",
    "                              answer_sentiments['neg'],answer_sentiments['neu'], answer_sentiments['pos'], answer_sentiments['comp']\n",
    "                             ])\n",
    "    \n",
    "    \n",
    "    # reshaping data for convolution layer\n",
    "    vect_text = vect_text[:,:,newaxis]\n",
    "    vect_cat_num = vect_cat_num[:,:,newaxis]\n",
    "    \n",
    "    #====================================================================================================\n",
    "    \n",
    "    #predicting on input data \n",
    "    prediction = model.predict([vect_text,vect_cat_num])\n",
    "    prediction = np.hstack([input_data[:,0][:,newaxis],prediction]) #giving \"qa_id\" as 1st column to identify the predictions\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 6079/6079 [00:00<00:00, 15873.94it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6079/6079 [00:03<00:00, 1534.66it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6079/6079 [00:04<00:00, 1455.14it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"google-quest-challenge/train.csv\")\n",
    "train_data = np.array(train_data)\n",
    "train_output = predict_CNN(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0.9206799864768982, 0.6444732546806335, ...,\n",
       "        0.12324012070894241, 0.7039588093757629, 0.9255793690681458],\n",
       "       [1, 0.9051656126976013, 0.6512964963912964, ...,\n",
       "        0.10090433806180954, 0.5737784504890442, 0.9125009179115295],\n",
       "       [2, 0.880452573299408, 0.5040979981422424, ...,\n",
       "        0.14553439617156982, 0.5804992914199829, 0.921263575553894],\n",
       "       ...,\n",
       "       [9645, 0.857827365398407, 0.47633975744247437, ...,\n",
       "        0.0990651324391365, 0.30216357111930847, 0.8806634545326233],\n",
       "       [9646, 0.921221911907196, 0.6582242846488953, ...,\n",
       "        0.09240725636482239, 0.708876371383667, 0.9302746057510376],\n",
       "       [9647, 0.9378359913825989, 0.7525654435157776, ...,\n",
       "        0.09269876778125763, 0.7716101408004761, 0.9221950769424438]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing output from predict_CNN function\n",
    "train_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6079, 30)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting actual value of class-labels\n",
    "actual_train = train_data[:,11:]\n",
    "actual_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35904301041451253"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spearman's correlation on train data\n",
    "spearman(actual_train,train_output[:,1:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
